const PORTFOLIO_SECTIONS = {"at-risk-student-dashboard-webpage":{"id":"at-risk-student-dashboard-webpage","title":"At-Risk Student Dashboard Webpage","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eAt-Risk Student Dashboard Webpage\u003c/h2\u003e\n\u003cp\u003eThe anonymized At-Risk Student Dashboard is rendered as a static HTML export that mirrors the live Python-driven version while safeguarding student privacy. This artifact documents the production-grade visualization that counselors use to triage interventions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDynamic tables update with key market and risk indicators reflecting historical data in the export.\u003c/li\u003e\n\u003cli\u003eJavaScript post-processing replaces names and IDs with sequential aliases to maintain FERPA compliance.\u003c/li\u003e\n\u003cli\u003eA branded cover image introduces the dashboard and aligns with the rest of the portfolio aesthetic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe export references authentic dashboard assets such as \u003ccode\u003eget_atm_iv_ts.png\u003c/code\u003e and \u003ccode\u003efinancial_analysis.png\u003c/code\u003e, demonstrating the breadth of analytics available to staff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAccess the artifact:\u003c/strong\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/output.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard Webpage\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"},"program-of-study":{"id":"program-of-study","title":"Program of Study","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eProgram of Study\u003c/h2\u003e\n\u003cp\u003eThe Program of Study for my ISLT degree includes the courses below. Each course lists the term, a concise description, and my reflection linking outcomes to practice and artifacts where applicable.\u003c/p\u003e\n\u003cdiv class=\"table-wrap\"\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCourse\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003cth\u003eTerm\u003c/th\u003e\n\u003cth\u003eReflection\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7355\u003c/strong\u003e\u003cbr/\u003eWeb Design \u0026amp; Development\u003c/td\u003e\n\u003ctd\u003ePrinciples of accessible, standards-based, responsive web design for learning.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eBefore this course, much of my web work was pragmatic: I could make pages function, but the underlying structure was inconsistent and not always accessible. IS_LT 7355 pushed me to slow down and design deliberately. I learned to treat HTML as more than a way to place content on a screen - it became a semantic contract with assistive technologies and a foundation for predictable, reusable components. I adopted landmarks (header, nav, main, footer), meaningful headings, and descriptive link text to improve navigability for screen reader users and keyboard-only users. The shift from \"it looks right\" to \"it\u0027s structured right\" has had a measurable impact on findability and comprehension.\u003c/p\u003e\n\u003cp\u003eAccessibility moved from a checklist to a design habit. I used WCAG 2.1 AA guidelines to evaluate color contrast and reworked palettes to remain legible in bright light and on older devices. I added a visible skip link, ensured logical tab order, and provided focus states that are easy to see. Images now include alt text that communicates purpose rather than appearance, and icon-only links gained accessible names. On mobile, I removed nonessential flourishes, simplified spacing, and set typography for readability at small sizes. These decisions were not merely technical; they respected the realities of my learners\u0027 devices, bandwidth, and attention.\u003c/p\u003e\n\u003cp\u003eResponsiveness became an equity issue as well as a design convenience. I adopted a mobile-first approach and designed content blocks that scale gracefully from small screens to large desktops. Tables were wrapped with responsive containers; long lines were broken into scannable chunks; headings carried meaningful hierarchy so readers could skim effectively. I used CSS to create a coherent visual system - consistent spacing, typographic rhythm, cards with gentle elevation, and clear affordances for links and buttons - so learners could transfer knowledge of one section\u0027s patterns to another.\u003c/p\u003e\n\u003cp\u003eCrucially, I learned to test with real users. Short sessions with students revealed friction points that I would have missed on my own: ambiguous labels, buttons that looked like headings, and instructions that were too dense. I iterated by clarifying link labels (\"Apply for Scholarships\" instead of \"Click here\"), adding microcopy at critical decision points, and reorganizing pages to match the way students actually searched for information. In a few cases, small layout changes - like moving deadlines into a consistent right-hand column and tightening line lengths - reduced confusion and cut time-on-task.\u003c/p\u003e\n\u003cp\u003eI also invested in maintainability. I documented a small design system for the portfolio, including tokens (colors, spacing, typography) and reusable components (navigation, callouts, tables). That system reduces rework when I add artifacts or revise content. Print styles ensure that key pages remain usable offline. I added basic performance considerations - compressing images, deferring noncritical assets - so pages load quickly on school Wi-Fi and cellular networks. These improvements are largely invisible to end users but contribute to a smoother experience.\u003c/p\u003e\n\u003cp\u003eThe impact shows up in student behavior and counselor workflow. Students send fewer \"Where do I find...?\" emails and complete more steps without one-on-one coaching. Counselors can link confidently to pages knowing the structure, labels, and accessibility are consistent. Most importantly, the site now reflects the values I hold as an educator: clarity, inclusion, and respect for learners\u0027 time and context.\u003c/p\u003e\n\u003cp\u003eRepresentative artifact: the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, which demonstrates semantic structure, improved scannability, accessible color contrast, descriptive links, and responsive layouts. The lessons from IS_LT 7355 underpin the rest of my portfolio and continue to guide decisions as I iterate.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7383\u003c/strong\u003e\u003cbr/\u003eRapid Development Tools\u003c/td\u003e\n\u003ctd\u003eLow-code tools and rapid prototyping for instructional products.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course reframed my development process from \"complete the product, then gather feedback\" to \"ship a slice, learn, and iterate.\" I adopted a build-measure-learn loop that emphasized quick cycles and concrete evidence. Rather than polishing a full module, I shared small increments - an intro video, a single practice task, a draft visual - and asked students to interact with them. Their responses immediately revealed what resonated and what needed revision, allowing me to redirect effort to the highest-impact changes.\u003c/p\u003e\n\u003cp\u003eLow-code tools accelerated this cadence. Templates, visual editors, and simple data integrations let me prototype without getting bogged down in implementation details. I storyboarded sequences, produced lightweight media, and assembled resources into coherent flows that students could try within a single class period. Because the artifacts were quick to build, they were also quick to change. When analytics and reflections showed that a prompt was unclear or a graphic was distracting, I revised the asset within hours, not days.\u003c/p\u003e\n\u003cp\u003eEvidence gathering was intentionally lightweight and ethical. I favored aggregate indicators - task completion rates, time-on-task, quick pulse checks - over invasive tracking. I paired these with short, structured reflections: one or two questions that helped me understand confusion points and cognitive load without overburdening students. This combination gave me direction while respecting privacy and class time. Over multiple iterations, completion improved and questions shifted from \"What do I do?\" to \"Why does this pattern appear?\" - a sign that the scaffolds were doing their job.\u003c/p\u003e\n\u003cp\u003eOne representative example is the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial Markets Slide Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Early drafts tried to cover too much content per slide, leading to cognitive overload. Student feedback prompted me to reduce text density, pace narration, and use consistent visual metaphors. I aligned each segment with a single learning objective and added brief checks for understanding to surface misconceptions in real time. The result was higher engagement and clearer discussions about cause and effect in market behavior.\u003c/p\u003e\n\u003cp\u003eAnother thread involved collaborative analysis using Python and Google Sheets. Students selected variables, co-constructed datasets, and iterated on visualizations as claims evolved. My role shifted from content deliverer to facilitator of inquiry: I provided exemplars, templates, and nudges, while learners explored multiple pathways to a solution. Rapid iteration - both in the artifacts and in the procedures - helped sustain momentum and made successes visible to the group.\u003c/p\u003e\n\u003cp\u003eJust as important were the pivots. Not every idea worked. Some interactive elements distracted more than they helped; a few templates were too rigid for diverse contexts. Because I was working in small slices, it was easy to remove, replace, or simplify without derailing progress. Those moments reinforced a key lesson: iteration is not a sign of failure but a mechanism for learning for both teacher and students.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, rapid development practices reduced time to value and increased alignment between design intent and learner experience. Students spent more time wrestling with ideas and less time deciphering instructions. For me, the process provided a repeatable, humane way to improve materials: small bets, honest evidence, and steady refinement. See the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e for additional examples and iteration notes that document this approach in practice.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach instructional design. I no longer see myself as a content expert, but rather as a facilitator of learning. I understand that my role is not to create perfect materials, but to create materials that are good enough to spark meaningful learning. I have learned to trust the process of iteration and to see it as a natural part of the design cycle.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more willing to take risks and try new things, knowing that I can always iterate and improve. I am more focused on creating materials that are accessible and inclusive, and I am more intentional about gathering feedback from students and using it to inform my design decisions.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of empathy in instructional design. I have learned to see things from the student\u0027s perspective and to design materials that meet their needs. I have also learned to be more mindful of the emotional and social aspects of learning, and to create materials that are engaging and motivating.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of instructional design and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9410\u003c/strong\u003e\u003cbr/\u003ePython Seminar: Data Pipelines \u0026amp; Dashboards\u003c/td\u003e\n\u003ctd\u003eHands-on Python coding course focused on automating data ingestion, cleaning, and visualization for learning analytics dashboards.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eUsing Python as the core toolset, I built reusable Colab notebooks that authenticate to APIs, pull live CSV extracts, and normalize the data with pandas so counselors receive clean, analysable tables without manual effort.\u003c/p\u003e\n\u003cp\u003eI refactored pipelines into modular functions for requests, validation, and storage, which now keep the Student Risk dashboard, census demographic summaries, and transcript audits up to date with a single run.\u003c/p\u003e\n\u003cp\u003eThe course deepened my proficiency with pandas, requests, and plotting libraries, culminating in automated heatmaps, transcript evaluations, and intake dashboards that anchor the coding artifacts in this portfolio.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9417\u003c/strong\u003e\u003cbr/\u003eAction Research\u003c/td\u003e\n\u003ctd\u003eInquiry cycles to investigate and improve practice.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course transformed reflective practice into systematic inquiry. I framed a practical, context-relevant question - would simplifying the scholarship site\u0027s information architecture improve student follow-through from discovery to application? - and designed a modest study to investigate it. I began with a baseline map of the existing journey, identifying the typical paths students took and where drop-offs occurred. I then documented specific changes (clearer labels, consistent placement of deadlines, shorter instructions, and a persistent \"Apply\" affordance) and predicted how each might reduce friction.\u003c/p\u003e\n\u003cp\u003eI collected evidence with an eye toward proportionality and feasibility. Rather than implementing heavy instrumentation, I tracked a handful of indicators: page visits to key resources, clicks on application links, time-on-task for guidance pages, and the number of initiated applications within a set period. I paired these with short student reflections gathered at natural checkpoints - two or three questions about clarity, confidence, and what they expected to do next. This mixed-methods approach allowed me to see both movement in the numbers and the reasons behind it.\u003c/p\u003e\n\u003cp\u003eAnalysis was iterative. Early results showed more visits to deadlines pages but no corresponding increase in application starts. Student comments revealed that while deadlines were clearer, they still felt overwhelmed by the number of steps. In response, I consolidated instructions into checklists, added concise summaries at the top of longer pages, and created a lightweight \"start here\" page that guided students through the first two actions. The next cycle showed improvement: students reached application pages faster and reported greater confidence about what to do.\u003c/p\u003e\n\u003cp\u003eI also explored communication nudges outside the site. Short, targeted reminders in newsletters - linked to specific actions rather than general announcements - helped maintain momentum. Importantly, I tracked unsubscribes and opt-outs to ensure we were not adding noise. The goal was to support, not pester. These outreach experiments were documented alongside site changes, creating a fuller picture of the ecosystem that influences student behavior.\u003c/p\u003e\n\u003cp\u003eThroughout the study, I attended to ethics and participant burden. All surveys were brief and optional, data were aggregated for reporting, and I avoided collecting sensitive information. Findings were shared back with students and counselors in plain terms, including what we tried that did not make a difference. That transparency built trust and generated better ideas for subsequent cycles.\u003c/p\u003e\n\u003cp\u003eThe most valuable outcome was not a single optimization but a repeatable process. I now approach improvements using small, testable changes, simple measures aligned to the behavior I care about, and short cycles that let me adapt quickly. This stance has spilled over into other projects, from curricular materials to advising workflows. Action research gave me the structure to learn from my own context without waiting for perfect studies or exhaustive datasets.\u003c/p\u003e\n\u003cp\u003eArtifacts across the portfolio - particularly the navigation revisions and microcopy updates in the scholarship resources - trace this inquiry process and the rationale behind each change. The habit of documenting what I expected to happen, what occurred, and what I will try next continues to guide my practice.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9455\u003c/strong\u003e\u003cbr/\u003eDesign Thinking Evaluation\u003c/td\u003e\n\u003ctd\u003eHuman-centered evaluation approaches for learning experiences.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eIn this course, I learned to evaluate learning experiences with empathy and rigor. We began with foundational questions: Who are the learners? What are they trying to accomplish? Where and how will they engage with the materials? These questions anchored every method we used, from heuristic evaluations to think-alouds. I practiced observing without leading, listening for moments of hesitation, and distinguishing between true usability issues and individual preferences. This mindset helped me see barriers I had normalized over time.\u003c/p\u003e\n\u003cp\u003eMy focal project was an evaluation of scholarship resources. I constructed representative tasks (e.g., identify eligibility, locate deadlines, begin an application) and recruited a small, diverse set of students to complete them on their own devices. During think-alouds, I noted places where instructions felt dense, labels were ambiguous, or layout implied the wrong hierarchy. I triangulated these findings with a quick heuristic review focused on consistency, recognition over recall, error prevention, and help and documentation. The convergence of observations and heuristics made it clear where to act first.\u003c/p\u003e\n\u003cp\u003eRevisions prioritized clarity and momentum. I chunked instructions, front-loaded essential information, and used verbs in headings to set expectations (\"Submit FAFSA\" \"Request Transcript\"). I standardized the placement of deadlines and calls-to-action, improved contrast for key buttons, and ensured that the primary path remained visible on mobile screens. Microcopy addressed common points of confusion with short, supportive prompts. I also added small success moments - checkmarks and confirmations - to reinforce progress.\u003c/p\u003e\n\u003cp\u003eMeasuring impact required balancing thoroughness with practicality. I compared task completion times and error rates before and after revisions, and I gathered brief post-task reflections on ease and confidence. The improvements were notable: students reached application links faster, backtracked less, and reported feeling more certain about next steps. Importantly, the changes held up across devices with different screen sizes and input methods.\u003c/p\u003e\n\u003cp\u003eBeyond the immediate project, I developed patterns for scalable evaluation. I created short protocol templates for think-alouds and heuristic reviews, along with a lightweight issue tracker that categorizes findings by severity and effort. These artifacts help me sustain evaluation as an ongoing practice rather than a one-time event. I also learned to frame findings constructively with stakeholders - pairing evidence of friction with concrete, feasible recommendations - so teams feel empowered to act.\u003c/p\u003e\n\u003cp\u003eThe course reinforced that evaluation is part of design, not an afterthought. By building evaluation into the cadence of development, I\u0027m more likely to catch issues before they compound and to keep the learner experience at the center. The strategies I practiced here now inform my work across the portfolio: clear goals, representative tasks, small tests, and respectful observation that leads to actionable insights.\u003c/p\u003e\n\u003cp\u003eEvidence of these changes appears across my artifacts, including the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003eRapid Development\u003c/a\u003e work and the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, where evaluation shaped sequencing, instructions, and the balance between guidance and autonomy.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9466\u003c/strong\u003e\u003cbr/\u003eLearning Analytics\u003c/td\u003e\n\u003ctd\u003eData collection, analysis, and interpretation to improve learning.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me separate curiosity from usefulness when it comes to data. I began by articulating the questions I actually needed to answer - Are students getting stuck on specific steps? Which explanations reduce confusion? Where does motivation flag? - and only then selecting indicators that could reasonably inform those questions. I resisted the temptation to instrument everything and focused instead on a small set of measures aligned to outcomes: task completion, time-on-task for key pages, error rates, and short reflections that captured perceived clarity and confidence.\u003c/p\u003e\n\u003cp\u003eDesigning the pipeline required attention to proportionality and privacy. I avoided collecting sensitive data that were not essential to learning goals, and I aggregated results whenever possible. For example, rather than logging every click, I tracked transitions between meaningful states (e.g., from \"view deadlines, to \"start application,). I documented retention windows and access controls and communicated in plain language what was being collected and why. This transparency increased trust and reduced anxiety for students and families accustomed to opaque data practices.\u003c/p\u003e\n\u003cp\u003eInterpreting analytics demanded humility. Time-on-task can reflect deep engagement or confusion; completion rates might improve because instructions are clearer or because learners are skipping important steps. To avoid false certainty, I paired quantitative indicators with quick qualitative checks - two-question pulses at natural breakpoints and occasional think-alouds with volunteers. These mixed methods helped me surface causal mechanisms and prioritize changes with the greatest impact on understanding and persistence.\u003c/p\u003e\n\u003cp\u003eWith these practices in place, I used analytics to drive specific improvements. When I noticed extended dwell times on a guidance page with low subsequent application starts, I rewrote the first two paragraphs to front-load purpose and reduced reading burden by chunking steps. I also added a short checklist and a \"What you\"ll need, callout. In the next cycle, dwell time decreased and application starts rose, with student reflections citing \"clearer next steps, as the main factor. In another case, I discovered that mobile users abandoned a long form more frequently; responsive tweaks and a save-and-return affordance reduced drop-offs.\u003c/p\u003e\n\u003cp\u003ePerhaps the most significant lesson was to keep analytics humane. Dashboards were designed for action, not for surveillance: simple visualizations tied to specific decisions, like which pages needed revision or where to place a clarifying prompt. I also built in guardrails - reminders about interpretation limits and links to the evidence behind a proposed change - so conversations stayed grounded. By closing the loop from evidence to iteration and back again, I created a sustainable rhythm of improvement that respected learners\" privacy and time.\u003c/p\u003e\n\u003cp\u003eThe approach now permeates my portfolio. From the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e to scholarship resources, analytics serve design intent: to clarify, to support, and to empower. The course shifted my mindset from \"more data is better, to \"the right data, used ethically, makes learning better.,\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9467\u003c/strong\u003e\u003cbr/\u003eTechnology to Enhance Learning\u003c/td\u003e\n\u003ctd\u003eTechnology integration strategies for impact and equity.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me to develop a deeper understanding of how technology can be used to enhance learning. I learned about the different types of technology integration, including substitution, augmentation, modification, and redefinition. I also learned about the importance of considering the SAMR model when designing technology-enhanced lessons.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of equity in technology integration. I learned about the digital divide and how it can impact student learning. I also learned about the importance of providing access to technology for all students, regardless of their background or socioeconomic status.\u003c/p\u003e\n\u003cp\u003eI also learned about the different types of technology that can be used to enhance learning, including learning management systems, educational software, and multimedia tools. I learned about the importance of selecting technology that is aligned with the learning objectives and that is accessible to all students.\u003c/p\u003e\n\u003cp\u003eThroughout the course, I had the opportunity to design and implement technology-enhanced lessons. I learned about the importance of considering the instructional design model when designing lessons, and I learned about the different types of instructional strategies that can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOne of the most significant challenges I faced in this course was learning to navigate the different types of technology. I had to learn how to use new software and tools, and I had to learn how to troubleshoot technical issues. However, with the support of my instructor and my peers, I was able to overcome these challenges and develop a deeper understanding of how technology can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOverall, this course was incredibly valuable. I learned about the importance of technology integration in education, and I developed a deeper understanding of how technology can be used to enhance learning. I also developed a range of skills, including instructional design, technology integration, and troubleshooting.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach teaching and learning. I no longer see technology as a separate entity from instruction, but rather as an integral part of the learning process. I understand that technology can be used to enhance learning, and I am committed to using it in a way that is equitable and accessible to all students.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more intentional about using technology to enhance learning, and I am more focused on creating lessons that are accessible and inclusive. I am also more willing to take risks and try new things, knowing that I can always iterate and improve.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of ongoing professional development. I learned about the importance of staying current with the latest research and trends in technology integration, and I learned about the importance of seeking out opportunities for professional growth and development.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of technology integration and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9471\u003c/strong\u003e\u003cbr/\u003eInstructional Systems Design\u003c/td\u003e\n\u003ctd\u003eSystematic analysis, design, development, and evaluation.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course operationalized ADDIE in my day-to-day work. In Analysis, I clarified the problem space with stakeholders - counselors, students, and families - using interviews, journey mapping, and quick artifact audits. We defined the gap in terms of learner performance and experience, not just content coverage. From there, I wrote measurable objectives that described what learners should be able to do and under what conditions, aligning them to authentic tasks they actually encounter.\u003c/p\u003e\n\u003cp\u003eDuring Design, I translated objectives into assessments and learning activities. I favored performance tasks over recall, and I arranged sequences to activate prior knowledge, reduce extraneous load, and support transfer. I made accessibility a non-negotiable constraint, specifying patterns (heading hierarchy, alt text, focus states), contrast thresholds, and mobile-first layouts. Prototypes at this stage were intentionally low-fidelity so I could iterate quickly based on feedback.\u003c/p\u003e\n\u003cp\u003eDevelopment focused on turning designs into maintainable artifacts. I built reusable components and wrote microcopy that anticipated sticking points. I organized assets for traceability and reuse, and documented decisions so collaborators could pick up where I left off. Throughout, I validated materials with quick checks - does the assessment truly measure the objective? Does the activity provide the right level of support? - and I fixed mismatches before they calcified.\u003c/p\u003e\n\u003cp\u003eImplementation emphasized readiness. I created facilitator notes, checklists, and support guides, and I rehearsed delivery with colleagues to catch practical issues. I monitored early sessions for surprises and captured them as candidates for the next iteration. Importantly, I framed changes as part of the process, not as evidence that something had \"failed\" which encouraged honest feedback and steady improvement.\u003c/p\u003e\n\u003cp\u003eEvaluation ran through everything, not just the end. I combined formative evidence (observations, quick polls, error patterns) with summative indicators (completion, performance against rubrics) to judge effectiveness. When evidence suggested a misalignment, say, learners succeeded on a task but could not transfer the skill, I revisited objectives and activities to tighten the link. I also reported findings in concise, actionable summaries to stakeholders so decisions about what to keep, revise, or retire were grounded in shared understanding.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, this disciplined approach improved coherence and outcomes. Artifacts like the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e show objectives, tasks, and assessments pulling in the same direction, while the rapid prototypes demonstrate how early evaluation prevents costly rework. ADDIE is no longer an abstract model for me; it is a practical rhythm for designing learning that respects people\u0027s time and leads to measurable, equitable gains.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9473\u003c/strong\u003e\u003cbr/\u003eProject Management\u003c/td\u003e\n\u003ctd\u003ePlanning, execution, risk, and stakeholder communication.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course provided the scaffolding to deliver learning products predictably and sustainably. I learned to define scope crisply - what is in, what is out, and why - and to translate that scope into milestones with observable outcomes. I time-boxed work, used Kanban-style boards to visualize flow, and limited work in progress so quality did not suffer. These habits reduced context switching and created momentum without requiring heroic efforts.\u003c/p\u003e\n\u003cp\u003eRisk management became a routine, not a formality. I identified technical, content, and schedule risks early and documented triggers, likelihood, and mitigations in a simple log. For web artifacts, recurring risks like link rot and browser quirks received standing checks: automated link health scans, manual spot checks on common devices, and a brief checklist for major releases. By making risk visible, I could negotiate trade-offs honestly with stakeholders - choosing to descale a feature, for instance, to protect a deadline and quality bar.\u003c/p\u003e\n\u003cp\u003eCommunication was another pillar. I wrote succinct status updates that focused on decisions, risks, and next steps rather than play-by-play activity. I set expectations for feedback windows and established one \"source of truth\" for assets to avoid version sprawl. Retrospectives closed each cycle, capturing what worked, what was painful, and what we would try next. Over time, these rituals built trust and reduced surprises.\u003c/p\u003e\n\u003cp\u003eResourcing and scheduling were treated as design problems. I estimated effort using comparative sizing rather than false precision, grouped similar tasks to gain efficiencies, and reserved capacity for the inevitable unknowns. I also created templates - issue trackers, release notes, change logs - that shortened ramp-up for collaborators and preserved context for future revisions.\u003c/p\u003e\n\u003cp\u003eThe payoff is evident across my portfolio. Releases are steadier, quality is more consistent, and maintenance overhead is lower. Students and colleagues benefit from reliable updates and clear communication; I benefit from a process that is humane and sustainable. Project management did not make the work less complex, but it made the path through that complexity clearer and fairer for everyone involved.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9474\u003c/strong\u003e\u003cbr/\u003eFront End Analysis\u003c/td\u003e\n\u003ctd\u003eNeeds assessment and problem framing techniques.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course taught me to slow down and frame the right problem before building solutions. I mapped the ecosystem around my learners - students, families, counselors, platforms - and gathered perspectives through interviews, quick surveys, and artifact reviews. I resisted the urge to jump to features and instead synthesized what I heard into jobs-to-be-done and pain points. Journey mapping exposed where energy was lost: unclear eligibility, scattered deadlines, instructions that assumed background knowledge students did not yet have.\u003c/p\u003e\n\u003cp\u003eFrom this analysis, I wrote problem statements that described the gap in observable terms and identified constraints we had to honor (devices, time, bandwidth, privacy). I evaluated candidate solutions against criteria grounded in this reality: Does the change reduce steps? Does it improve comprehension at the moment of decision? Is it sustainable for counselors to maintain? This filtered out ideas that were flashy but fragile and surfaced simpler interventions with outsized impact - clearer labels, checklists, consistent placement of critical information, and targeted microcopy.\u003c/p\u003e\n\u003cp\u003eI also honed techniques for gathering just enough data to move forward. Lightweight card sorts clarified language; five-minute usability checks with students revealed misleading layouts; quick analytics confirmed whether changes produced the expected behavior. Importantly, I documented not only what we learned but what we chose not to do and why, preserving rationale for future teams and preventing drift.\u003c/p\u003e\n\u003cp\u003eFront-end analysis improved alignment across stakeholders. With a shared understanding of the problem and the criteria for success, conversations shifted from personal preference to learner impact. That alignment made subsequent design and development faster and less contentious. It also made evaluation clearer: we knew what we intended to change and how we would know if it worked.\u003c/p\u003e\n\u003cp\u003eUltimately, this course embedded a habit of inquiry that now precedes major changes in my portfolio. I approach new challenges with curiosity and discipline, confident that the time invested up front will pay dividends in clarity, quality, and equity for the learners I serve.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e"},"reflection":{"id":"reflection","title":"Reflection Statement","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eReflection Statement\u003c/h2\u003e\n\u003ch3\u003eContext\u003c/h3\u003e\n\u003cp\u003eWhen I entered the ISLT program, I brought a counselorâ€™s mindset and a financierâ€™s analytical discipline, but my technology practice was largely pragmatic. In daily school work, I made tools â€œwork well enoughâ€ to meet immediate student needs , sharing resources, simplifying instructions, and troubleshooting access barriers. What I lacked was a systematic framework for designing technology-enhanced learning experiences end to end: aligning needs to outcomes, selecting tools intentionally, measuring impact, and then iterating for equity and clarity. I chose the ISLT program, Technology in Schools emphasis, to build that structure around my practice. I wanted to develop reusable patterns for accessible web design, rapid prototyping, and responsible use of learning data that could scale beyond individual interventions. Entering the program, I was comfortable with quantitative reasoning and stakeholder communication from my finance background and counseling experience, but I needed deeper knowledge in interaction design, evaluation, and standards-based technology integration. The ISLT coursework challenged me to move from ad hoc solutions to an evidence-driven, ethically grounded design approach, one that serves students and families across devices, bandwidth, and contexts while protecting their privacy and honoring their time.\u003c/p\u003e\n\u003ch3\u003eSignificant Learnings\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccessibility and clarity are design requirements, not afterthoughts.\u003c/strong\u003e Across projects, I learned that semantic HTML, WCAG-aligned color and typography choices, and mobile-first layouts are not just technical niceties; they directly reduce cognitive load and make resources more actionable for students and families. By treating headings, lists, labels, and link text as navigational cues, especially for screen readers and small screens, I improved findability and comprehension. This shift changed how I write microcopy, structure pages, and audit contrast. It also made my resources more equitable for learners who access materials on mobile devices or with limited bandwidth.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e. This curriculum showcases consistent semantic structure, clear task chunking, and high-contrast, readable typography. It demonstrates how accessible patterns improve the student experience without sacrificing visual appeal.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRapid prototyping accelerates learning and quality.\u003c/strong\u003e The program taught me to plan short, focused build cycles with quick feedback loops, using low-code tools when appropriate. By putting workable prototypes in front of students and colleagues early, I surfaced misconceptions and usability issues before they became entrenched. I adopted a cadence of design sprints, check-ins, and micro-evaluations that helped me converge on effective media sequencing and interaction patterns faster than when I built â€œfinishedâ€ products up front. Rapid development also fostered a culture of co-design, students felt comfortable giving candid feedback because iteration was expected.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e. This portfolio collects prototypes that moved from concept to testable artifacts quickly, documenting how feedback informed successive versions and how scope discipline produced better outcomes in less time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData-informed decision-making strengthens instructional impact.\u003c/strong\u003e I learned to select practical, ethical indicators tied to objectives, click-through paths, time on task, completion and error rates, and rubric-aligned performance, so that I could close the loop between design and evidence. Rather than collecting data for its own sake, I now plan the question first, identify the smallest useful signals, and then adjust the resource or activity accordingly. This approach keeps measurement purposeful, protects student privacy, and leads to visible improvements in navigation and comprehension.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Iterations of this multimedia lesson were guided by lightweight analytics and formative checks (e.g., sequence adjustments based on confusion points and pacing). The result is a clearer narrative flow and more targeted prompts that support transfer.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eImpact on Practice and Growth toward ISTE Educator Standards\u003c/h3\u003e\n\u003cp\u003eThe ISLT program has reshaped how I design, facilitate, and evaluate learning experiences. In daily practice, I now begin with learner context and constraints, articulate measurable outcomes, and choose tools that minimize barriers. Prototyping early and often has become standard, and I treat accessibility reviews and contrast checks as nonnegotiable gates for release. My communication artifacts, web pages, multimedia sequences, and counselor-facing resources, are more concise, scannable, and consistent, which has reduced misinterpretation and follow-up overhead for students and families. Equally important, my data practices are lean and purposeful: I gather only what I need, explain how it will be used, and then show stakeholders the improvements it enables.\u003c/p\u003e\n\u003cp\u003eThis work aligns with and advances my growth in the ISTE Educator Standards. As a \u003cstrong\u003eDesigner\u003c/strong\u003e (5aâ€“5c), I build accessible, learner-centered materials with authentic, goal-aligned activities. As a \u003cstrong\u003eFacilitator\u003c/strong\u003e (6aâ€“6d), I create conditions for ownership, using technology to enable exploration, feedback, and creativity while managing tools and strategies responsibly. As an \u003cstrong\u003eAnalyst\u003c/strong\u003e (7aâ€“7c), I employ alternative demonstrations of learning and use assessment data appropriately to inform next steps without over-collecting or compromising privacy. I have also grown as a \u003cstrong\u003eCollaborator\u003c/strong\u003e (4b) by co-learning with students through iterative prototyping, as a \u003cstrong\u003eLeader\u003c/strong\u003e (2b) by advocating equitable access through mobile-first, high-contrast designs, and as a \u003cstrong\u003eCitizen\u003c/strong\u003e (3c) by modeling safe, ethical use of data. Together, these shifts mean I am not merely curating resources but designing learning experiences that are equitable, measurable, and sustainable in a real school setting.\u003c/p\u003e\n\u003cp\u003eGoing forward, I will continue to maintain a reusable design system (components, tokens, and patterns) across counselor resources, expand quick evaluation toolkits to include student-friendly reflection prompts, and formalize lightweight analytics dashboards that respect privacy. Most importantly, I will keep centering accessibility and student voice, ensuring that the artifacts I produce help students navigate opportunities confidently, make informed decisions, and demonstrate their learning in multiple ways.\u003c/p\u003e\n\u003c/div\u003e"},"program-goals":{"id":"program-goals","title":"Program Goals \u0026 Outcomes","html_content":"\u003csection id=\"program-goals\"\u003e\n\u003ch2\u003eLearning Technologies Program Goals and Student Learning Outcomes\u003c/h2\u003e\n\u003cp\u003eThe Learning Technologies (LTD) program prepares students to systematically design, develop, evaluate, and lead learning technology and instructional solutions within schools, universities, corporations, and public and private organizations. For me, this program has been the bridge between my day to day work as a high school counselor and data systems designer in Saint Louis Public Schools and my longer term goal of pursuing a PhD that examines how real time feedback, interoperable data systems, and equitable analytics can change students\u0027 postsecondary trajectories.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eGoal 1:\u003c/strong\u003e Students will develop theory and research-based skills for innovative, aesthetic, accessible, equitable, effective and sustainable design and development of technologies for learning opportunities and systems.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.1:\u003c/strong\u003e Students will design and develop learning and performance opportunities and systems including direct instruction, student-centered learning, collaborative work, and performance support.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.2:\u003c/strong\u003e Students will design and develop learning opportunities and systems for meaningful learning; promote student engagement in online learning environments; and select appropriate technology and learning objects to support learners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.3:\u003c/strong\u003e Students can conduct project management activities to support the total lifecycle of design, development and implementation of learning opportunities and systems.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.4:\u003c/strong\u003e Students will demonstrate mastery of technological production skills in web development, digital asset creation, and other applicable development skills necessary for the creation of learning systems (for example, web page prototyping, game design, digital media).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003eGoal 2:\u003c/strong\u003e Students will learn to apply systematic methods for analyzing instructional or training needs, applying those results to instructional design and development, and collecting and analyzing formative and summative evaluation data to improve the instructional product.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 2.1:\u003c/strong\u003e Students can choose and conduct appropriate analytical methods and apply the results to the systematic design and development of learning environments and systems.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 2.2:\u003c/strong\u003e Students will design assessments aligned with the intended outcomes of the learning environments, choose and implement appropriate methods to conduct formative and summative evaluation of the learning environment, and use data to continuously improve the instructional product or products.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003eGoal 3:\u003c/strong\u003e Students will understand the impact of technologies we create or use on society at large and strive to produce ethical, inclusive, and equitable instructional systems and technologies.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 3.1:\u003c/strong\u003e Students will demonstrate ethical and inclusive practices to all aspects of analytical data collection and analysis methods used in the design and evaluation of instructional products or learning environments (for example, sampling methods).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 3.2:\u003c/strong\u003e Students will design and implement instructional products and learning environments based upon Universal Design Principles.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 1.1\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and develop learning and performance opportunities and systems including direct instruction, student-centered learning, collaborative work, and performance support.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBefore entering the EdS program, I understood instruction primarily through the lens of one-on-one counseling conversations. I could guide a student through a FAFSA form or explain the difference between subsidized and unsubsidized loans, but I had not conceptualized how to design systems that could deliver such guidance at scale or across modalities. The Design of Learning Environments course fundamentally changed this. It introduced me to cognitive apprenticeship, distributed expertise, and problem-centered design, frameworks that helped me see that effective learning systems must intentionally layer direct instruction, exploration, collaboration, and just-in-time support rather than treating them as competing approaches.\u003c/p\u003e\n\u003cp\u003eThis shift became concrete when I developed the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning module\u003c/a\u003e. I designed the module to sequence direct instruction about source credibility with student-centered scenarios where learners practice evaluating real websites on their own. Rather than lecturing about misinformation and hoping students would apply it later, I built in immediate practice opportunities with authentic examples. The module also includes performance support elements, quick-reference guides and decision trees, that students can use beyond the learning context. This design reflects my new understanding that learning environments should anticipate the full arc from initial exposure through independent application.\u003c/p\u003e\n\u003cp\u003eMy work at SLPS reinforced this integrated approach. When I redesigned counselor-facing dashboards, I applied cognitive apprenticeship structures: modeling how to interpret risk indicators, scaffolding the decision process with guiding questions, and gradually releasing responsibility as counselors gained confidence. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part2_part2.html\" target=\"_blank\" rel=\"noopener\"\u003ePython Code Simulation\u003c/a\u003e embodies this same philosophy in a different domain. It provides scaffolded examples that anchor learners while inviting them to manipulate parameters and observe changing outputs. Students interact with a dynamic visualization that models instructional scenarios, and the simulation serves as both a learning tool and a performance support system they can return to when applying concepts in their own work.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part1_part8.html\" target=\"_blank\" rel=\"noopener\"\u003eLinear Regression Manipulative\u003c/a\u003e extends this philosophy into collaborative data exploration. I designed it so learners work together to adjust regression parameters, discuss patterns, and test predictions. The manipulative includes performance support features such as visual feedback, constraint hints, and progress indicators that guide collaborative work without replacing the thinking. This artifact emerged directly from my realization that technical systems will fail without accompanying social infrastructure. Counselors adopting my data tools needed collaborative contexts for sense-making, not just interfaces for data retrieval.\u003c/p\u003e\n\u003cp\u003eTogether, these artifacts demonstrate that I now design learning systems that move flexibly across instructional modes in service of deeper understanding. The Digital Literacy module layers direct instruction with student-centered practice; the Python simulation balances guidance with productive struggle; and the regression manipulative supports collaboration while providing just-in-time scaffolding. Each represents a different configuration of the same underlying commitment: learning environments should be designed as integrated ecosystems, not isolated activities.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part2_part2.html\" target=\"_blank\" rel=\"noopener\"\u003ePython Code Simulation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part1_part8.html\" target=\"_blank\" rel=\"noopener\"\u003eLinear Regression Manipulative\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 1.2\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and develop learning opportunities and systems for meaningful learning; promote student engagement in online learning environments; and select appropriate technology and learning objects to support learners.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMeaningful learning has become a central thread tying my EdS work to my emerging research interests in real-time feedback and postsecondary attainment. The Learning Analytics and Data Science for Learning course fundamentally changed how I think about engagement. Before, I evaluated learning experiences by surface indicators: Did students complete the module? Did they click through all the screens? After this course, I began thinking in terms of feedback loops, learner models, and data-informed decision ecologies. Engagement became less about entertainment and more about alignment between structure, context, and learner purpose. I now evaluate my design choices through questions such as: Will this format help learners build a coherent mental model? Does it invite them to make predictions, explain their reasoning, and apply ideas in authentic contexts?\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning module\u003c/a\u003e represents my attempt to answer these questions in practice. I carefully coordinated narration with visuals and used interactive prompts to help learners test their understanding of concepts like liquidity and risk. I selected the slideshow format because it allowed controlled pacing and integration of visual examples with narration, which research suggests supports schema building for complex, unfamiliar content. More importantly, the module includes embedded formative checks that surface misconceptions in real time rather than waiting for a summative assessment at the end. This design reflects my conviction, sharpened by the Learning Analytics course, that delayed feedback disproportionately harms learners who lack external support structures.\u003c/p\u003e\n\u003cp\u003eThis conviction connects directly to my SLPS work. I realized that counselors were making decisions with data delayed by weeks, which created avoidable student failure points. A student whose grades were declining in October might not appear on a counselor\u0027s radar until December, by which time recovery options had narrowed dramatically. This insight, that latency in feedback systems has equity implications, now pervades how I design both classroom-facing materials and the interfaces that support counselors and students navigating high-stakes decisions.\u003c/p\u003e\n\u003cp\u003eAt the program level, I applied the same principles to my \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e section. I designed an information structure that allows readers to scan for high-level themes while still accessing detailed reflections and course-level evidence when they choose to dive deeper. The interactive table with expandable reflections encourages exploration and self-assessment. I selected collapsible content as the technology because it reduces cognitive load while keeping detailed information accessible, demonstrating thoughtful technology selection aligned with learner needs rather than decorative complexity.\u003c/p\u003e\n\u003cp\u003eMy \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#reflection\" target=\"_blank\" rel=\"noopener\"\u003eReflection Statement\u003c/a\u003e models meaningful learning and engagement for practitioners by demonstrating how structured reflection can synthesize disparate experiences into an integrated professional identity. The structured format of context, significant learnings, and growth narrative scaffolds the reflective process that promotes deep engagement. Through these designs, I have internalized a key insight from the program: meaningful learning is not about making content enjoyable but about designing structures that connect new information to existing knowledge, invite active processing, and provide timely feedback that allows learners to adjust their understanding.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#reflection\" target=\"_blank\" rel=\"noopener\"\u003eReflection Statement\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 1.3\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents can conduct project management activities to support the total lifecycle of design, development and implementation of learning opportunities and systems.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis outcome marks one of the closest intersections between my EdS coursework and my ongoing work in SLPS. Before the program, much of my technology work resembled heroic effort rather than managed projects. I would push a promising idea far enough to solve an immediate problem, then move on without fully systematizing its development, documentation, or sustainability. The Data-Driven Decision-Making and Improvement Science course shifted my thinking from data as reporting to data as disciplined inquiry. It clarified the importance of rapid cycles, change ideas, and user-centered diagnostics. I now use plan-do-study-act reasoning daily when refining data workflows at SLPS, and I treat each instructional system as a full lifecycle project.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html\" target=\"_blank\" rel=\"noopener\"\u003ePortfolio\u003c/a\u003e itself serves as primary evidence of project management mastery. Building this portfolio required scope definition by identifying artifacts for each SLO, milestone planning through content inventory, information architecture, visual design, and implementation phases. It required risk management addressing cross-browser compatibility and mobile responsiveness. When dynamically loaded content broke toggle functionality, I diagnosed the root cause and fixed it within the same sprint, demonstrating agile response to technical issues. The portfolio also required version control using Git, a practice I have extended to all my SLPS data projects.\u003c/p\u003e\n\u003cp\u003eAt SLPS, I have led projects including the districtwide Postsecondary Attainment Infrastructure, which integrates advising workflows, scholarship pipelines, summer program access, and dual credit expansion. This initiative required stakeholder alignment across counselors, administrators, and external partners. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e translates one component of this infrastructure into a reproducible blueprint that describes the workflow, data sources, automation schedule, and maintenance requirements. I learned that tools depending on a single practitioner\u0027s tacit knowledge do not scale. Documentation is not an afterthought but a core project deliverable.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/EverettStuckeyResume%20Data.pdf\" target=\"_blank\" rel=\"noopener\"\u003ePortfolio Resume\u003c/a\u003e demonstrates project management through professional documentation. Creating an effective resume required analyzing stakeholder needs, prioritizing content for limited space with maximum impact, and iterating based on feedback. At a broader scale, my \u003ca href=\"https://everettstuckey.github.io/islt_7310/assets/coverflow/learn_and_earn_diagram.png\" target=\"_blank\" rel=\"noopener\"\u003eInternship Program Data Pipeline\u003c/a\u003e models how I now think about whole lifecycles by mapping student applications, employer data, and placement tracking into a coherent pipeline supported by Teams and Excel. The diagram itself captures the project\u0027s scope and shows how I communicate technical designs to diverse audiences, a key project management skill.\u003c/p\u003e\n\u003cp\u003eThese projects required iterative design, data cleaning, stakeholder alignment, and constant feedback loops with counselors, administrators, and students. I now think in terms of project charters, milestone planning, dependency mapping, and risk management, even when the product is just a dashboard, an eLearning module, or a Python script. The program helped me acquire both the language and the habits of project management so that my designs can be adopted, maintained, and improved by others long after I move to the next challenge.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html\" target=\"_blank\" rel=\"noopener\"\u003ePortfolio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/EverettStuckeyResume%20Data.pdf\" target=\"_blank\" rel=\"noopener\"\u003ePortfolio Resume\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/assets/coverflow/learn_and_earn_diagram.png\" target=\"_blank\" rel=\"noopener\"\u003eInternship Program Data Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 1.4\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will demonstrate mastery of technological production skills in web development, digital asset creation, and other applicable development skills necessary for the creation of learning systems (for example, web page prototyping, game design, digital media).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis outcome has been critical for building my identity as a scholar-practitioner who can both theorize and produce. The EdS program required me to work across a range of production environments: HTML, CSS, JavaScript, Python, multimedia editors, GIS tools, and data visualization platforms. Over time, I began to see these not as disconnected skill sets but as a shared language for expressing learning designs in concrete, testable forms. My \u003ca href=\"https://everettstuckey.github.io/islt_7310/\" target=\"_blank\" rel=\"noopener\"\u003eProfessional Webpage\u003c/a\u003e demonstrates this integration: a fully hand-coded site that combines responsive layout, accessible navigation, and a coherent visual identity to communicate my work as an EdS candidate and district leader.\u003c/p\u003e\n\u003cp\u003eDeveloping production fluency has shifted how I think about leadership at SLPS. Being able to design and implement a prototype myself changes the conversation with IT staff, principals, and teachers. Instead of describing a desired system in abstract terms, I can show a working version, gather feedback, and iterate. When I proposed the real-time academic monitoring pipeline, I did not submit a requirements document and wait months for IT to build something. I built a working prototype using Python and HTML, demonstrated it to stakeholders, and refined it based on their responses. The \u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard (Python and HTML)\u003c/a\u003e video showcases both the technical implementation and my digital media production skills. Creating this walkthrough required planning narrative pacing, synchronizing screen recordings with voiceover narration, and editing for clarity.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://youtu.be/W4DFjhLx0wY\" target=\"_blank\" rel=\"noopener\"\u003eStates of Matter Animation\u003c/a\u003e extends my skills into motion graphics. This project required understanding keyframe animation, timing curves, and visual storytelling. I designed the animation to support science learning objectives, using color, motion, and pacing to illustrate molecular behavior. Early versions were too fast and cluttered; iteration taught me to use deliberate pacing and clear visual hierarchy. The \u003ca href=\"https://youtu.be/IVYAi723RLM\" target=\"_blank\" rel=\"noopener\"\u003eWeb Scraping Tutorial\u003c/a\u003e demonstrates my ability to create instructional video content for technical skills. I structured the tutorial with clear learning objectives, step-by-step demonstrations, and explanations of underlying concepts. The video shows screen recordings synchronized with narration, code highlighting, and visual annotations, production techniques that support technical learning.\u003c/p\u003e\n\u003cp\u003eMy \u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" target=\"_blank\" rel=\"noopener\"\u003eGIS Map of Career Z and CHIPS CTE Challenge Semifinalists\u003c/a\u003e extends these skills into spatial visualization. This artifact emerged from my realization that enrichment participation disparities are partly a network problem: privileged families have high-bandwidth connections to opportunity nodes while underserved families do not. The map presents geographic patterns in a form that school and district leaders can immediately interpret, supporting my regional summer programs mapping initiative. Across these examples, production is not just technical proficiency; it is a vehicle for making educational ideas tangible and testable. My production skills now allow me to prototype research ideas in the same local systems that shape students\u0027 daily experiences.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/\" target=\"_blank\" rel=\"noopener\"\u003eProfessional Webpage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard (Python and HTML)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/W4DFjhLx0wY\" target=\"_blank\" rel=\"noopener\"\u003eStates of Matter Animation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/IVYAi723RLM\" target=\"_blank\" rel=\"noopener\"\u003eWeb Scraping Tutorial\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" target=\"_blank\" rel=\"noopener\"\u003eGIS Map of Semifinalists\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 2.1\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents can choose and conduct appropriate analytical methods and apply the results to the systematic design and development of learning environments and systems.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMy work in this outcome sits at the heart of my professional trajectory toward learning analytics and educational data science. The Research Methodology and Design course was a turning point because it reframed my understanding of causal inference and internal validity. It helped me recognize why many K-12 improvement initiatives fail: not because they lack merit, but because they lack the methodological infrastructure to demonstrate impact. I learned to think carefully about unit of analysis, sources of bias, and the interpretability demands of the audiences who will use the results. Research concepts like selection bias, treatment heterogeneity, and validity threats now inform how I evaluate external programs and partnerships at SLPS.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/scholarships_data.html\" target=\"_blank\" rel=\"noopener\"\u003eScholarships Database\u003c/a\u003e demonstrates user research applied to system design. I conducted interviews and task analyses with students and counselors, revealing that scholarship access is not just a matter of awareness but an information system and workflow problem. Students could not find opportunities matching their profiles; counselors could not track application status across hundreds of students. Card sorts clarified intuitive labels; usability checks revealed hierarchy problems. These findings directly shaped the searchable, filterable interface I built, which curates over 150 scholarships with encoded eligibility variables and communication workflows.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://colab.research.google.com/drive/1lXuPQLNkrZvcnsP0Y00BZgRuf7tXp1Io?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eHeatmap Visualization Script\u003c/a\u003e demonstrates data analysis for pattern recognition. I chose heatmap visualization because stakeholders needed to identify concentrations and gaps across multiple variables simultaneously. The script applies statistical analysis to transform raw data into visual patterns, then presents findings in formats that support decision-making. Using multilevel thinking from research methods, I designed the visualization to reveal school-level variation that uniform advising policies had obscured.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e demonstrates systematic data collection for needs analysis. I designed the script to pull demographic and socioeconomic data that contextualizes educational outcomes. By automating census data retrieval, I created a reusable analytical tool that supports equity-focused design decisions across multiple projects. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" target=\"_blank\" rel=\"noopener\"\u003eGIS Map of Career Z and CHIPS CTE Challenge Semifinalists\u003c/a\u003e demonstrates spatial analysis applied to educational opportunity. Stakeholder interviews indicated counselors needed geographic understanding of program participation. I applied geocoding and clustering methods to transform semifinalist data into actionable geographic insights for outreach planning.\u003c/p\u003e\n\u003cp\u003eWhen I analyze grade patterns, course enrollments, or scholarship application flows, I now ask not only what is happening but also how we should redesign systems, interfaces, prompts, or supports to respond. That orientation connects directly to my emerging research agenda: how real-time academic and behavioral signals can support equitable decision-making for counselors and teachers, and how data systems can reduce delay between student need and adult response.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/scholarships_data.html\" target=\"_blank\" rel=\"noopener\"\u003eScholarships Database\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1lXuPQLNkrZvcnsP0Y00BZgRuf7tXp1Io?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eHeatmap Visualization Script\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" target=\"_blank\" rel=\"noopener\"\u003eGIS Map of Semifinalists\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 2.2\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design assessments aligned with the intended outcomes of the learning environments, choose and implement appropriate methods to conduct formative and summative evaluation of the learning environment, and use data to continuously improve the instructional product or products.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis outcome has shaped how I think about evidence and improvement, both in digital learning contexts and in district-level advising systems. I entered the program comfortable with high-stakes indicators like ACT scores or graduation rates. The Data-Driven Decision-Making and Improvement Science course helped me learn to design assessments that are much closer to the learning experience, aligned tightly with specific objectives, and useful for continuous improvement rather than only for accountability. It shifted my thinking from data as reporting to data as disciplined inquiry, clarifying the importance of rapid cycles, change ideas, and user-centered diagnostics.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://youtu.be/2Jbn5Da5mYs\" target=\"_blank\" rel=\"noopener\"\u003eExcel Student Risk Dashboard\u003c/a\u003e demonstrates aligned assessment design. I identified the intended outcome, that counselors quickly identify at-risk students, and designed indicators including grade trends, attendance patterns, and credit accumulation that directly predict those outcomes. Applying learning analytics principles, I redesigned the dashboard to emphasize temporal patterns, specifically the slope of decline, rather than static grades. A student with a C who was recently at a B is in a different situation than a student with a C who was recently at a D. The dashboard implements formative evaluation principles by providing timely, actionable feedback that counselors can use to intervene before situations become unrecoverable.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard\u003c/a\u003e shows continuous improvement through iteration. The web-based version evolved from the Excel prototype based on counselor feedback. I simplified displays, added risk-level badges, and created expandable views. Each change addressed specific usability findings gathered through plan-do-study-act cycles. Counselors reported that the original version required too many clicks to reach actionable information; I restructured the hierarchy to surface urgent cases immediately. This iterative refinement demonstrates how I use evaluation data to improve products rather than treating initial designs as final.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e captures the evaluation and improvement process itself. The documentation records design decisions, feedback received, changes implemented, and rationales. It serves as both summative record and guide for future iteration. This transparency enables others to understand and extend the evaluation cycle. The \u003ca href=\"https://colab.research.google.com/drive/1kxPYj4eiCoeiXmsywbf9a1BbNu-m3lTw?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eTranscript Evaluation Script\u003c/a\u003e demonstrates automated assessment aligned with graduation outcomes. I designed the script to evaluate transcripts against credit requirements, course sequences, and pathway completion. The script provides both formative feedback showing current progress and summative evaluation showing pathway completion status.\u003c/p\u003e\n\u003cp\u003eA dashboard or script is not complete until it includes mechanisms for monitoring its own effectiveness and informing its own revision. My commitment to equitable analytics is personal: I work daily with high-achieving, low-income students who lose opportunities not because of ability but because systems fail to detect and respond to early signals. Assessment and evaluation are the mechanisms that make timely response possible.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/2Jbn5Da5mYs\" target=\"_blank\" rel=\"noopener\"\u003eExcel Student Risk Dashboard\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1kxPYj4eiCoeiXmsywbf9a1BbNu-m3lTw?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eTranscript Evaluation Script\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 3.1\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will demonstrate ethical and inclusive practices to all aspects of analytical data collection and analysis methods used in the design and evaluation of instructional products or learning environments (for example, sampling methods).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis outcome speaks directly to the ethical tensions I encounter daily in my SLPS work. Building advisory dashboards, prediction models, or performance reports means deciding what to measure, whom to include, and how to categorize students. The Educational Equity and Policy course deepened my focus on structural barriers in urban districts. It helped me understand why postsecondary gaps persist and why interventions must be designed around systemic friction points rather than student deficits. I now think deliberately about sampling, missingness, representation, fairness, and interpretability whenever I design data collection or analysis methods.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard\u003c/a\u003e required ethical decisions at every stage. I minimized data collection to what is necessary for the advising task, implemented role-based access controls so that counselors see only students in their caseload, and designed the interface to show aggregate patterns before individual records. Risk indicators replace raw data points, reducing granularity to protect privacy while preserving actionability. When I disaggregate outcomes by school, demographic group, or neighborhood, I ask whether the analysis helps adults change their behavior in ways that benefit students, or whether it risks reinforcing deficit narratives.\u003c/p\u003e\n\u003cp\u003eI have become more cautious about which variables I incorporate and how I label risk categories. Early versions of the dashboard used language like high risk and low risk, which counselors reported felt deterministic and potentially stigmatizing. I revised the language to needs attention and on track, framing risk as a prompt for adult action rather than a judgment of student capacity. For sharing and training purposes, I created anonymized versions with sequential aliases for FERPA compliance. I also think about consent and transparency, especially when students\u0027 data are being repurposed across systems like SIS, LMS, and external platforms.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e makes ethical choices transparent. The documentation explains what data is collected and why, access controls, retention policies, and algorithm limitations. I explicitly document that risk scores require human judgment, that the dashboard supports decisions rather than making them. This transparency serves accountability and helps new users understand appropriate use. The \u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e demonstrates inclusive analytical practices. I designed the script to bring demographic context into educational analysis, supporting equity-focused design decisions. The script uses publicly available aggregate data, avoiding individual-level collection, documents data sources transparently, and acknowledges limitations in small-sample estimates.\u003c/p\u003e\n\u003cp\u003eMy emerging research agenda centers on how district ecosystems can be redesigned to embody principles of improvement science and distributed expertise. Central to that agenda is the conviction that learning analytics systems should expand opportunity rather than simply describe inequality. By integrating these ethical considerations into concrete designs, I grow closer to building systems that detect and respond to early signals in ways that serve students who have historically been underserved by delayed, deficit-oriented data practices.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eStudent Learning Outcome 3.2\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and implement instructional products and learning environments based upon Universal Design Principles.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eUniversal Design for Learning (UDL) has become a lens through which I now view almost all of my instructional and advising work. The program helped me move beyond a narrow understanding of accessibility as compliance and into a richer view of UDL as anticipating learner variability in perception, language, executive function, and motivation. That perspective is especially important in SLPS, where students\u0027 access to devices, bandwidth, prior academic preparation, and support outside school varies widely. Building processes that remain usable for non-technical stakeholders has been a persistent challenge, and UDL principles provide the framework for addressing it.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning module\u003c/a\u003e implements all three UDL principles. For multiple means of representation, I present concepts through text, visuals, and interactive scenarios rather than relying on any single modality. For multiple means of engagement, I use relevant scenarios drawn from students\u0027 actual digital lives and offer choice in examples so learners can pursue paths that match their interests. For multiple means of action and expression, learners can demonstrate understanding through writing, verbal explanation, or annotation. This flexibility recognizes that students come to digital literacy with different strengths and that rigid assessment formats can obscure actual understanding.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning module\u003c/a\u003e reflects UDL thinking in its pacing, use of consistent metaphors, and optional review segments that allow learners to revisit complex ideas without penalty. I paced narration to align with visuals, reduced extraneous text that competed for attention, and used consistent metaphors for schema building so that new concepts connect to familiar structures. Transcripts ensure access for those who prefer reading or need accommodations. Quick comprehension checks prompt retrieval without high stakes, allowing learners to gauge their understanding and adjust their approach. These design choices emerged from my recognition that supporting counselors who often lack real-time visibility across systems requires the same flexibility I build into student-facing products.\u003c/p\u003e\n\u003cp\u003eMy \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e section applies UDL principles to information design. I provide multiple means of representation through both table and narrative formats. The collapsible structure offers multiple means of engagement, allowing overview for scanning and detail for deep reading. Clear headings, consistent formatting, and responsive layout ensure the content works across devices and for users with different navigation preferences. When I redesigned counselor-facing dashboards using cognitive apprenticeship structures, I applied these same principles: modeling, scaffolding, gradual release, and multiple entry points for users at different skill levels.\u003c/p\u003e\n\u003cp\u003eUDL is not only for classroom instruction but also for the systems that students and staff use to understand risk, opportunity, and progress. Every data artifact I create is a designed learning environment with affordances, friction points, and implicit theories of action. I now use concepts such as formative feedback, task visibility, and actionable granularity when evaluating or constructing data dashboards, and I treat UDL as foundational to my aspiration to build systems that work for every learner and every practitioner, not just those for whom schools and data systems were originally designed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArtifacts:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/section\u003e"}};