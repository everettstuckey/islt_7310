const PORTFOLIO_SECTIONS = {"at-risk-student-dashboard-webpage":{"id":"at-risk-student-dashboard-webpage","title":"At-Risk Student Dashboard Webpage","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eAt-Risk Student Dashboard Webpage\u003c/h2\u003e\n\u003cp\u003eThe anonymized At-Risk Student Dashboard is rendered as a static HTML export that mirrors the live Python-driven version while safeguarding student privacy. This artifact documents the production-grade visualization that counselors use to triage interventions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDynamic tables update with key market and risk indicators reflecting historical data in the export.\u003c/li\u003e\n\u003cli\u003eJavaScript post-processing replaces names and IDs with sequential aliases to maintain FERPA compliance.\u003c/li\u003e\n\u003cli\u003eA branded cover image introduces the dashboard and aligns with the rest of the portfolio aesthetic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe export references authentic dashboard assets such as \u003ccode\u003eget_atm_iv_ts.png\u003c/code\u003e and \u003ccode\u003efinancial_analysis.png\u003c/code\u003e, demonstrating the breadth of analytics available to staff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAccess the artifact:\u003c/strong\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/output.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard Webpage\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"},"program-of-study":{"id":"program-of-study","title":"Program of Study","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eProgram of Study\u003c/h2\u003e\n\u003cp\u003eThe Program of Study for my ISLT degree includes the courses below. Each course lists the term, a concise description, and my reflection linking outcomes to practice and artifacts where applicable.\u003c/p\u003e\n\u003cdiv class=\"table-wrap\"\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCourse\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003cth\u003eTerm\u003c/th\u003e\n\u003cth\u003eReflection\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7355\u003c/strong\u003e\u003cbr/\u003eWeb Design \u0026amp; Development\u003c/td\u003e\n\u003ctd\u003ePrinciples of accessible, standards-based, responsive web design for learning.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eBefore this course, much of my web work was pragmatic: I could make pages function, but the underlying structure was inconsistent and not always accessible. IS_LT 7355 pushed me to slow down and design deliberately. I learned to treat HTML as more than a way to place content on a screen - it became a semantic contract with assistive technologies and a foundation for predictable, reusable components. I adopted landmarks (header, nav, main, footer), meaningful headings, and descriptive link text to improve navigability for screen reader users and keyboard-only users. The shift from \"it looks right\" to \"it\u0027s structured right\" has had a measurable impact on findability and comprehension.\u003c/p\u003e\n\u003cp\u003eAccessibility moved from a checklist to a design habit. I used WCAG 2.1 AA guidelines to evaluate color contrast and reworked palettes to remain legible in bright light and on older devices. I added a visible skip link, ensured logical tab order, and provided focus states that are easy to see. Images now include alt text that communicates purpose rather than appearance, and icon-only links gained accessible names. On mobile, I removed nonessential flourishes, simplified spacing, and set typography for readability at small sizes. These decisions were not merely technical; they respected the realities of my learners\u0027 devices, bandwidth, and attention.\u003c/p\u003e\n\u003cp\u003eResponsiveness became an equity issue as well as a design convenience. I adopted a mobile-first approach and designed content blocks that scale gracefully from small screens to large desktops. Tables were wrapped with responsive containers; long lines were broken into scannable chunks; headings carried meaningful hierarchy so readers could skim effectively. I used CSS to create a coherent visual system - consistent spacing, typographic rhythm, cards with gentle elevation, and clear affordances for links and buttons - so learners could transfer knowledge of one section\u0027s patterns to another.\u003c/p\u003e\n\u003cp\u003eCrucially, I learned to test with real users. Short sessions with students revealed friction points that I would have missed on my own: ambiguous labels, buttons that looked like headings, and instructions that were too dense. I iterated by clarifying link labels (\"Apply for Scholarships\" instead of \"Click here\"), adding microcopy at critical decision points, and reorganizing pages to match the way students actually searched for information. In a few cases, small layout changes - like moving deadlines into a consistent right-hand column and tightening line lengths - reduced confusion and cut time-on-task.\u003c/p\u003e\n\u003cp\u003eI also invested in maintainability. I documented a small design system for the portfolio, including tokens (colors, spacing, typography) and reusable components (navigation, callouts, tables). That system reduces rework when I add artifacts or revise content. Print styles ensure that key pages remain usable offline. I added basic performance considerations - compressing images, deferring noncritical assets - so pages load quickly on school Wi-Fi and cellular networks. These improvements are largely invisible to end users but contribute to a smoother experience.\u003c/p\u003e\n\u003cp\u003eThe impact shows up in student behavior and counselor workflow. Students send fewer \"Where do I find...?\" emails and complete more steps without one-on-one coaching. Counselors can link confidently to pages knowing the structure, labels, and accessibility are consistent. Most importantly, the site now reflects the values I hold as an educator: clarity, inclusion, and respect for learners\u0027 time and context.\u003c/p\u003e\n\u003cp\u003eRepresentative artifact: the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, which demonstrates semantic structure, improved scannability, accessible color contrast, descriptive links, and responsive layouts. The lessons from IS_LT 7355 underpin the rest of my portfolio and continue to guide decisions as I iterate.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7383\u003c/strong\u003e\u003cbr/\u003eRapid Development Tools\u003c/td\u003e\n\u003ctd\u003eLow-code tools and rapid prototyping for instructional products.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course reframed my development process from \"complete the product, then gather feedback\" to \"ship a slice, learn, and iterate.\" I adopted a build-measure-learn loop that emphasized quick cycles and concrete evidence. Rather than polishing a full module, I shared small increments - an intro video, a single practice task, a draft visual - and asked students to interact with them. Their responses immediately revealed what resonated and what needed revision, allowing me to redirect effort to the highest-impact changes.\u003c/p\u003e\n\u003cp\u003eLow-code tools accelerated this cadence. Templates, visual editors, and simple data integrations let me prototype without getting bogged down in implementation details. I storyboarded sequences, produced lightweight media, and assembled resources into coherent flows that students could try within a single class period. Because the artifacts were quick to build, they were also quick to change. When analytics and reflections showed that a prompt was unclear or a graphic was distracting, I revised the asset within hours, not days.\u003c/p\u003e\n\u003cp\u003eEvidence gathering was intentionally lightweight and ethical. I favored aggregate indicators - task completion rates, time-on-task, quick pulse checks - over invasive tracking. I paired these with short, structured reflections: one or two questions that helped me understand confusion points and cognitive load without overburdening students. This combination gave me direction while respecting privacy and class time. Over multiple iterations, completion improved and questions shifted from \"What do I do?\" to \"Why does this pattern appear?\" - a sign that the scaffolds were doing their job.\u003c/p\u003e\n\u003cp\u003eOne representative example is the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial Markets Slide Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Early drafts tried to cover too much content per slide, leading to cognitive overload. Student feedback prompted me to reduce text density, pace narration, and use consistent visual metaphors. I aligned each segment with a single learning objective and added brief checks for understanding to surface misconceptions in real time. The result was higher engagement and clearer discussions about cause and effect in market behavior.\u003c/p\u003e\n\u003cp\u003eAnother thread involved collaborative analysis using Python and Google Sheets. Students selected variables, co-constructed datasets, and iterated on visualizations as claims evolved. My role shifted from content deliverer to facilitator of inquiry: I provided exemplars, templates, and nudges, while learners explored multiple pathways to a solution. Rapid iteration - both in the artifacts and in the procedures - helped sustain momentum and made successes visible to the group.\u003c/p\u003e\n\u003cp\u003eJust as important were the pivots. Not every idea worked. Some interactive elements distracted more than they helped; a few templates were too rigid for diverse contexts. Because I was working in small slices, it was easy to remove, replace, or simplify without derailing progress. Those moments reinforced a key lesson: iteration is not a sign of failure but a mechanism for learning for both teacher and students.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, rapid development practices reduced time to value and increased alignment between design intent and learner experience. Students spent more time wrestling with ideas and less time deciphering instructions. For me, the process provided a repeatable, humane way to improve materials: small bets, honest evidence, and steady refinement. See the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e for additional examples and iteration notes that document this approach in practice.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach instructional design. I no longer see myself as a content expert, but rather as a facilitator of learning. I understand that my role is not to create perfect materials, but to create materials that are good enough to spark meaningful learning. I have learned to trust the process of iteration and to see it as a natural part of the design cycle.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more willing to take risks and try new things, knowing that I can always iterate and improve. I am more focused on creating materials that are accessible and inclusive, and I am more intentional about gathering feedback from students and using it to inform my design decisions.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of empathy in instructional design. I have learned to see things from the student\u0027s perspective and to design materials that meet their needs. I have also learned to be more mindful of the emotional and social aspects of learning, and to create materials that are engaging and motivating.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of instructional design and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9410\u003c/strong\u003e\u003cbr/\u003ePython Seminar: Data Pipelines \u0026amp; Dashboards\u003c/td\u003e\n\u003ctd\u003eHands-on Python coding course focused on automating data ingestion, cleaning, and visualization for learning analytics dashboards.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eUsing Python as the core toolset, I built reusable Colab notebooks that authenticate to APIs, pull live CSV extracts, and normalize the data with pandas so counselors receive clean, analysable tables without manual effort.\u003c/p\u003e\n\u003cp\u003eI refactored pipelines into modular functions for requests, validation, and storage, which now keep the Student Risk dashboard, census demographic summaries, and transcript audits up to date with a single run.\u003c/p\u003e\n\u003cp\u003eThe course deepened my proficiency with pandas, requests, and plotting libraries, culminating in automated heatmaps, transcript evaluations, and intake dashboards that anchor the coding artifacts in this portfolio.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9417\u003c/strong\u003e\u003cbr/\u003eAction Research\u003c/td\u003e\n\u003ctd\u003eInquiry cycles to investigate and improve practice.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course transformed reflective practice into systematic inquiry. I framed a practical, context-relevant question - would simplifying the scholarship site\u0027s information architecture improve student follow-through from discovery to application? - and designed a modest study to investigate it. I began with a baseline map of the existing journey, identifying the typical paths students took and where drop-offs occurred. I then documented specific changes (clearer labels, consistent placement of deadlines, shorter instructions, and a persistent \"Apply\" affordance) and predicted how each might reduce friction.\u003c/p\u003e\n\u003cp\u003eI collected evidence with an eye toward proportionality and feasibility. Rather than implementing heavy instrumentation, I tracked a handful of indicators: page visits to key resources, clicks on application links, time-on-task for guidance pages, and the number of initiated applications within a set period. I paired these with short student reflections gathered at natural checkpoints - two or three questions about clarity, confidence, and what they expected to do next. This mixed-methods approach allowed me to see both movement in the numbers and the reasons behind it.\u003c/p\u003e\n\u003cp\u003eAnalysis was iterative. Early results showed more visits to deadlines pages but no corresponding increase in application starts. Student comments revealed that while deadlines were clearer, they still felt overwhelmed by the number of steps. In response, I consolidated instructions into checklists, added concise summaries at the top of longer pages, and created a lightweight \"start here\" page that guided students through the first two actions. The next cycle showed improvement: students reached application pages faster and reported greater confidence about what to do.\u003c/p\u003e\n\u003cp\u003eI also explored communication nudges outside the site. Short, targeted reminders in newsletters - linked to specific actions rather than general announcements - helped maintain momentum. Importantly, I tracked unsubscribes and opt-outs to ensure we were not adding noise. The goal was to support, not pester. These outreach experiments were documented alongside site changes, creating a fuller picture of the ecosystem that influences student behavior.\u003c/p\u003e\n\u003cp\u003eThroughout the study, I attended to ethics and participant burden. All surveys were brief and optional, data were aggregated for reporting, and I avoided collecting sensitive information. Findings were shared back with students and counselors in plain terms, including what we tried that did not make a difference. That transparency built trust and generated better ideas for subsequent cycles.\u003c/p\u003e\n\u003cp\u003eThe most valuable outcome was not a single optimization but a repeatable process. I now approach improvements using small, testable changes, simple measures aligned to the behavior I care about, and short cycles that let me adapt quickly. This stance has spilled over into other projects, from curricular materials to advising workflows. Action research gave me the structure to learn from my own context without waiting for perfect studies or exhaustive datasets.\u003c/p\u003e\n\u003cp\u003eArtifacts across the portfolio - particularly the navigation revisions and microcopy updates in the scholarship resources - trace this inquiry process and the rationale behind each change. The habit of documenting what I expected to happen, what occurred, and what I will try next continues to guide my practice.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9455\u003c/strong\u003e\u003cbr/\u003eDesign Thinking Evaluation\u003c/td\u003e\n\u003ctd\u003eHuman-centered evaluation approaches for learning experiences.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eIn this course, I learned to evaluate learning experiences with empathy and rigor. We began with foundational questions: Who are the learners? What are they trying to accomplish? Where and how will they engage with the materials? These questions anchored every method we used, from heuristic evaluations to think-alouds. I practiced observing without leading, listening for moments of hesitation, and distinguishing between true usability issues and individual preferences. This mindset helped me see barriers I had normalized over time.\u003c/p\u003e\n\u003cp\u003eMy focal project was an evaluation of scholarship resources. I constructed representative tasks (e.g., identify eligibility, locate deadlines, begin an application) and recruited a small, diverse set of students to complete them on their own devices. During think-alouds, I noted places where instructions felt dense, labels were ambiguous, or layout implied the wrong hierarchy. I triangulated these findings with a quick heuristic review focused on consistency, recognition over recall, error prevention, and help and documentation. The convergence of observations and heuristics made it clear where to act first.\u003c/p\u003e\n\u003cp\u003eRevisions prioritized clarity and momentum. I chunked instructions, front-loaded essential information, and used verbs in headings to set expectations (\"Submit FAFSA\" \"Request Transcript\"). I standardized the placement of deadlines and calls-to-action, improved contrast for key buttons, and ensured that the primary path remained visible on mobile screens. Microcopy addressed common points of confusion with short, supportive prompts. I also added small success moments - checkmarks and confirmations - to reinforce progress.\u003c/p\u003e\n\u003cp\u003eMeasuring impact required balancing thoroughness with practicality. I compared task completion times and error rates before and after revisions, and I gathered brief post-task reflections on ease and confidence. The improvements were notable: students reached application links faster, backtracked less, and reported feeling more certain about next steps. Importantly, the changes held up across devices with different screen sizes and input methods.\u003c/p\u003e\n\u003cp\u003eBeyond the immediate project, I developed patterns for scalable evaluation. I created short protocol templates for think-alouds and heuristic reviews, along with a lightweight issue tracker that categorizes findings by severity and effort. These artifacts help me sustain evaluation as an ongoing practice rather than a one-time event. I also learned to frame findings constructively with stakeholders - pairing evidence of friction with concrete, feasible recommendations - so teams feel empowered to act.\u003c/p\u003e\n\u003cp\u003eThe course reinforced that evaluation is part of design, not an afterthought. By building evaluation into the cadence of development, I\u0027m more likely to catch issues before they compound and to keep the learner experience at the center. The strategies I practiced here now inform my work across the portfolio: clear goals, representative tasks, small tests, and respectful observation that leads to actionable insights.\u003c/p\u003e\n\u003cp\u003eEvidence of these changes appears across my artifacts, including the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003eRapid Development\u003c/a\u003e work and the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, where evaluation shaped sequencing, instructions, and the balance between guidance and autonomy.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9466\u003c/strong\u003e\u003cbr/\u003eLearning Analytics\u003c/td\u003e\n\u003ctd\u003eData collection, analysis, and interpretation to improve learning.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me separate curiosity from usefulness when it comes to data. I began by articulating the questions I actually needed to answer - Are students getting stuck on specific steps? Which explanations reduce confusion? Where does motivation flag? - and only then selecting indicators that could reasonably inform those questions. I resisted the temptation to instrument everything and focused instead on a small set of measures aligned to outcomes: task completion, time-on-task for key pages, error rates, and short reflections that captured perceived clarity and confidence.\u003c/p\u003e\n\u003cp\u003eDesigning the pipeline required attention to proportionality and privacy. I avoided collecting sensitive data that were not essential to learning goals, and I aggregated results whenever possible. For example, rather than logging every click, I tracked transitions between meaningful states (e.g., from \"view deadlines, to \"start application,). I documented retention windows and access controls and communicated in plain language what was being collected and why. This transparency increased trust and reduced anxiety for students and families accustomed to opaque data practices.\u003c/p\u003e\n\u003cp\u003eInterpreting analytics demanded humility. Time-on-task can reflect deep engagement or confusion; completion rates might improve because instructions are clearer or because learners are skipping important steps. To avoid false certainty, I paired quantitative indicators with quick qualitative checks - two-question pulses at natural breakpoints and occasional think-alouds with volunteers. These mixed methods helped me surface causal mechanisms and prioritize changes with the greatest impact on understanding and persistence.\u003c/p\u003e\n\u003cp\u003eWith these practices in place, I used analytics to drive specific improvements. When I noticed extended dwell times on a guidance page with low subsequent application starts, I rewrote the first two paragraphs to front-load purpose and reduced reading burden by chunking steps. I also added a short checklist and a \"What you\"ll need, callout. In the next cycle, dwell time decreased and application starts rose, with student reflections citing \"clearer next steps, as the main factor. In another case, I discovered that mobile users abandoned a long form more frequently; responsive tweaks and a save-and-return affordance reduced drop-offs.\u003c/p\u003e\n\u003cp\u003ePerhaps the most significant lesson was to keep analytics humane. Dashboards were designed for action, not for surveillance: simple visualizations tied to specific decisions, like which pages needed revision or where to place a clarifying prompt. I also built in guardrails - reminders about interpretation limits and links to the evidence behind a proposed change - so conversations stayed grounded. By closing the loop from evidence to iteration and back again, I created a sustainable rhythm of improvement that respected learners\" privacy and time.\u003c/p\u003e\n\u003cp\u003eThe approach now permeates my portfolio. From the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e to scholarship resources, analytics serve design intent: to clarify, to support, and to empower. The course shifted my mindset from \"more data is better, to \"the right data, used ethically, makes learning better.,\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9467\u003c/strong\u003e\u003cbr/\u003eTechnology to Enhance Learning\u003c/td\u003e\n\u003ctd\u003eTechnology integration strategies for impact and equity.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me to develop a deeper understanding of how technology can be used to enhance learning. I learned about the different types of technology integration, including substitution, augmentation, modification, and redefinition. I also learned about the importance of considering the SAMR model when designing technology-enhanced lessons.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of equity in technology integration. I learned about the digital divide and how it can impact student learning. I also learned about the importance of providing access to technology for all students, regardless of their background or socioeconomic status.\u003c/p\u003e\n\u003cp\u003eI also learned about the different types of technology that can be used to enhance learning, including learning management systems, educational software, and multimedia tools. I learned about the importance of selecting technology that is aligned with the learning objectives and that is accessible to all students.\u003c/p\u003e\n\u003cp\u003eThroughout the course, I had the opportunity to design and implement technology-enhanced lessons. I learned about the importance of considering the instructional design model when designing lessons, and I learned about the different types of instructional strategies that can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOne of the most significant challenges I faced in this course was learning to navigate the different types of technology. I had to learn how to use new software and tools, and I had to learn how to troubleshoot technical issues. However, with the support of my instructor and my peers, I was able to overcome these challenges and develop a deeper understanding of how technology can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOverall, this course was incredibly valuable. I learned about the importance of technology integration in education, and I developed a deeper understanding of how technology can be used to enhance learning. I also developed a range of skills, including instructional design, technology integration, and troubleshooting.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach teaching and learning. I no longer see technology as a separate entity from instruction, but rather as an integral part of the learning process. I understand that technology can be used to enhance learning, and I am committed to using it in a way that is equitable and accessible to all students.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more intentional about using technology to enhance learning, and I am more focused on creating lessons that are accessible and inclusive. I am also more willing to take risks and try new things, knowing that I can always iterate and improve.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of ongoing professional development. I learned about the importance of staying current with the latest research and trends in technology integration, and I learned about the importance of seeking out opportunities for professional growth and development.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of technology integration and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9471\u003c/strong\u003e\u003cbr/\u003eInstructional Systems Design\u003c/td\u003e\n\u003ctd\u003eSystematic analysis, design, development, and evaluation.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course operationalized ADDIE in my day-to-day work. In Analysis, I clarified the problem space with stakeholders - counselors, students, and families - using interviews, journey mapping, and quick artifact audits. We defined the gap in terms of learner performance and experience, not just content coverage. From there, I wrote measurable objectives that described what learners should be able to do and under what conditions, aligning them to authentic tasks they actually encounter.\u003c/p\u003e\n\u003cp\u003eDuring Design, I translated objectives into assessments and learning activities. I favored performance tasks over recall, and I arranged sequences to activate prior knowledge, reduce extraneous load, and support transfer. I made accessibility a non-negotiable constraint, specifying patterns (heading hierarchy, alt text, focus states), contrast thresholds, and mobile-first layouts. Prototypes at this stage were intentionally low-fidelity so I could iterate quickly based on feedback.\u003c/p\u003e\n\u003cp\u003eDevelopment focused on turning designs into maintainable artifacts. I built reusable components and wrote microcopy that anticipated sticking points. I organized assets for traceability and reuse, and documented decisions so collaborators could pick up where I left off. Throughout, I validated materials with quick checks - does the assessment truly measure the objective? Does the activity provide the right level of support? - and I fixed mismatches before they calcified.\u003c/p\u003e\n\u003cp\u003eImplementation emphasized readiness. I created facilitator notes, checklists, and support guides, and I rehearsed delivery with colleagues to catch practical issues. I monitored early sessions for surprises and captured them as candidates for the next iteration. Importantly, I framed changes as part of the process, not as evidence that something had \"failed\" which encouraged honest feedback and steady improvement.\u003c/p\u003e\n\u003cp\u003eEvaluation ran through everything, not just the end. I combined formative evidence (observations, quick polls, error patterns) with summative indicators (completion, performance against rubrics) to judge effectiveness. When evidence suggested a misalignment, say, learners succeeded on a task but could not transfer the skill, I revisited objectives and activities to tighten the link. I also reported findings in concise, actionable summaries to stakeholders so decisions about what to keep, revise, or retire were grounded in shared understanding.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, this disciplined approach improved coherence and outcomes. Artifacts like the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e show objectives, tasks, and assessments pulling in the same direction, while the rapid prototypes demonstrate how early evaluation prevents costly rework. ADDIE is no longer an abstract model for me; it is a practical rhythm for designing learning that respects people\u0027s time and leads to measurable, equitable gains.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9473\u003c/strong\u003e\u003cbr/\u003eProject Management\u003c/td\u003e\n\u003ctd\u003ePlanning, execution, risk, and stakeholder communication.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course provided the scaffolding to deliver learning products predictably and sustainably. I learned to define scope crisply - what is in, what is out, and why - and to translate that scope into milestones with observable outcomes. I time-boxed work, used Kanban-style boards to visualize flow, and limited work in progress so quality did not suffer. These habits reduced context switching and created momentum without requiring heroic efforts.\u003c/p\u003e\n\u003cp\u003eRisk management became a routine, not a formality. I identified technical, content, and schedule risks early and documented triggers, likelihood, and mitigations in a simple log. For web artifacts, recurring risks like link rot and browser quirks received standing checks: automated link health scans, manual spot checks on common devices, and a brief checklist for major releases. By making risk visible, I could negotiate trade-offs honestly with stakeholders - choosing to descale a feature, for instance, to protect a deadline and quality bar.\u003c/p\u003e\n\u003cp\u003eCommunication was another pillar. I wrote succinct status updates that focused on decisions, risks, and next steps rather than play-by-play activity. I set expectations for feedback windows and established one \"source of truth\" for assets to avoid version sprawl. Retrospectives closed each cycle, capturing what worked, what was painful, and what we would try next. Over time, these rituals built trust and reduced surprises.\u003c/p\u003e\n\u003cp\u003eResourcing and scheduling were treated as design problems. I estimated effort using comparative sizing rather than false precision, grouped similar tasks to gain efficiencies, and reserved capacity for the inevitable unknowns. I also created templates - issue trackers, release notes, change logs - that shortened ramp-up for collaborators and preserved context for future revisions.\u003c/p\u003e\n\u003cp\u003eThe payoff is evident across my portfolio. Releases are steadier, quality is more consistent, and maintenance overhead is lower. Students and colleagues benefit from reliable updates and clear communication; I benefit from a process that is humane and sustainable. Project management did not make the work less complex, but it made the path through that complexity clearer and fairer for everyone involved.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9474\u003c/strong\u003e\u003cbr/\u003eFront End Analysis\u003c/td\u003e\n\u003ctd\u003eNeeds assessment and problem framing techniques.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course taught me to slow down and frame the right problem before building solutions. I mapped the ecosystem around my learners - students, families, counselors, platforms - and gathered perspectives through interviews, quick surveys, and artifact reviews. I resisted the urge to jump to features and instead synthesized what I heard into jobs-to-be-done and pain points. Journey mapping exposed where energy was lost: unclear eligibility, scattered deadlines, instructions that assumed background knowledge students did not yet have.\u003c/p\u003e\n\u003cp\u003eFrom this analysis, I wrote problem statements that described the gap in observable terms and identified constraints we had to honor (devices, time, bandwidth, privacy). I evaluated candidate solutions against criteria grounded in this reality: Does the change reduce steps? Does it improve comprehension at the moment of decision? Is it sustainable for counselors to maintain? This filtered out ideas that were flashy but fragile and surfaced simpler interventions with outsized impact - clearer labels, checklists, consistent placement of critical information, and targeted microcopy.\u003c/p\u003e\n\u003cp\u003eI also honed techniques for gathering just enough data to move forward. Lightweight card sorts clarified language; five-minute usability checks with students revealed misleading layouts; quick analytics confirmed whether changes produced the expected behavior. Importantly, I documented not only what we learned but what we chose not to do and why, preserving rationale for future teams and preventing drift.\u003c/p\u003e\n\u003cp\u003eFront-end analysis improved alignment across stakeholders. With a shared understanding of the problem and the criteria for success, conversations shifted from personal preference to learner impact. That alignment made subsequent design and development faster and less contentious. It also made evaluation clearer: we knew what we intended to change and how we would know if it worked.\u003c/p\u003e\n\u003cp\u003eUltimately, this course embedded a habit of inquiry that now precedes major changes in my portfolio. I approach new challenges with curiosity and discipline, confident that the time invested up front will pay dividends in clarity, quality, and equity for the learners I serve.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e"},"reflection":{"id":"reflection","title":"Reflection Statement","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eReflection Statement\u003c/h2\u003e\n\u003ch3\u003eContext\u003c/h3\u003e\n\u003cp\u003eWhen I entered the ISLT program, I brought a counselorâ€™s mindset and a financierâ€™s analytical discipline, but my technology practice was largely pragmatic. In daily school work, I made tools â€œwork well enoughâ€ to meet immediate student needs , sharing resources, simplifying instructions, and troubleshooting access barriers. What I lacked was a systematic framework for designing technology-enhanced learning experiences end to end: aligning needs to outcomes, selecting tools intentionally, measuring impact, and then iterating for equity and clarity. I chose the ISLT program, Technology in Schools emphasis, to build that structure around my practice. I wanted to develop reusable patterns for accessible web design, rapid prototyping, and responsible use of learning data that could scale beyond individual interventions. Entering the program, I was comfortable with quantitative reasoning and stakeholder communication from my finance background and counseling experience, but I needed deeper knowledge in interaction design, evaluation, and standards-based technology integration. The ISLT coursework challenged me to move from ad hoc solutions to an evidence-driven, ethically grounded design approach, one that serves students and families across devices, bandwidth, and contexts while protecting their privacy and honoring their time.\u003c/p\u003e\n\u003ch3\u003eSignificant Learnings\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccessibility and clarity are design requirements, not afterthoughts.\u003c/strong\u003e Across projects, I learned that semantic HTML, WCAG-aligned color and typography choices, and mobile-first layouts are not just technical niceties; they directly reduce cognitive load and make resources more actionable for students and families. By treating headings, lists, labels, and link text as navigational cues, especially for screen readers and small screens, I improved findability and comprehension. This shift changed how I write microcopy, structure pages, and audit contrast. It also made my resources more equitable for learners who access materials on mobile devices or with limited bandwidth.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e. This curriculum showcases consistent semantic structure, clear task chunking, and high-contrast, readable typography. It demonstrates how accessible patterns improve the student experience without sacrificing visual appeal.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRapid prototyping accelerates learning and quality.\u003c/strong\u003e The program taught me to plan short, focused build cycles with quick feedback loops, using low-code tools when appropriate. By putting workable prototypes in front of students and colleagues early, I surfaced misconceptions and usability issues before they became entrenched. I adopted a cadence of design sprints, check-ins, and micro-evaluations that helped me converge on effective media sequencing and interaction patterns faster than when I built â€œfinishedâ€ products up front. Rapid development also fostered a culture of co-design, students felt comfortable giving candid feedback because iteration was expected.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e. This portfolio collects prototypes that moved from concept to testable artifacts quickly, documenting how feedback informed successive versions and how scope discipline produced better outcomes in less time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData-informed decision-making strengthens instructional impact.\u003c/strong\u003e I learned to select practical, ethical indicators tied to objectives, click-through paths, time on task, completion and error rates, and rubric-aligned performance, so that I could close the loop between design and evidence. Rather than collecting data for its own sake, I now plan the question first, identify the smallest useful signals, and then adjust the resource or activity accordingly. This approach keeps measurement purposeful, protects student privacy, and leads to visible improvements in navigation and comprehension.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Iterations of this multimedia lesson were guided by lightweight analytics and formative checks (e.g., sequence adjustments based on confusion points and pacing). The result is a clearer narrative flow and more targeted prompts that support transfer.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eImpact on Practice and Growth toward ISTE Educator Standards\u003c/h3\u003e\n\u003cp\u003eThe ISLT program has reshaped how I design, facilitate, and evaluate learning experiences. In daily practice, I now begin with learner context and constraints, articulate measurable outcomes, and choose tools that minimize barriers. Prototyping early and often has become standard, and I treat accessibility reviews and contrast checks as nonnegotiable gates for release. My communication artifacts, web pages, multimedia sequences, and counselor-facing resources, are more concise, scannable, and consistent, which has reduced misinterpretation and follow-up overhead for students and families. Equally important, my data practices are lean and purposeful: I gather only what I need, explain how it will be used, and then show stakeholders the improvements it enables.\u003c/p\u003e\n\u003cp\u003eThis work aligns with and advances my growth in the ISTE Educator Standards. As a \u003cstrong\u003eDesigner\u003c/strong\u003e (5aâ€“5c), I build accessible, learner-centered materials with authentic, goal-aligned activities. As a \u003cstrong\u003eFacilitator\u003c/strong\u003e (6aâ€“6d), I create conditions for ownership, using technology to enable exploration, feedback, and creativity while managing tools and strategies responsibly. As an \u003cstrong\u003eAnalyst\u003c/strong\u003e (7aâ€“7c), I employ alternative demonstrations of learning and use assessment data appropriately to inform next steps without over-collecting or compromising privacy. I have also grown as a \u003cstrong\u003eCollaborator\u003c/strong\u003e (4b) by co-learning with students through iterative prototyping, as a \u003cstrong\u003eLeader\u003c/strong\u003e (2b) by advocating equitable access through mobile-first, high-contrast designs, and as a \u003cstrong\u003eCitizen\u003c/strong\u003e (3c) by modeling safe, ethical use of data. Together, these shifts mean I am not merely curating resources but designing learning experiences that are equitable, measurable, and sustainable in a real school setting.\u003c/p\u003e\n\u003cp\u003eGoing forward, I will continue to maintain a reusable design system (components, tokens, and patterns) across counselor resources, expand quick evaluation toolkits to include student-friendly reflection prompts, and formalize lightweight analytics dashboards that respect privacy. Most importantly, I will keep centering accessibility and student voice, ensuring that the artifacts I produce help students navigate opportunities confidently, make informed decisions, and demonstrate their learning in multiple ways.\u003c/p\u003e\n\u003c/div\u003e"},"program-goals":{"id":"program-goals","title":"Program Goals \u0026 Outcomes","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eLearning Technologies Program Goals and Student Learning Outcomes\u003c/h2\u003e\n\u003cp\u003eThe Learning Technologies (LTD) program prepares practitioners to systematically design, develop, evaluate, and lead learning technology and instructional solutions across educational and organizational contexts. The reflections and artifacts below demonstrate how theory and research informed my practice in equitable, accessible, and sustainable ways.\u003c/p\u003e\n\n\u003ch3\u003eGoal 1\u003c/h3\u003e\n\u003cp\u003eStudents will develop theory and research-based skills for innovative, aesthetic, accessible, equitable, effective and sustainable design and development of technologies for learning opportunities and systems.\u003c/p\u003e\n\n\u003ch4\u003eStudent Learning Outcome 1.1\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will design and develop learning and performance opportunities and systems including direct instruction, student-centered learning, collaborative work, and performance support.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eMy approach to design and development matured from assembling activities to architecting coherent learning experiences. At the center of this shift is a commitment to build opportunities that are accessible, equitable, and durable across contexts. I now begin by clarifying performance goals and actionable success criteria, then select modalities - direct instruction, inquiry, collaboration, or performance support - that best serve those goals for my learners and setting. This outcome challenged me to move beyond personal preference and to ground choices in evidence about how people actually learn and what they need at the moment of action.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e exemplifies this evolution. Early drafts were content-heavy; learners had to sift through long passages to find critical steps. Iteration led me to concise, purposeful explanations followed by practice where thinking is visible. I embedded options consistent with Universal Design for Learning: text paired with visuals, short scenarios in place of abstractions, and flexible response formats. Every section answers a simple question - what do learners need to understand or do, and what support will help them get there - and unnecessary flourish is trimmed away. The curriculum demonstrates direct instruction through structured lessons on evaluating online sources and recognizing misinformation, student-centered learning through scenario-based practice where learners apply credibility checks to realistic examples, and performance support through embedded checklists and quick-reference guides that learners access at the moment of need.\u003c/p\u003e\n\u003cp\u003eComplementing direct instruction, I designed student-centered inquiry where learners choose variables, collect data, and test claims. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part2_part2.html\" rel=\"noopener\" target=\"_blank\"\u003ePython Code Simulation\u003c/a\u003e artifact demonstrates this approach: students interact with a dynamic visualization that models instructional scenarios and supports differentiated feedback. I provided scaffolded code examples and interactive controls that emphasized thinking over syntax. This balance helped students practice critical evaluation while keeping cognitive load appropriate. The simulation serves as both a learning tool and a performance support system, allowing learners to test hypotheses and observe outcomes in real time.\u003c/p\u003e\n\u003cp\u003eAcross these designs, accessibility and equity remained non-negotiable. I used semantic HTML and heading hierarchy for structure, ensured color contrast and visible focus states, and designed mobile-first so materials worked well on phones. The goal is not to create one \"perfect\" artifact but a resilient set of experiences that meet learners where they are and invite them further.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part2_part2.html\" rel=\"noopener\" target=\"_blank\"\u003ePython Code Simulation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4\u003eStudent Learning Outcome 1.2\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will design and develop learning opportunities and systems for meaningful learning; promote student engagement in online learning environments; and select appropriate technology and learning objects to support learners.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome taught me to link content, context, and cognition in concrete ways. I now start by activating prior knowledge and explicitly naming the transfer target - what learners should be able to do outside the activity. This framing prevents the all-too-common drift toward interesting but disconnected tasks. I then sequence experiences from worked examples to guided practice to more open challenges, tracking cognitive load along the way so that the difficulty comes from the thinking, not from deciphering instructions.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets eLearning\u003c/a\u003e slideshow demonstrates my approach to promoting meaningful learning and student engagement. I designed the presentation with concise narration, consistent visual metaphors, and pacing aligned to learning objectives. I reduced extraneous load by limiting new ideas per slide, aligning text and visuals, and inserting brief checks for understanding to surface misconceptions early. The technology choices - interactive slides with embedded formative checks - serve pedagogy rather than existing as ends in themselves. Students reported that the structure helped them follow the argument and prepared them to ask better questions in subsequent activities. This artifact shows how I select appropriate technology and learning objects: I chose a slideshow format because it allowed controlled pacing and the integration of visual examples with narration, which research suggests supports schema building for complex financial concepts.\u003c/p\u003e\n\u003cp\u003eMedia design supported attention and retention throughout the experience. I used contrastive examples, consistent metaphors, and brief formative checks to keep attention and surface misconceptions. When learners moved into analysis, templates and exemplars provided structure while still allowing for choice of variables and visualizations. Assessment aligned with the goals of meaning and transfer - rubrics emphasized reasoning, evidence, and communication over rote recall.\u003c/p\u003e\n\u003cp\u003ePerhaps most importantly, I learned to narrate the why of design choices to learners. Explaining how a prompt reduces cognitive load or why a representation supports inference invited students into the design conversation and improved metacognition. Meaningful learning, I discovered, is not just something we design for learners; it is something we design with them as partners.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets eLearning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4\u003eStudent Learning Outcome 1.3\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents can conduct project management activities to support the total lifecycle of design, development and implementation of learning opportunities and systems.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome pushed me to treat instructional design not only as a creative process but also as a disciplined practice supported by project management. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html\" rel=\"noopener\" target=\"_blank\"\u003ePortfolio\u003c/a\u003e itself serves as my primary evidence of project management mastery. Building this portfolio required me to define scope crisply - identifying which artifacts would demonstrate each SLO, establishing a timeline for completion, and determining what was in-bounds and out-of-bounds for the final deliverable. I translated that scope into milestones with concrete deliverables: first the content inventory, then the information architecture, then the visual design system, and finally the interactive coverflow implementation.\u003c/p\u003e\n\u003cp\u003eRisk management became routine instead of an afterthought during portfolio development. I identified technical risks (cross-browser compatibility, mobile responsiveness), content risks (ensuring each SLO had sufficient evidence), and schedule risks (balancing portfolio work with other coursework and professional responsibilities). For web artifacts, recurring risks like link rot, browser quirks, and inconsistent mobile behavior received standing checks: I implemented automated link validation, spot testing on common devices, and a pre-release checklist. When I discovered that dynamically loaded content wasn\u0027t initializing toggle buttons correctly, I diagnosed the root cause and implemented a fix within the same sprint - demonstrating the agile response to technical issues that effective project management enables.\u003c/p\u003e\n\u003cp\u003eVersioning and traceability supported iteration at speed. I organized assets with clear naming conventions, maintained the codebase in Git with meaningful commit messages, and documented decisions so collaborators (and my future self) could pick up work without losing context. The portfolio\u0027s modular architecture - with separate JSON files for section content and JavaScript files for course data - reflects deliberate choices to support maintenance and future updates. When feedback suggested improvements to the reflection toggle functionality, these structural choices allowed me to implement changes quickly without breaking other components.\u003c/p\u003e\n\u003cp\u003eMaintenance planning was part of delivery, not a separate phase. I adopted a small design system - tokens for color, spacing, and typography; reusable components for navigation, callouts, and cards - so changes propagate consistently across the portfolio. This approach lowered total cost of ownership and made continuous improvement realistic. The portfolio demonstrates the complete lifecycle this SLO requires: from initial analysis and scoping, through iterative development and testing, to implementation and ongoing maintenance.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html\" rel=\"noopener\" target=\"_blank\"\u003ePortfolio\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4\u003eStudent Learning Outcome 1.4\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will demonstrate mastery of technological production skills in web development, digital asset creation, and other applicable development skills necessary for the creation of learning systems (e.g., web page prototyping, game design, digital media, etc.).\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome represents the craft side of my work - the ability to produce web and media artifacts that are both technically sound and pedagogically effective. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/\" rel=\"noopener\" target=\"_blank\"\u003eProfessional Webpage\u003c/a\u003e demonstrates my mastery of web development skills. I built it using semantic HTML5, ensuring that structure communicates meaning to assistive technologies and creates a reliable foundation for styling and scripting. I implemented responsive CSS patterns using flexbox and grid layouts that scale gracefully from phones to desktops, and I standardized spacing, typography, and component styles so visitors encounter a coherent visual language. Color and contrast received special attention: using WCAG guidelines as a floor, I adjusted palettes for sufficient contrast and tested focus states under real conditions. The site includes proper heading hierarchy, accessible navigation, and alt text that conveys purpose and context rather than mere appearance.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard video\u003c/a\u003e showcases my digital media production skills. Creating this walkthrough required me to plan narrative pacing, synchronize screen recordings with voiceover narration, and edit the final product for clarity and engagement. I learned to align each segment to a single objective, reduce visual density, and use callouts to direct viewer attention. The video demonstrates both the technical dashboard I built (combining Python data processing with HTML/CSS presentation) and my ability to communicate complex technical workflows to diverse audiences through multimedia.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://youtu.be/W4DFjhLx0wY\" rel=\"noopener\" target=\"_blank\"\u003eStates of Matter Animation\u003c/a\u003e extends my digital asset creation skills into motion graphics. This project required understanding keyframe animation principles, timing curves, and visual storytelling. I designed the animation to support science learning objectives, using color, motion, and pacing to illustrate molecular behavior at different temperatures. The process taught me to iterate on visual explanations - early versions were too fast and visually cluttered, while the final version uses deliberate pacing and clear visual hierarchy to support comprehension.\u003c/p\u003e\n\u003cp\u003eTogether, these artifacts demonstrate the range of production skills this SLO requires: semantic, accessible web development; video production and editing; animation and motion graphics; and the judgment to select the right medium for each learning goal.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/\" rel=\"noopener\" target=\"_blank\"\u003eProfessional Webpage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard (Python \u0026amp; HTML)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/W4DFjhLx0wY\" rel=\"noopener\" target=\"_blank\"\u003eStates of Matter Animation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eGoal 2\u003c/h3\u003e\n\u003cp\u003eStudents will learn to apply systematic methods for analyzing instructional or training needs, applying those results to instructional design and development, and collecting and analyzing formative and summative evaluation data to improve the instructional product.\u003c/p\u003e\n\n\u003ch4\u003eStudent Learning Outcome 2.1\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents can choose and conduct appropriate analytical methods and apply the results to the systematic design and development of learning environments and systems.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome taught me to treat analysis as a purposeful and proportional inquiry rather than an endless phase. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/scholarships_data.html\" rel=\"noopener\" target=\"_blank\"\u003eScholarships Database\u003c/a\u003e demonstrates how I applied analytical methods to identify learner needs and translate findings into system design. I began by conducting user interviews and task analyses with students and counselors, which revealed two critical issues: findability (students reported they \"didn\u0027t know where to start\") and language complexity (instructions felt dense and overwhelming). I mapped the user journey from discovery to application and marked points of hesitation. Card sorts clarified which category labels felt intuitive, while brief usability checks revealed where layout implied the wrong hierarchy.\u003c/p\u003e\n\u003cp\u003eThese analytical findings directly shaped the database design. I implemented clearer category labels based on card sort results, front-loaded purpose statements based on task analysis findings, and chunked information into scannable sections based on reading pattern observations. The Scholarships Database now features a searchable, filterable interface that addresses the specific pain points analysis revealed - proving that good design flows from good analysis.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" rel=\"noopener\" target=\"_blank\"\u003eGIS Map of Career Z and CHIPS CTE Challenge Semifinalists\u003c/a\u003e demonstrates a different analytical approach: spatial analysis to reveal patterns in educational opportunity. I chose geographic visualization because stakeholder interviews indicated that counselors needed to understand where successful program participants were located to inform outreach strategies. The map applies analytical methods (geocoding, clustering, choropleth visualization) to transform raw semifinalist data into actionable geographic insights. This artifact shows how I match analytical methods to the questions stakeholders need answered.\u003c/p\u003e\n\u003cp\u003eCrucially, I framed analysis as iterative. Each design sprint began with a hypothesis, a small test, and a plan for what evidence would confirm or challenge the claim. This created a practical rhythm: gather light evidence, act, and learn. By keeping analysis close to decisions and right-sized to context, I created a design practice that is rigorous enough to be trustworthy and light enough to sustain in a busy school environment.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/scholarships_data.html\" rel=\"noopener\" target=\"_blank\"\u003eScholarships Database\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" rel=\"noopener\" target=\"_blank\"\u003eGIS Map of Career Z and CHIPS CTE Challenge Semifinalists\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4\u003eStudent Learning Outcome 2.2\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will design assessments aligned with the intended outcomes of the learning environments, choose and implement appropriate methods to conduct formative and summative evaluation of the learning environment, and use data to continuously improve the instructional product(s).\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome focused my attention on alignment between outcomes, activities, and evidence, and on the practical ways assessment can drive iteration without exhausting learners. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Student Dashboard\u003c/a\u003e embodies this approach to data-driven continuous improvement. The dashboard was designed with clear intended outcomes: counselors should be able to quickly identify students needing intervention, prioritize by risk level, and track whether interventions are working.\u003c/p\u003e\n\u003cp\u003eI designed the dashboard\u0027s assessments to align directly with these outcomes. Key performance indicators - grade trends, attendance patterns, credit accumulation - were selected because they predict the outcomes counselors care about (on-time graduation, course success). The visual design uses color coding and sorting to make the most critical information immediately apparent, implementing formative evaluation principles: provide timely, actionable feedback that guides the next decision.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e demonstrates how I use data to continuously improve the instructional product. The documentation captures the iterative refinement process: initial counselor feedback revealed that the first version had too many columns and unclear priority signals. I implemented changes based on this formative evaluation - simplifying the display, adding risk-level badges, and creating expandable detail views - then gathered additional feedback to verify improvements. The documentation itself serves as a summative record of design decisions and their rationales, enabling future iteration.\u003c/p\u003e\n\u003cp\u003eThroughout this work, I combined interaction metrics with brief reflections to learn where users struggled or disengaged. I tracked transitions between meaningful states (viewing the dashboard, drilling into student details, recording interventions), time spent on key steps, and completion of intended workflows. Because metrics alone can mislead, I paired them with brief user interviews. This blend produced actionable signals while respecting privacy and time. The dashboard continues to evolve based on this evaluation cycle, demonstrating how assessment drives continuous improvement.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Student Dashboard\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eGoal 3\u003c/h3\u003e\n\u003cp\u003eStudents will understand the impact of technologies we create or use on society at large and strive to produce ethical, inclusive, and equitable instructional systems and technologies.\u003c/p\u003e\n\n\u003ch4\u003eStudent Learning Outcome 3.1\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will demonstrate ethical and inclusive practices to all aspects of analytical data collection and analysis methods used in the design and evaluation of instructional products or learning environments (e.g., sampling methods).\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome centers the ethical and societal dimensions of educational technology. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Student Dashboard\u003c/a\u003e required me to grapple directly with ethical data practices. The dashboard aggregates sensitive student information - grades, attendance, behavior incidents - to support counselor interventions. Building it ethically required decisions at every stage: what data to collect, how to store it, who could access it, and how to present it without stigmatizing students.\u003c/p\u003e\n\u003cp\u003ePrivacy by design guided my implementation. I audited data flows, noting collection points, storage locations, access controls, and retention windows. I removed sensitive fields that were not essential to the intervention goal, implemented role-based access so only authorized counselors see student details, and designed the interface to show aggregate patterns before individual records. The dashboard displays risk indicators rather than raw data points, reducing granularity to what\u0027s necessary for decision-making while protecting student privacy. When the dashboard needed to be shared for documentation purposes, I created an anonymized version that replaces names and IDs with sequential aliases to maintain FERPA compliance.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e makes these ethical choices transparent and reproducible. The documentation explains what data is collected and why, how access is controlled, what the retention policy is, and how the risk algorithm works. This transparency serves both accountability and training purposes - new counselors understand the system\u0027s limitations and appropriate use. I explicitly document that the dashboard is a decision-support tool, not a decision-making tool, and that risk scores require human judgment and context before action.\u003c/p\u003e\n\u003cp\u003eEquity informed my analytical choices throughout. I examined whether the risk algorithm performed consistently across demographic groups and documented limitations where sample sizes were small. I avoided framing analytics as judgments of individual worth or effort; instead, they are signals about where students might need support and where the system might be getting in the way. By foregrounding privacy, proportionality, and inclusion, I aim to ensure that the benefits of this technology flow to all students - not just those best positioned to navigate opaque systems.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Student Dashboard\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" rel=\"noopener\" target=\"_blank\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4\u003eStudent Learning Outcome 3.2\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eStudents will design and implement instructional products and learning environments based upon Universal Design Principles.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch4\u003eReflection\u003c/h4\u003e\n\u003cp\u003eThis outcome challenged me to operationalize Universal Design for Learning (UDL) as a practical set of design moves rather than a slogan. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy eLearning\u003c/a\u003e curriculum demonstrates UDL principles throughout its design. I created materials that offer multiple means of engagement, representation, and action/expression so that variability is anticipated from the start.\u003c/p\u003e\n\u003cp\u003eFor multiple means of representation, I presented core ideas through short, plain-language explanations paired with examples and visuals. Key concepts appear in text, are illustrated with screenshots and diagrams, and are reinforced through interactive scenarios. For multiple means of engagement, I used scenario prompts that connect to students\u0027 real online experiences and provided choice in which examples to analyze. For multiple means of action and expression, I offered alternative ways to demonstrate understanding - written justification, quick verbal explanations, or annotated screenshots.\u003c/p\u003e\n\u003cp\u003eAccessibility is embedded in the code and design. I implemented semantic HTML with proper heading hierarchy, ensuring screen readers can navigate the content logically. I tested color contrast against WCAG AA standards and added visible focus states for keyboard navigation. I provided printable versions and lightweight downloads for learners with limited connectivity, recognizing that access conditions vary. Captions and transcripts accompany any audio or video content.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets eLearning\u003c/a\u003e extends UDL principles to multimedia. I paced narration to align with visual changes, reduced extraneous text to manage cognitive load, and used consistent metaphors to support schema building. Quick checks for understanding prompt retrieval and transfer without high stakes. Transcripts ensure access for those who prefer to read or who need accommodations. Templates and exemplars provide structure for subsequent activities while still allowing for choice.\u003c/p\u003e\n\u003cp\u003eAcross both artifacts, these UDL moves reduced barriers and improved participation. Students reported that they could enter tasks more confidently, navigate more easily on phones, and choose ways of demonstrating understanding that felt authentic. By combining UDL with an iterative mindset, I continue to refine materials so that inclusion is not a one-time effort but a living practice.\u003c/p\u003e\n\u003ch5\u003eArtifacts\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy eLearning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets eLearning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"}};