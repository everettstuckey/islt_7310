const PORTFOLIO_SECTIONS = {"at-risk-student-dashboard-webpage":{"id":"at-risk-student-dashboard-webpage","title":"At-Risk Student Dashboard Webpage","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eAt-Risk Student Dashboard Webpage\u003c/h2\u003e\n\u003cp\u003eThe anonymized At-Risk Student Dashboard is rendered as a static HTML export that mirrors the live Python-driven version while safeguarding student privacy. This artifact documents the production-grade visualization that counselors use to triage interventions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDynamic tables update with key market and risk indicators reflecting historical data in the export.\u003c/li\u003e\n\u003cli\u003eJavaScript post-processing replaces names and IDs with sequential aliases to maintain FERPA compliance.\u003c/li\u003e\n\u003cli\u003eA branded cover image introduces the dashboard and aligns with the rest of the portfolio aesthetic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe export references authentic dashboard assets such as \u003ccode\u003eget_atm_iv_ts.png\u003c/code\u003e and \u003ccode\u003efinancial_analysis.png\u003c/code\u003e, demonstrating the breadth of analytics available to staff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAccess the artifact:\u003c/strong\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/output.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard Webpage\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"},"program-of-study":{"id":"program-of-study","title":"Program of Study","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eProgram of Study\u003c/h2\u003e\n\u003cp\u003eThe Program of Study for my ISLT degree includes the courses below. Each course lists the term, a concise description, and my reflection linking outcomes to practice and artifacts where applicable.\u003c/p\u003e\n\u003cdiv class=\"table-wrap\"\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCourse\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003cth\u003eTerm\u003c/th\u003e\n\u003cth\u003eReflection\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7355\u003c/strong\u003e\u003cbr/\u003eWeb Design \u0026amp; Development\u003c/td\u003e\n\u003ctd\u003ePrinciples of accessible, standards-based, responsive web design for learning.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eBefore this course, much of my web work was pragmatic: I could make pages function, but the underlying structure was inconsistent and not always accessible. IS_LT 7355 pushed me to slow down and design deliberately. I learned to treat HTML as more than a way to place content on a screen - it became a semantic contract with assistive technologies and a foundation for predictable, reusable components. I adopted landmarks (header, nav, main, footer), meaningful headings, and descriptive link text to improve navigability for screen reader users and keyboard-only users. The shift from \"it looks right\" to \"it\u0027s structured right\" has had a measurable impact on findability and comprehension.\u003c/p\u003e\n\u003cp\u003eAccessibility moved from a checklist to a design habit. I used WCAG 2.1 AA guidelines to evaluate color contrast and reworked palettes to remain legible in bright light and on older devices. I added a visible skip link, ensured logical tab order, and provided focus states that are easy to see. Images now include alt text that communicates purpose rather than appearance, and icon-only links gained accessible names. On mobile, I removed nonessential flourishes, simplified spacing, and set typography for readability at small sizes. These decisions were not merely technical; they respected the realities of my learners\u0027 devices, bandwidth, and attention.\u003c/p\u003e\n\u003cp\u003eResponsiveness became an equity issue as well as a design convenience. I adopted a mobile-first approach and designed content blocks that scale gracefully from small screens to large desktops. Tables were wrapped with responsive containers; long lines were broken into scannable chunks; headings carried meaningful hierarchy so readers could skim effectively. I used CSS to create a coherent visual system - consistent spacing, typographic rhythm, cards with gentle elevation, and clear affordances for links and buttons - so learners could transfer knowledge of one section\u0027s patterns to another.\u003c/p\u003e\n\u003cp\u003eCrucially, I learned to test with real users. Short sessions with students revealed friction points that I would have missed on my own: ambiguous labels, buttons that looked like headings, and instructions that were too dense. I iterated by clarifying link labels (\"Apply for Scholarships\" instead of \"Click here\"), adding microcopy at critical decision points, and reorganizing pages to match the way students actually searched for information. In a few cases, small layout changes - like moving deadlines into a consistent right-hand column and tightening line lengths - reduced confusion and cut time-on-task.\u003c/p\u003e\n\u003cp\u003eI also invested in maintainability. I documented a small design system for the portfolio, including tokens (colors, spacing, typography) and reusable components (navigation, callouts, tables). That system reduces rework when I add artifacts or revise content. Print styles ensure that key pages remain usable offline. I added basic performance considerations - compressing images, deferring noncritical assets - so pages load quickly on school Wi-Fi and cellular networks. These improvements are largely invisible to end users but contribute to a smoother experience.\u003c/p\u003e\n\u003cp\u003eThe impact shows up in student behavior and counselor workflow. Students send fewer \"Where do I find...?\" emails and complete more steps without one-on-one coaching. Counselors can link confidently to pages knowing the structure, labels, and accessibility are consistent. Most importantly, the site now reflects the values I hold as an educator: clarity, inclusion, and respect for learners\u0027 time and context.\u003c/p\u003e\n\u003cp\u003eRepresentative artifact: the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, which demonstrates semantic structure, improved scannability, accessible color contrast, descriptive links, and responsive layouts. The lessons from IS_LT 7355 underpin the rest of my portfolio and continue to guide decisions as I iterate.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 7383\u003c/strong\u003e\u003cbr/\u003eRapid Development Tools\u003c/td\u003e\n\u003ctd\u003eLow-code tools and rapid prototyping for instructional products.\u003c/td\u003e\n\u003ctd\u003e2024 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course reframed my development process from \"complete the product, then gather feedback\" to \"ship a slice, learn, and iterate.\" I adopted a build-measure-learn loop that emphasized quick cycles and concrete evidence. Rather than polishing a full module, I shared small increments - an intro video, a single practice task, a draft visual - and asked students to interact with them. Their responses immediately revealed what resonated and what needed revision, allowing me to redirect effort to the highest-impact changes.\u003c/p\u003e\n\u003cp\u003eLow-code tools accelerated this cadence. Templates, visual editors, and simple data integrations let me prototype without getting bogged down in implementation details. I storyboarded sequences, produced lightweight media, and assembled resources into coherent flows that students could try within a single class period. Because the artifacts were quick to build, they were also quick to change. When analytics and reflections showed that a prompt was unclear or a graphic was distracting, I revised the asset within hours, not days.\u003c/p\u003e\n\u003cp\u003eEvidence gathering was intentionally lightweight and ethical. I favored aggregate indicators - task completion rates, time-on-task, quick pulse checks - over invasive tracking. I paired these with short, structured reflections: one or two questions that helped me understand confusion points and cognitive load without overburdening students. This combination gave me direction while respecting privacy and class time. Over multiple iterations, completion improved and questions shifted from \"What do I do?\" to \"Why does this pattern appear?\" - a sign that the scaffolds were doing their job.\u003c/p\u003e\n\u003cp\u003eOne representative example is the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial Markets Slide Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Early drafts tried to cover too much content per slide, leading to cognitive overload. Student feedback prompted me to reduce text density, pace narration, and use consistent visual metaphors. I aligned each segment with a single learning objective and added brief checks for understanding to surface misconceptions in real time. The result was higher engagement and clearer discussions about cause and effect in market behavior.\u003c/p\u003e\n\u003cp\u003eAnother thread involved collaborative analysis using Python and Google Sheets. Students selected variables, co-constructed datasets, and iterated on visualizations as claims evolved. My role shifted from content deliverer to facilitator of inquiry: I provided exemplars, templates, and nudges, while learners explored multiple pathways to a solution. Rapid iteration - both in the artifacts and in the procedures - helped sustain momentum and made successes visible to the group.\u003c/p\u003e\n\u003cp\u003eJust as important were the pivots. Not every idea worked. Some interactive elements distracted more than they helped; a few templates were too rigid for diverse contexts. Because I was working in small slices, it was easy to remove, replace, or simplify without derailing progress. Those moments reinforced a key lesson: iteration is not a sign of failure but a mechanism for learning for both teacher and students.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, rapid development practices reduced time to value and increased alignment between design intent and learner experience. Students spent more time wrestling with ideas and less time deciphering instructions. For me, the process provided a repeatable, humane way to improve materials: small bets, honest evidence, and steady refinement. See the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e for additional examples and iteration notes that document this approach in practice.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach instructional design. I no longer see myself as a content expert, but rather as a facilitator of learning. I understand that my role is not to create perfect materials, but to create materials that are good enough to spark meaningful learning. I have learned to trust the process of iteration and to see it as a natural part of the design cycle.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more willing to take risks and try new things, knowing that I can always iterate and improve. I am more focused on creating materials that are accessible and inclusive, and I am more intentional about gathering feedback from students and using it to inform my design decisions.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of empathy in instructional design. I have learned to see things from the student\u0027s perspective and to design materials that meet their needs. I have also learned to be more mindful of the emotional and social aspects of learning, and to create materials that are engaging and motivating.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of instructional design and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9410\u003c/strong\u003e\u003cbr/\u003ePython Seminar: Data Pipelines \u0026amp; Dashboards\u003c/td\u003e\n\u003ctd\u003eHands-on Python coding course focused on automating data ingestion, cleaning, and visualization for learning analytics dashboards.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eUsing Python as the core toolset, I built reusable Colab notebooks that authenticate to APIs, pull live CSV extracts, and normalize the data with pandas so counselors receive clean, analysable tables without manual effort.\u003c/p\u003e\n\u003cp\u003eI refactored pipelines into modular functions for requests, validation, and storage, which now keep the Student Risk dashboard, census demographic summaries, and transcript audits up to date with a single run.\u003c/p\u003e\n\u003cp\u003eThe course deepened my proficiency with pandas, requests, and plotting libraries, culminating in automated heatmaps, transcript evaluations, and intake dashboards that anchor the coding artifacts in this portfolio.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9417\u003c/strong\u003e\u003cbr/\u003eAction Research\u003c/td\u003e\n\u003ctd\u003eInquiry cycles to investigate and improve practice.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course transformed reflective practice into systematic inquiry. I framed a practical, context-relevant question - would simplifying the scholarship site\u0027s information architecture improve student follow-through from discovery to application? - and designed a modest study to investigate it. I began with a baseline map of the existing journey, identifying the typical paths students took and where drop-offs occurred. I then documented specific changes (clearer labels, consistent placement of deadlines, shorter instructions, and a persistent \"Apply\" affordance) and predicted how each might reduce friction.\u003c/p\u003e\n\u003cp\u003eI collected evidence with an eye toward proportionality and feasibility. Rather than implementing heavy instrumentation, I tracked a handful of indicators: page visits to key resources, clicks on application links, time-on-task for guidance pages, and the number of initiated applications within a set period. I paired these with short student reflections gathered at natural checkpoints - two or three questions about clarity, confidence, and what they expected to do next. This mixed-methods approach allowed me to see both movement in the numbers and the reasons behind it.\u003c/p\u003e\n\u003cp\u003eAnalysis was iterative. Early results showed more visits to deadlines pages but no corresponding increase in application starts. Student comments revealed that while deadlines were clearer, they still felt overwhelmed by the number of steps. In response, I consolidated instructions into checklists, added concise summaries at the top of longer pages, and created a lightweight \"start here\" page that guided students through the first two actions. The next cycle showed improvement: students reached application pages faster and reported greater confidence about what to do.\u003c/p\u003e\n\u003cp\u003eI also explored communication nudges outside the site. Short, targeted reminders in newsletters - linked to specific actions rather than general announcements - helped maintain momentum. Importantly, I tracked unsubscribes and opt-outs to ensure we were not adding noise. The goal was to support, not pester. These outreach experiments were documented alongside site changes, creating a fuller picture of the ecosystem that influences student behavior.\u003c/p\u003e\n\u003cp\u003eThroughout the study, I attended to ethics and participant burden. All surveys were brief and optional, data were aggregated for reporting, and I avoided collecting sensitive information. Findings were shared back with students and counselors in plain terms, including what we tried that did not make a difference. That transparency built trust and generated better ideas for subsequent cycles.\u003c/p\u003e\n\u003cp\u003eThe most valuable outcome was not a single optimization but a repeatable process. I now approach improvements using small, testable changes, simple measures aligned to the behavior I care about, and short cycles that let me adapt quickly. This stance has spilled over into other projects, from curricular materials to advising workflows. Action research gave me the structure to learn from my own context without waiting for perfect studies or exhaustive datasets.\u003c/p\u003e\n\u003cp\u003eArtifacts across the portfolio - particularly the navigation revisions and microcopy updates in the scholarship resources - trace this inquiry process and the rationale behind each change. The habit of documenting what I expected to happen, what occurred, and what I will try next continues to guide my practice.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9455\u003c/strong\u003e\u003cbr/\u003eDesign Thinking Evaluation\u003c/td\u003e\n\u003ctd\u003eHuman-centered evaluation approaches for learning experiences.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eIn this course, I learned to evaluate learning experiences with empathy and rigor. We began with foundational questions: Who are the learners? What are they trying to accomplish? Where and how will they engage with the materials? These questions anchored every method we used, from heuristic evaluations to think-alouds. I practiced observing without leading, listening for moments of hesitation, and distinguishing between true usability issues and individual preferences. This mindset helped me see barriers I had normalized over time.\u003c/p\u003e\n\u003cp\u003eMy focal project was an evaluation of scholarship resources. I constructed representative tasks (e.g., identify eligibility, locate deadlines, begin an application) and recruited a small, diverse set of students to complete them on their own devices. During think-alouds, I noted places where instructions felt dense, labels were ambiguous, or layout implied the wrong hierarchy. I triangulated these findings with a quick heuristic review focused on consistency, recognition over recall, error prevention, and help and documentation. The convergence of observations and heuristics made it clear where to act first.\u003c/p\u003e\n\u003cp\u003eRevisions prioritized clarity and momentum. I chunked instructions, front-loaded essential information, and used verbs in headings to set expectations (\"Submit FAFSA\" \"Request Transcript\"). I standardized the placement of deadlines and calls-to-action, improved contrast for key buttons, and ensured that the primary path remained visible on mobile screens. Microcopy addressed common points of confusion with short, supportive prompts. I also added small success moments - checkmarks and confirmations - to reinforce progress.\u003c/p\u003e\n\u003cp\u003eMeasuring impact required balancing thoroughness with practicality. I compared task completion times and error rates before and after revisions, and I gathered brief post-task reflections on ease and confidence. The improvements were notable: students reached application links faster, backtracked less, and reported feeling more certain about next steps. Importantly, the changes held up across devices with different screen sizes and input methods.\u003c/p\u003e\n\u003cp\u003eBeyond the immediate project, I developed patterns for scalable evaluation. I created short protocol templates for think-alouds and heuristic reviews, along with a lightweight issue tracker that categorizes findings by severity and effort. These artifacts help me sustain evaluation as an ongoing practice rather than a one-time event. I also learned to frame findings constructively with stakeholders - pairing evidence of friction with concrete, feasible recommendations - so teams feel empowered to act.\u003c/p\u003e\n\u003cp\u003eThe course reinforced that evaluation is part of design, not an afterthought. By building evaluation into the cadence of development, I\u0027m more likely to catch issues before they compound and to keep the learner experience at the center. The strategies I practiced here now inform my work across the portfolio: clear goals, representative tasks, small tests, and respectful observation that leads to actionable insights.\u003c/p\u003e\n\u003cp\u003eEvidence of these changes appears across my artifacts, including the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003eRapid Development\u003c/a\u003e work and the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e, where evaluation shaped sequencing, instructions, and the balance between guidance and autonomy.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9466\u003c/strong\u003e\u003cbr/\u003eLearning Analytics\u003c/td\u003e\n\u003ctd\u003eData collection, analysis, and interpretation to improve learning.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me separate curiosity from usefulness when it comes to data. I began by articulating the questions I actually needed to answer - Are students getting stuck on specific steps? Which explanations reduce confusion? Where does motivation flag? - and only then selecting indicators that could reasonably inform those questions. I resisted the temptation to instrument everything and focused instead on a small set of measures aligned to outcomes: task completion, time-on-task for key pages, error rates, and short reflections that captured perceived clarity and confidence.\u003c/p\u003e\n\u003cp\u003eDesigning the pipeline required attention to proportionality and privacy. I avoided collecting sensitive data that were not essential to learning goals, and I aggregated results whenever possible. For example, rather than logging every click, I tracked transitions between meaningful states (e.g., from \"view deadlines, to \"start application,). I documented retention windows and access controls and communicated in plain language what was being collected and why. This transparency increased trust and reduced anxiety for students and families accustomed to opaque data practices.\u003c/p\u003e\n\u003cp\u003eInterpreting analytics demanded humility. Time-on-task can reflect deep engagement or confusion; completion rates might improve because instructions are clearer or because learners are skipping important steps. To avoid false certainty, I paired quantitative indicators with quick qualitative checks - two-question pulses at natural breakpoints and occasional think-alouds with volunteers. These mixed methods helped me surface causal mechanisms and prioritize changes with the greatest impact on understanding and persistence.\u003c/p\u003e\n\u003cp\u003eWith these practices in place, I used analytics to drive specific improvements. When I noticed extended dwell times on a guidance page with low subsequent application starts, I rewrote the first two paragraphs to front-load purpose and reduced reading burden by chunking steps. I also added a short checklist and a \"What you\"ll need, callout. In the next cycle, dwell time decreased and application starts rose, with student reflections citing \"clearer next steps, as the main factor. In another case, I discovered that mobile users abandoned a long form more frequently; responsive tweaks and a save-and-return affordance reduced drop-offs.\u003c/p\u003e\n\u003cp\u003ePerhaps the most significant lesson was to keep analytics humane. Dashboards were designed for action, not for surveillance: simple visualizations tied to specific decisions, like which pages needed revision or where to place a clarifying prompt. I also built in guardrails - reminders about interpretation limits and links to the evidence behind a proposed change - so conversations stayed grounded. By closing the loop from evidence to iteration and back again, I created a sustainable rhythm of improvement that respected learners\" privacy and time.\u003c/p\u003e\n\u003cp\u003eThe approach now permeates my portfolio. From the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e to scholarship resources, analytics serve design intent: to clarify, to support, and to empower. The course shifted my mindset from \"more data is better, to \"the right data, used ethically, makes learning better.,\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9467\u003c/strong\u003e\u003cbr/\u003eTechnology to Enhance Learning\u003c/td\u003e\n\u003ctd\u003eTechnology integration strategies for impact and equity.\u003c/td\u003e\n\u003ctd\u003e2025 Summer\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course helped me to develop a deeper understanding of how technology can be used to enhance learning. I learned about the different types of technology integration, including substitution, augmentation, modification, and redefinition. I also learned about the importance of considering the SAMR model when designing technology-enhanced lessons.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of equity in technology integration. I learned about the digital divide and how it can impact student learning. I also learned about the importance of providing access to technology for all students, regardless of their background or socioeconomic status.\u003c/p\u003e\n\u003cp\u003eI also learned about the different types of technology that can be used to enhance learning, including learning management systems, educational software, and multimedia tools. I learned about the importance of selecting technology that is aligned with the learning objectives and that is accessible to all students.\u003c/p\u003e\n\u003cp\u003eThroughout the course, I had the opportunity to design and implement technology-enhanced lessons. I learned about the importance of considering the instructional design model when designing lessons, and I learned about the different types of instructional strategies that can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOne of the most significant challenges I faced in this course was learning to navigate the different types of technology. I had to learn how to use new software and tools, and I had to learn how to troubleshoot technical issues. However, with the support of my instructor and my peers, I was able to overcome these challenges and develop a deeper understanding of how technology can be used to enhance learning.\u003c/p\u003e\n\u003cp\u003eOverall, this course was incredibly valuable. I learned about the importance of technology integration in education, and I developed a deeper understanding of how technology can be used to enhance learning. I also developed a range of skills, including instructional design, technology integration, and troubleshooting.\u003c/p\u003e\n\u003cp\u003eAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach teaching and learning. I no longer see technology as a separate entity from instruction, but rather as an integral part of the learning process. I understand that technology can be used to enhance learning, and I am committed to using it in a way that is equitable and accessible to all students.\u003c/p\u003e\n\u003cp\u003eThis shift in mindset has had a profound impact on my practice. I am more intentional about using technology to enhance learning, and I am more focused on creating lessons that are accessible and inclusive. I am also more willing to take risks and try new things, knowing that I can always iterate and improve.\u003c/p\u003e\n\u003cp\u003eOne of the most significant takeaways from this course is the importance of ongoing professional development. I learned about the importance of staying current with the latest research and trends in technology integration, and I learned about the importance of seeking out opportunities for professional growth and development.\u003c/p\u003e\n\u003cp\u003eOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of technology integration and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9471\u003c/strong\u003e\u003cbr/\u003eInstructional Systems Design\u003c/td\u003e\n\u003ctd\u003eSystematic analysis, design, development, and evaluation.\u003c/td\u003e\n\u003ctd\u003e2025 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course operationalized ADDIE in my day-to-day work. In Analysis, I clarified the problem space with stakeholders - counselors, students, and families - using interviews, journey mapping, and quick artifact audits. We defined the gap in terms of learner performance and experience, not just content coverage. From there, I wrote measurable objectives that described what learners should be able to do and under what conditions, aligning them to authentic tasks they actually encounter.\u003c/p\u003e\n\u003cp\u003eDuring Design, I translated objectives into assessments and learning activities. I favored performance tasks over recall, and I arranged sequences to activate prior knowledge, reduce extraneous load, and support transfer. I made accessibility a non-negotiable constraint, specifying patterns (heading hierarchy, alt text, focus states), contrast thresholds, and mobile-first layouts. Prototypes at this stage were intentionally low-fidelity so I could iterate quickly based on feedback.\u003c/p\u003e\n\u003cp\u003eDevelopment focused on turning designs into maintainable artifacts. I built reusable components and wrote microcopy that anticipated sticking points. I organized assets for traceability and reuse, and documented decisions so collaborators could pick up where I left off. Throughout, I validated materials with quick checks - does the assessment truly measure the objective? Does the activity provide the right level of support? - and I fixed mismatches before they calcified.\u003c/p\u003e\n\u003cp\u003eImplementation emphasized readiness. I created facilitator notes, checklists, and support guides, and I rehearsed delivery with colleagues to catch practical issues. I monitored early sessions for surprises and captured them as candidates for the next iteration. Importantly, I framed changes as part of the process, not as evidence that something had \"failed\" which encouraged honest feedback and steady improvement.\u003c/p\u003e\n\u003cp\u003eEvaluation ran through everything, not just the end. I combined formative evidence (observations, quick polls, error patterns) with summative indicators (completion, performance against rubrics) to judge effectiveness. When evidence suggested a misalignment, say, learners succeeded on a task but could not transfer the skill, I revisited objectives and activities to tighten the link. I also reported findings in concise, actionable summaries to stakeholders so decisions about what to keep, revise, or retire were grounded in shared understanding.\u003c/p\u003e\n\u003cp\u003eAcross the portfolio, this disciplined approach improved coherence and outcomes. Artifacts like the \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e show objectives, tasks, and assessments pulling in the same direction, while the rapid prototypes demonstrate how early evaluation prevents costly rework. ADDIE is no longer an abstract model for me; it is a practical rhythm for designing learning that respects people\u0027s time and leads to measurable, equitable gains.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9473\u003c/strong\u003e\u003cbr/\u003eProject Management\u003c/td\u003e\n\u003ctd\u003ePlanning, execution, risk, and stakeholder communication.\u003c/td\u003e\n\u003ctd\u003e2024 Spring\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course provided the scaffolding to deliver learning products predictably and sustainably. I learned to define scope crisply - what is in, what is out, and why - and to translate that scope into milestones with observable outcomes. I time-boxed work, used Kanban-style boards to visualize flow, and limited work in progress so quality did not suffer. These habits reduced context switching and created momentum without requiring heroic efforts.\u003c/p\u003e\n\u003cp\u003eRisk management became a routine, not a formality. I identified technical, content, and schedule risks early and documented triggers, likelihood, and mitigations in a simple log. For web artifacts, recurring risks like link rot and browser quirks received standing checks: automated link health scans, manual spot checks on common devices, and a brief checklist for major releases. By making risk visible, I could negotiate trade-offs honestly with stakeholders - choosing to descale a feature, for instance, to protect a deadline and quality bar.\u003c/p\u003e\n\u003cp\u003eCommunication was another pillar. I wrote succinct status updates that focused on decisions, risks, and next steps rather than play-by-play activity. I set expectations for feedback windows and established one \"source of truth\" for assets to avoid version sprawl. Retrospectives closed each cycle, capturing what worked, what was painful, and what we would try next. Over time, these rituals built trust and reduced surprises.\u003c/p\u003e\n\u003cp\u003eResourcing and scheduling were treated as design problems. I estimated effort using comparative sizing rather than false precision, grouped similar tasks to gain efficiencies, and reserved capacity for the inevitable unknowns. I also created templates - issue trackers, release notes, change logs - that shortened ramp-up for collaborators and preserved context for future revisions.\u003c/p\u003e\n\u003cp\u003eThe payoff is evident across my portfolio. Releases are steadier, quality is more consistent, and maintenance overhead is lower. Students and colleagues benefit from reliable updates and clear communication; I benefit from a process that is humane and sustainable. Project management did not make the work less complex, but it made the path through that complexity clearer and fairer for everyone involved.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eIS_LT 9474\u003c/strong\u003e\u003cbr/\u003eFront End Analysis\u003c/td\u003e\n\u003ctd\u003eNeeds assessment and problem framing techniques.\u003c/td\u003e\n\u003ctd\u003e2024 Fall\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThis course taught me to slow down and frame the right problem before building solutions. I mapped the ecosystem around my learners - students, families, counselors, platforms - and gathered perspectives through interviews, quick surveys, and artifact reviews. I resisted the urge to jump to features and instead synthesized what I heard into jobs-to-be-done and pain points. Journey mapping exposed where energy was lost: unclear eligibility, scattered deadlines, instructions that assumed background knowledge students did not yet have.\u003c/p\u003e\n\u003cp\u003eFrom this analysis, I wrote problem statements that described the gap in observable terms and identified constraints we had to honor (devices, time, bandwidth, privacy). I evaluated candidate solutions against criteria grounded in this reality: Does the change reduce steps? Does it improve comprehension at the moment of decision? Is it sustainable for counselors to maintain? This filtered out ideas that were flashy but fragile and surfaced simpler interventions with outsized impact - clearer labels, checklists, consistent placement of critical information, and targeted microcopy.\u003c/p\u003e\n\u003cp\u003eI also honed techniques for gathering just enough data to move forward. Lightweight card sorts clarified language; five-minute usability checks with students revealed misleading layouts; quick analytics confirmed whether changes produced the expected behavior. Importantly, I documented not only what we learned but what we chose not to do and why, preserving rationale for future teams and preventing drift.\u003c/p\u003e\n\u003cp\u003eFront-end analysis improved alignment across stakeholders. With a shared understanding of the problem and the criteria for success, conversations shifted from personal preference to learner impact. That alignment made subsequent design and development faster and less contentious. It also made evaluation clearer: we knew what we intended to change and how we would know if it worked.\u003c/p\u003e\n\u003cp\u003eUltimately, this course embedded a habit of inquiry that now precedes major changes in my portfolio. I approach new challenges with curiosity and discipline, confident that the time invested up front will pay dividends in clarity, quality, and equity for the learners I serve.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e"},"reflection":{"id":"reflection","title":"Reflection Statement","html_content":"\u003cdiv class=\"container\"\u003e\n\u003ch2\u003eReflection Statement\u003c/h2\u003e\n\u003ch3\u003eContext\u003c/h3\u003e\n\u003cp\u003eWhen I entered the ISLT program, I brought a counselorâ€™s mindset and a financierâ€™s analytical discipline, but my technology practice was largely pragmatic. In daily school work, I made tools â€œwork well enoughâ€ to meet immediate student needs , sharing resources, simplifying instructions, and troubleshooting access barriers. What I lacked was a systematic framework for designing technology-enhanced learning experiences end to end: aligning needs to outcomes, selecting tools intentionally, measuring impact, and then iterating for equity and clarity. I chose the ISLT program, Technology in Schools emphasis, to build that structure around my practice. I wanted to develop reusable patterns for accessible web design, rapid prototyping, and responsible use of learning data that could scale beyond individual interventions. Entering the program, I was comfortable with quantitative reasoning and stakeholder communication from my finance background and counseling experience, but I needed deeper knowledge in interaction design, evaluation, and standards-based technology integration. The ISLT coursework challenged me to move from ad hoc solutions to an evidence-driven, ethically grounded design approach, one that serves students and families across devices, bandwidth, and contexts while protecting their privacy and honoring their time.\u003c/p\u003e\n\u003ch3\u003eSignificant Learnings\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccessibility and clarity are design requirements, not afterthoughts.\u003c/strong\u003e Across projects, I learned that semantic HTML, WCAG-aligned color and typography choices, and mobile-first layouts are not just technical niceties; they directly reduce cognitive load and make resources more actionable for students and families. By treating headings, lists, labels, and link text as navigational cues, especially for screen readers and small screens, I improved findability and comprehension. This shift changed how I write microcopy, structure pages, and audit contrast. It also made my resources more equitable for learners who access materials on mobile devices or with limited bandwidth.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" rel=\"noopener\" target=\"_blank\"\u003eDigital Literacy Curriculum\u003c/a\u003e. This curriculum showcases consistent semantic structure, clear task chunking, and high-contrast, readable typography. It demonstrates how accessible patterns improve the student experience without sacrificing visual appeal.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRapid prototyping accelerates learning and quality.\u003c/strong\u003e The program taught me to plan short, focused build cycles with quick feedback loops, using low-code tools when appropriate. By putting workable prototypes in front of students and colleagues early, I surfaced misconceptions and usability issues before they became entrenched. I adopted a cadence of design sprints, check-ins, and micro-evaluations that helped me converge on effective media sequencing and interaction patterns faster than when I built â€œfinishedâ€ products up front. Rapid development also fostered a culture of co-design, students felt comfortable giving candid feedback because iteration was expected.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/\" rel=\"noopener\" target=\"_blank\"\u003ePython and Google Sheets eLearning Module and States of Matter Animation\u003c/a\u003e. This portfolio collects prototypes that moved from concept to testable artifacts quickly, documenting how feedback informed successive versions and how scope discipline produced better outcomes in less time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData-informed decision-making strengthens instructional impact.\u003c/strong\u003e I learned to select practical, ethical indicators tied to objectives, click-through paths, time on task, completion and error rates, and rubric-aligned performance, so that I could close the loop between design and evidence. Rather than collecting data for its own sake, I now plan the question first, identify the smallest useful signals, and then adjust the resource or activity accordingly. This approach keeps measurement purposeful, protects student privacy, and leads to visible improvements in navigation and comprehension.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eRepresentative artifact:\u003c/em\u003e \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" rel=\"noopener\" target=\"_blank\"\u003eFinancial Markets Slideshow\u003c/a\u003e. Iterations of this multimedia lesson were guided by lightweight analytics and formative checks (e.g., sequence adjustments based on confusion points and pacing). The result is a clearer narrative flow and more targeted prompts that support transfer.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eImpact on Practice and Growth toward ISTE Educator Standards\u003c/h3\u003e\n\u003cp\u003eThe ISLT program has reshaped how I design, facilitate, and evaluate learning experiences. In daily practice, I now begin with learner context and constraints, articulate measurable outcomes, and choose tools that minimize barriers. Prototyping early and often has become standard, and I treat accessibility reviews and contrast checks as nonnegotiable gates for release. My communication artifacts, web pages, multimedia sequences, and counselor-facing resources, are more concise, scannable, and consistent, which has reduced misinterpretation and follow-up overhead for students and families. Equally important, my data practices are lean and purposeful: I gather only what I need, explain how it will be used, and then show stakeholders the improvements it enables.\u003c/p\u003e\n\u003cp\u003eThis work aligns with and advances my growth in the ISTE Educator Standards. As a \u003cstrong\u003eDesigner\u003c/strong\u003e (5aâ€“5c), I build accessible, learner-centered materials with authentic, goal-aligned activities. As a \u003cstrong\u003eFacilitator\u003c/strong\u003e (6aâ€“6d), I create conditions for ownership, using technology to enable exploration, feedback, and creativity while managing tools and strategies responsibly. As an \u003cstrong\u003eAnalyst\u003c/strong\u003e (7aâ€“7c), I employ alternative demonstrations of learning and use assessment data appropriately to inform next steps without over-collecting or compromising privacy. I have also grown as a \u003cstrong\u003eCollaborator\u003c/strong\u003e (4b) by co-learning with students through iterative prototyping, as a \u003cstrong\u003eLeader\u003c/strong\u003e (2b) by advocating equitable access through mobile-first, high-contrast designs, and as a \u003cstrong\u003eCitizen\u003c/strong\u003e (3c) by modeling safe, ethical use of data. Together, these shifts mean I am not merely curating resources but designing learning experiences that are equitable, measurable, and sustainable in a real school setting.\u003c/p\u003e\n\u003cp\u003eGoing forward, I will continue to maintain a reusable design system (components, tokens, and patterns) across counselor resources, expand quick evaluation toolkits to include student-friendly reflection prompts, and formalize lightweight analytics dashboards that respect privacy. Most importantly, I will keep centering accessibility and student voice, ensuring that the artifacts I produce help students navigate opportunities confidently, make informed decisions, and demonstrate their learning in multiple ways.\u003c/p\u003e\n\u003c/div\u003e"},"program-goals":{"id":"program-goals","title":"Program Goals \u0026 Outcomes","html_content":"\u003cdiv class=\"program-goals-content\"\u003e\n\u003ch2 data-toggable=\"1\"\u003eLearning Technologies Program Goals and Student Learning Outcomes\u003c/h2\u003e\n\n\u003cp class=\"intro\"\u003eThe Learning Technologies (LTD) program prepares students to systematically design, develop, evaluate, and lead learning technology and instructional solutions within schools, universities, corporations, and public and private organizations. For me, this program has been the bridge between my day to day work as a high school counselor and data systems designer in Saint Louis Public Schools and my longer term goal of pursuing a PhD that examines how real time feedback, interoperable data systems, and equitable analytics can change students\u0027 postsecondary trajectories.\u003c/p\u003e\n\n\u003cp class=\"goal-label\"\u003e\u003cstrong\u003eGoal 1:\u003c/strong\u003e Students will develop theory and research-based skills for innovative, aesthetic, accessible, equitable, effective and sustainable design and development of technologies for learning opportunities and systems.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.1:\u003c/strong\u003e Students will design and develop learning and performance opportunities and systems including direct instruction, student-centered learning, collaborative work, and performance support.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.2:\u003c/strong\u003e Students will design and develop learning opportunities and systems for meaningful learning; promote student engagement in online learning environments; and select appropriate technology and learning objects to support learners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.3:\u003c/strong\u003e Students can conduct project management activities to support the total lifecycle of design, development and implementation of learning opportunities and systems.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 1.4:\u003c/strong\u003e Students will demonstrate mastery of technological production skills in web development, digital asset creation, and other applicable development skills necessary for the creation of learning systems (for example, web page prototyping, game design, digital media).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp class=\"goal-label\"\u003e\u003cstrong\u003eGoal 2:\u003c/strong\u003e Students will learn to apply systematic methods for analyzing instructional or training needs, applying those results to instructional design and development, and collecting and analyzing formative and summative evaluation data to improve the instructional product.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 2.1:\u003c/strong\u003e Students can choose and conduct appropriate analytical methods and apply the results to the systematic design and development of learning environments and systems.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 2.2:\u003c/strong\u003e Students will design assessments aligned with the intended outcomes of the learning environments, choose and implement appropriate methods to conduct formative and summative evaluation of the learning environment, and use data to continuously improve the instructional product or products.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp class=\"goal-label\"\u003e\u003cstrong\u003eGoal 3:\u003c/strong\u003e Students will understand the impact of technologies we create or use on society at large and strive to produce ethical, inclusive, and equitable instructional systems and technologies.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 3.1:\u003c/strong\u003e Students will demonstrate ethical and inclusive practices to all aspects of analytical data collection and analysis methods used in the design and evaluation of instructional products or learning environments (for example, sampling methods).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStudent Learning Outcome 3.2:\u003c/strong\u003e Students will design and implement instructional products and learning environments based upon Universal Design Principles.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 1.1\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and develop learning and performance opportunities and systems including direct instruction, student-centered learning, collaborative work, and performance support.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eMy understanding of design moved from assembling isolated activities to orchestrating systems that support learning across different modalities. As a counselor and data practitioner, I have always been attentive to individual students and specific decisions, but the program forced me to think in terms of learning ecosystems, where direct instruction, inquiry, collaboration, and performance support are layered intentionally rather than chosen in isolation.\u003c/p\u003e\n\n\u003cp\u003eI learned to see direct instruction as a way to provide cognitive anchors that reduce unnecessary search, student-centered learning as a space where students experiment and make sense of those anchors, collaborative work as a structure for distributed sense making, and performance support as the infrastructure that allows learners and educators to act at the moment of need. This shift has already changed how I design tools for SLPS. For example, when I guide students through complex digital tasks or postsecondary planning, I now deliberately combine clear instructional explanations with spaces for exploration and tools that support ongoing decision making.\u003c/p\u003e\n\n\u003cp\u003eThis integrated approach is visible across several artifacts. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning module\u003c/a\u003e uses concise, structured explanations followed by scenario-based activities where students apply credibility checks and safe use practices on their own. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part2_part2.html\" target=\"_blank\" rel=\"noopener\"\u003ePython Code Simulation\u003c/a\u003e invites learners to manipulate parameters and observe changing outputs while still being guided by scaffolded examples that keep cognitive demand productive rather than overwhelming. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/part1_part8.html\" target=\"_blank\" rel=\"noopener\"\u003eLinear Regression Manipulative\u003c/a\u003e supports collaborative data exploration, giving groups a shared visual object to discuss and adjust while built-in hints and constraints function as performance support. Together, these artifacts show that I can now design learning systems that move flexibly across instructional modes in service of deeper understanding.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 1.2\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and develop learning opportunities and systems for meaningful learning; promote student engagement in online learning environments; and select appropriate technology and learning objects to support learners.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eMeaningful learning has become a central thread that ties my EdS work to my long term research interests in real time feedback and postsecondary attainment. The program pushed me to move beyond \"covering\" content and instead design experiences where students see why the content matters, how it is structured, and how it connects to their own aspirations. I now evaluate my design choices through questions such as: Will this format help learners build a coherent mental model? Does it invite them to make predictions, explain their reasoning, and apply ideas in authentic contexts? Does the technology used here amplify understanding, or is it merely decorative?\u003c/p\u003e\n\n\u003cp\u003eLearning about media theory and cognitive psychology helped me become more disciplined in selecting technologies that promote engagement without adding unnecessary complexity. I now use narration, visuals, and text intentionally rather than as a checklist of modalities. I think about pacing, signaling, and interactivity as levers that can either support or interfere with students\u0027 capacity to follow complex arguments. These principles now guide how I design both classroom-facing materials and the interfaces that support counselors and students navigating high-stakes decisions.\u003c/p\u003e\n\n\u003cp\u003eThese commitments are evident in the \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning module\u003c/a\u003e, where I carefully coordinated narration with visuals and used interactive prompts to help learners test their understanding of concepts like liquidity and risk. At the program level, I applied the same principles to my \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e section, where I designed an information structure that allows readers to scan for high level themes while still accessing detailed reflections and course level evidence when they choose to dive deeper. My \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#reflection\" target=\"_blank\" rel=\"noopener\"\u003eReflection Statement\u003c/a\u003e models meaningful learning and engagement for practitioners by demonstrating how structured reflection can synthesize disparate experiences into an integrated professional identity. Through these designs, I have come to see engagement less as entertainment and more as alignment between structure, context, and learner purpose.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 1.3\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents can conduct project management activities to support the total lifecycle of design, development and implementation of learning opportunities and systems.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThis outcome marks one of the closest intersections between my EdS coursework and my ongoing work in SLPS. Before the program, much of my technology work resembled heroic effort rather than managed projects. I would push a promising idea far enough to solve an immediate problem, then move on without fully systematizing its development, documentation, or sustainability. Through LTD coursework and applied assignments, I learned to treat each instructional system as a full lifecycle project: starting with stakeholder analysis and scoping, moving through design and development, and continuing into deployment, training, maintenance, and iteration.\u003c/p\u003e\n\n\u003cp\u003eI now think in terms of project charters, milestone planning, dependency mapping, and risk management, even when the product is \"just\" a dashboard, an eLearning module, or a Python script. This has been crucial for my broader goal of building durable, district level infrastructures for feedback and advising. Tools that depend on a single practitioner\u0027s tacit knowledge do not scale. The program helped me acquire both the language and the habits of project management so that my designs can be adopted, maintained, and improved by others.\u003c/p\u003e\n\n\u003cp\u003eThis shift is evident in the design and rollout of my at risk monitoring system. The \u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard (Python and HTML)\u003c/a\u003e required careful coordination of data feeds, end user needs, and technical constraints, and I approached it as a multi phase project rather than a one off build. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e translates that project into a reproducible blueprint that describes the workflow, data sources, automation schedule, and maintenance requirements. Even the earlier \u003ca href=\"https://youtu.be/2Jbn5Da5mYs\" target=\"_blank\" rel=\"noopener\"\u003eExcel Student Risk Dashboard\u003c/a\u003e demanded a structured project plan, including scoping indicators, aligning columns with counselor workflows, and planning for training and handoff to school teams. At a broader scale, my \u003ca href=\"https://everettstuckey.github.io/islt_7310/assets/coverflow/learn_and_earn_diagram.png\" target=\"_blank\" rel=\"noopener\"\u003eInternship Program Data Pipeline\u003c/a\u003e models how I now think about whole lifecycles by mapping student applications, employer data, and placement tracking into a coherent pipeline supported by Teams and Excel. Collectively, these projects show that I can manage complex instructional technology initiatives from concept through institutionalization.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 1.4\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will demonstrate mastery of technological production skills in web development, digital asset creation, and other applicable development skills necessary for the creation of learning systems (for example, web page prototyping, game design, digital media).\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThis outcome has been critical for building my identity as a scholar practitioner who can both theorize and produce. The EdS program required me to work across a range of production environments: HTML, CSS, JavaScript, Python, multimedia editors, GIS tools, and data visualization platforms. Over time, I began to see these not as disconnected skill sets, but as a shared language for expressing learning designs in concrete, testable forms.\u003c/p\u003e\n\n\u003cp\u003eDeveloping production fluency has also shifted how I think about leadership. Being able to design and implement a prototype myself changes the conversation with IT staff, principals, and teachers. Instead of describing a desired system in abstract terms, I can show a working version, gather feedback, and then iterate. This fits directly with my long term goal of bridging research, design, and implementation in real K-12 contexts. My production skills now allow me to prototype research ideas in the same local systems that shape students\u0027 daily experiences.\u003c/p\u003e\n\n\u003cp\u003eSeveral artifacts demonstrate this breadth. My \u003ca href=\"https://everettstuckey.github.io/islt_7310/\" target=\"_blank\" rel=\"noopener\"\u003eProfessional Webpage\u003c/a\u003e is a fully hand coded site that integrates responsive layout, accessible navigation, and a coherent visual identity to communicate my work as an EdS candidate and district leader. The \u003ca href=\"https://youtu.be/W4DFjhLx0wY\" target=\"_blank\" rel=\"noopener\"\u003eStates of Matter Animation\u003c/a\u003e shows that I can create digital media assets that explain complex phenomena visually and temporally, aligning motion graphics with conceptual progression. The \u003ca href=\"https://youtu.be/IVYAi723RLM\" target=\"_blank\" rel=\"noopener\"\u003eWeb Scraping Tutorial\u003c/a\u003e translates advanced Python techniques into an instructional video that both demonstrates code execution and explains the reasoning behind each step. My \u003ca href=\"https://everettstuckey.github.io/islt_7310/SemifinalistsGISMap.html\" target=\"_blank\" rel=\"noopener\"\u003eGIS Map of Career Z and CHIPS CTE Challenge Semifinalists\u003c/a\u003e extends these skills into spatial visualization, presenting geographic patterns in opportunity pathways in a form that school and district leaders can immediately interpret. Across these examples, production is not just technical proficiency; it is a vehicle for making educational ideas tangible.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 2.1\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents can choose and conduct appropriate analytical methods and apply the results to the systematic design and development of learning environments and systems.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eMy work in this outcome sits at the heart of my professional trajectory toward learning analytics and educational data science. The EdS program gave me a stronger toolkit for choosing analytical methods that fit the question, the data, and the ethical stakes, rather than reaching reflexively for whatever analysis is most familiar. I learned to think carefully about unit of analysis, sources of bias, and the interpretability demands of the audiences who will use the results.\u003c/p\u003e\n\n\u003cp\u003eIn practice, this meant learning to move fluently between descriptive statistics, visual pattern recognition, and more complex approaches, and then tying those analytic choices directly to design implications. When I analyze grade patterns, course enrollments, or scholarship application flows, I now ask not only \"what is happening\" but also \"how should we redesign systems, interfaces, prompts, or supports to respond.\" That orientation connects directly to my aspiration to study how lower latency and more interpretable data can change counselor, teacher, and student behavior.\u003c/p\u003e\n\n\u003cp\u003eSeveral artifacts serve as concrete evidence of this analytic design cycle. The \u003ca href=\"https://colab.research.google.com/drive/1lXuPQLNkrZvcnsP0Y00BZgRuf7tXp1Io?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eHeatmap Visualization Script\u003c/a\u003e uses Python to reveal temporal and categorical patterns that are difficult to see in tabular form, guiding where to focus subsequent interventions. The \u003ca href=\"https://colab.research.google.com/drive/1kxPYj4eiCoeiXmsywbf9a1BbNu-m3lTw?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eTranscript Evaluation Script\u003c/a\u003e automates complex eligibility and progress checks, and the structure of the notebook reflects deliberate analytic decisions about how to categorize, validate, and flag key conditions for advisors. The \u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e integrates neighborhood level demographic data with school level information, allowing more contextually grounded analyses of opportunity and risk. In each case, the analysis is not an end in itself but a driver of better system design.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 2.2\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design assessments aligned with the intended outcomes of the learning environments, choose and implement appropriate methods to conduct formative and summative evaluation of the learning environment, and use data to continuously improve the instructional product or products.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThis outcome has shaped how I think about evidence and improvement, both in digital learning contexts and in district level advising systems. I entered the program comfortable with high stakes indicators like ACT scores or graduation rates. The EdS work helped me learn to design assessments that are much closer to the learning experience, aligned tightly with specific objectives, and useful for continuous improvement rather than only for accountability.\u003c/p\u003e\n\n\u003cp\u003eI learned to distinguish between formative checks that surface misconceptions in real time and summative measures that provide an overall judgment of mastery. More importantly, I began to design instructional products with assessment in mind from the start, rather than bolting on quizzes at the end. That shift mirrors how I now think about counselor and administrator tools: a dashboard or script is not complete until it includes mechanisms for monitoring its own effectiveness and informing its own revision.\u003c/p\u003e\n\n\u003cp\u003eIn my \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning module\u003c/a\u003e, each scenario and prompt is aligned with specific outcomes around safe technology use, source evaluation, and responsible participation; embedded checks provide low stakes opportunities for learners to test their reasoning and receive feedback. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning module\u003c/a\u003e uses comprehension checks, structured prompts, and pacing decisions that reflect an explicit theory about how learners should be able to explain and apply concepts by the end of the experience. At a systems level, the \u003ca href=\"https://everettstuckey.github.io/islt_7310/ICU_Report.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Student Dashboard\u003c/a\u003e and related tools are themselves part of an evaluative loop; they allow counselors and administrators to see which students are being reached, which supports are triggered, and where there are persistent gaps, which then informs subsequent iterations of the dashboard and associated workflows. In all of these examples, assessment is not just measurement; it is the mechanism that makes improvement possible.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 3.1\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will demonstrate ethical and inclusive practices to all aspects of analytical data collection and analysis methods used in the design and evaluation of instructional products or learning environments (for example, sampling methods).\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThis outcome speaks directly to the ethical tensions I encounter daily in my SLPS work. Building advisory dashboards, prediction models, or performance reports means deciding what to measure, whom to include, and how to categorize students. The EdS program gave me a framework for treating these choices as ethical decisions rather than neutral technical steps. I now think deliberately about sampling, missingness, representation, fairness, and interpretability whenever I design data collection or analysis methods.\u003c/p\u003e\n\n\u003cp\u003eI have become more cautious about which variables I incorporate, how I label risk categories, and how I present differences between groups. When I disaggregate outcomes by school, demographic group, or neighborhood, I ask whether the analysis helps adults change their behavior in ways that benefit students, or whether it risks reinforcing deficit narratives. I also think about consent and transparency, especially when students\u0027 data are being repurposed across systems like SIS, LMS, and external platforms.\u003c/p\u003e\n\n\u003cp\u003eThese commitments are reflected in several analytics-focused artifacts. The \u003ca href=\"https://youtu.be/TpqWWJGb1Yo\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard (Python and HTML)\u003c/a\u003e is built around clear indicator definitions and transparent logic, so that counselors can see why a student is flagged rather than experiencing the dashboard as a black box. The \u003ca href=\"https://everettstuckey.github.io/islt_7310/AtRiskDashboardDocumentation.html\" target=\"_blank\" rel=\"noopener\"\u003eAt-Risk Dashboard Documentation\u003c/a\u003e makes the data sources, filters, and automation steps explicit, offering a form of methodological transparency that aligns with ethical practice. The \u003ca href=\"https://colab.research.google.com/drive/1C8pYam8D-6yMlc3vj0TOsg84S70tIvO4?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003eCensus Data Retrieval Script\u003c/a\u003e and related tools help incorporate contextual information in ways that foreground structural conditions rather than individual blame. By integrating these ethical considerations into concrete designs, I grow closer to my long term goal of building learning analytics systems that expand opportunity rather than simply describe inequality.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003ch2 data-toggable=\"1\"\u003eStudent Learning Outcome 3.2\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStudents will design and implement instructional products and learning environments based upon Universal Design Principles.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eUniversal Design for Learning (UDL) has become a lens through which I now view almost all of my instructional and advising work. The program helped me move beyond a narrow understanding of accessibility as compliance and into a richer view of UDL as anticipating learner variability in perception, language, executive function, and motivation. That perspective is especially important in SLPS, where students\u0027 access to devices, bandwidth, prior academic preparation, and support outside school varies widely.\u003c/p\u003e\n\n\u003cp\u003eIn my design work, I now ask how a product will function for a student reading on a phone, a student who prefers listening, a student who needs more time to process, and a student who is already advanced in the content. I think about multiple means of representation, engagement, and action as mutually reinforcing commitments rather than separate checkboxes. These principles also inform my postsecondary data tools; UDL is not only for classroom instruction but also for the systems that students and staff use to understand risk, opportunity, and progress.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html\" target=\"_blank\" rel=\"noopener\"\u003eDigital Literacy eLearning module\u003c/a\u003e embodies UDL by offering text, visuals, and scenarios for representation, relatable and choice-based examples for engagement, and multiple ways for learners to express understanding, including written responses and applied tasks. The \u003ca href=\"https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html\" target=\"_blank\" rel=\"noopener\"\u003eFinancial Markets eLearning module\u003c/a\u003e reflects UDL thinking in its pacing, use of consistent metaphors, and optional review segments that allow learners to revisit complex ideas without penalty. My \u003ca href=\"https://everettstuckey.github.io/islt_7310/portfolio.html#program-of-study\" target=\"_blank\" rel=\"noopener\"\u003eProgram of Study\u003c/a\u003e section applies UDL principles to information design by providing both tabular and narrative views of my coursework, along with collapsible sections that let readers control the level of detail they engage with. Through these and related designs, I have come to see UDL as foundational to my aspiration to build systems that work for every learner, not just those for whom schools were originally designed.\u003c/p\u003e\n\n\u003c/div\u003e"}};