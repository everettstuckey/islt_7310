const PORTFOLIO_COURSE_DATA = {
  "courses": [
    {
      "course_code": "IS_LT 7355",
      "title": "Web Design & Development",
      "description": "Principles of accessible, standards-based, responsive web design for learning.",
      "term": "2024 Summer",
      "reflection": "Before this course, much of my web work was pragmatic: I could make pages function, but the underlying structure was inconsistent and not always accessible. IS_LT 7355 pushed me to slow down and design deliberately. I learned to treat HTML as more than a way to place content on a screen - it became a semantic contract with assistive technologies and a foundation for predictable, reusable components. I adopted landmarks (header, nav, main, footer), meaningful headings, and descriptive link text to improve navigability for screen reader users and keyboard-only users. The shift from \"it looks right\" to \"it's structured right\" has had a measurable impact on findability and comprehension.\n\nAccessibility moved from a checklist to a design habit. I used WCAG 2.1 AA guidelines to evaluate color contrast and reworked palettes to remain legible in bright light and on older devices. I added a visible skip link, ensured logical tab order, and provided focus states that are easy to see. Images now include alt text that communicates purpose rather than appearance, and icon-only links gained accessible names. On mobile, I removed nonessential flourishes, simplified spacing, and set typography for readability at small sizes. These decisions were not merely technical; they respected the realities of my learners' devices, bandwidth, and attention.\n\nResponsiveness became an equity issue as well as a design convenience. I adopted a mobile-first approach and designed content blocks that scale gracefully from small screens to large desktops. Tables were wrapped with responsive containers; long lines were broken into scannable chunks; headings carried meaningful hierarchy so readers could skim effectively. I used CSS to create a coherent visual system - consistent spacing, typographic rhythm, cards with gentle elevation, and clear affordances for links and buttons - so learners could transfer knowledge of one section's patterns to another.\n\nCrucially, I learned to test with real users. Short sessions with students revealed friction points that I would have missed on my own: ambiguous labels, buttons that looked like headings, and instructions that were too dense. I iterated by clarifying link labels (\"Apply for Scholarships\" instead of \"Click here\"), adding microcopy at critical decision points, and reorganizing pages to match the way students actually searched for information. In a few cases, small layout changes - like moving deadlines into a consistent right-hand column and tightening line lengths - reduced confusion and cut time-on-task.\n\nI also invested in maintainability. I documented a small design system for the portfolio, including tokens (colors, spacing, typography) and reusable components (navigation, callouts, tables). That system reduces rework when I add artifacts or revise content. Print styles ensure that key pages remain usable offline. I added basic performance considerations - compressing images, deferring noncritical assets - so pages load quickly on school Wi-Fi and cellular networks. These improvements are largely invisible to end users but contribute to a smoother experience.\n\nThe impact shows up in student behavior and counselor workflow. Students send fewer \"Where do I find...?\" emails and complete more steps without one-on-one coaching. Counselors can link confidently to pages knowing the structure, labels, and accessibility are consistent. Most importantly, the site now reflects the values I hold as an educator: clarity, inclusion, and respect for learners' time and context.\n\nRepresentative artifact: the Digital Literacy Curriculum , which demonstrates semantic structure, improved scannability, accessible color contrast, descriptive links, and responsive layouts. The lessons from IS_LT 7355 underpin the rest of my portfolio and continue to guide decisions as I iterate.",
      "materials": [
        {
          "title": "Digital Literacy Curriculum",
          "href": "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html"
        }
      ]
    },
    {
      "course_code": "IS_LT 7383",
      "title": "Rapid Development Tools",
      "description": "Low-code tools and rapid prototyping for instructional products.",
      "term": "2024 Summer",
      "reflection": "This course reframed my development process from \"complete the product, then gather feedback\" to \"ship a slice, learn, and iterate.\" I adopted a build-measure-learn loop that emphasized quick cycles and concrete evidence. Rather than polishing a full module, I shared small increments - an intro video, a single practice task, a draft visual - and asked students to interact with them. Their responses immediately revealed what resonated and what needed revision, allowing me to redirect effort to the highest-impact changes.\n\nLow-code tools accelerated this cadence. Templates, visual editors, and simple data integrations let me prototype without getting bogged down in implementation details. I storyboarded sequences, produced lightweight media, and assembled resources into coherent flows that students could try within a single class period. Because the artifacts were quick to build, they were also quick to change. When analytics and reflections showed that a prompt was unclear or a graphic was distracting, I revised the asset within hours, not days.\n\nEvidence gathering was intentionally lightweight and ethical. I favored aggregate indicators - task completion rates, time-on-task, quick pulse checks - over invasive tracking. I paired these with short, structured reflections: one or two questions that helped me understand confusion points and cognitive load without overburdening students. This combination gave me direction while respecting privacy and class time. Over multiple iterations, completion improved and questions shifted from \"What do I do?\" to \"Why does this pattern appear?\" - a sign that the scaffolds were doing their job.\n\nOne representative example is the Financial Markets Slideshow . Early drafts tried to cover too much content per slide, leading to cognitive overload. Student feedback prompted me to reduce text density, pace narration, and use consistent visual metaphors. I aligned each segment with a single learning objective and added brief checks for understanding to surface misconceptions in real time. The result was higher engagement and clearer discussions about cause and effect in market behavior.\n\nAnother thread involved collaborative analysis using Python and Google Sheets. Students selected variables, co-constructed datasets, and iterated on visualizations as claims evolved. My role shifted from content deliverer to facilitator of inquiry: I provided exemplars, templates, and nudges, while learners explored multiple pathways to a solution. Rapid iteration - both in the artifacts and in the procedures - helped sustain momentum and made successes visible to the group.\n\nJust as important were the pivots. Not every idea worked. Some interactive elements distracted more than they helped; a few templates were too rigid for diverse contexts. Because I was working in small slices, it was easy to remove, replace, or simplify without derailing progress. Those moments reinforced a key lesson: iteration is not a sign of failure but a mechanism for learning for both teacher and students.\n\nAcross the portfolio, rapid development practices reduced time to value and increased alignment between design intent and learner experience. Students spent more time wrestling with ideas and less time deciphering instructions. For me, the process provided a repeatable, humane way to improve materials: small bets, honest evidence, and steady refinement. See the Rapid Development Tools Portfolio for additional examples and iteration notes that document this approach in practice.\n\nAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach instructional design. I no longer see myself as a content expert, but rather as a facilitator of learning. I understand that my role is not to create perfect materials, but to create materials that are good enough to spark meaningful learning. I have learned to trust the process of iteration and to see it as a natural part of the design cycle.\n\nThis shift in mindset has had a profound impact on my practice. I am more willing to take risks and try new things, knowing that I can always iterate and improve. I am more focused on creating materials that are accessible and inclusive, and I am more intentional about gathering feedback from students and using it to inform my design decisions.\n\nOne of the most significant takeaways from this course is the importance of empathy in instructional design. I have learned to see things from the student's perspective and to design materials that meet their needs. I have also learned to be more mindful of the emotional and social aspects of learning, and to create materials that are engaging and motivating.\n\nOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of instructional design and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work.",
      "materials": [
        {
          "title": "Financial Markets Slideshow",
          "href": "https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial Markets Slide Show.html"
        },
        {
          "title": "Rapid Development Tools Portfolio",
          "href": "https://everettstuckey.github.io/IS_LT-7383/"
        }
      ]
    },
    {
      "course_code": "IS_LT 9410",
      "title": "Seminar in Info Sci/Learn Tech",
      "description": "Current issues and scholarship in learning technologies.",
      "term": "2024 Fall",
      "reflection": "This seminar grounded my practice in contemporary scholarship and helped me develop a principled stance toward technology in learning. Weekly readings and discussions pushed me beyond tool comparisons to examine the social, ethical, and policy dimensions of technology adoption. We interrogated assumptions about personalization, automation, and data-driven decision making, weighing potential benefits against risks like surveillance creep, algorithmic bias, and loss of student agency. This perspective was invaluable as I refined my own criteria for choosing tools and methods in school settings where trust and equity matter.\n\nA major focus for me was privacy and proportionality in data collection. I mapped the life cycle of student data in my artifacts - from collection to storage to access to deletion - and identified where my practices needed to change. I rewrote consent language in plain terms, clarified what indicators I would collect and why, and set shorter retention windows for nonessential data. I also documented de-identification techniques (e.g., hashing, aggregation, suppression of small-n categories) so I could learn from patterns without exposing individuals. These steps were paired with transparent communication to students and families about what information was being used and how it would improve instruction.\n\nThe course also sharpened my ability to critically evaluate research claims. We analyzed methodologies, sample sizes, measures of learning, and the generalizability of results. This training helped me avoid overinterpreting analytics and reminded me to anchor decisions in multiple forms of evidence - including learner reflections and observational data, not just clickstream metrics. I brought this lens back to my portfolio work, combining quantitative indicators with qualitative insights to create a more complete picture of learner experience.\n\nWe discussed accessibility and inclusion not as add-ons but as design requirements. I reflected on who benefits - and who might be left out - when a school adopts a particular platform or workflow. This led me to prioritize mobile-friendly designs, low-bandwidth alternatives, and offline access when feasible. I also examined how language and imagery can unintentionally exclude or discourage participation, prompting me to revise content for broader cultural relevance and readability.\n\nThe seminar format modeled community learning: we co-constructed norms for respectful debate, shared examples from our contexts, and iterated on position statements over time. That experience influenced how I facilitate feedback with colleagues. I now host shorter, more frequent conversations focused on specific claims (e.g., \"This dashboard helps counselors spot students who need support earlier\") and invite counter-examples to test assumptions. The goal is not consensus for its own sake, but clarity about trade-offs and alignment with our values.\n\nBy the end of the term, I had a tighter feedback loop between ethics and implementation. I adopted a \"privacy by design\" mindset for new features, began documenting data decisions in plain language, and created a short checklist I now use before introducing technology to students. These practices improved trust and made it easier to explain choices to administrators and families. The seminar did not hand me universal answers; it equipped me with better questions and a framework for making decisions transparently and responsibly.\n\nRepresentative connections across my portfolio include clearer consent and data notes within the Rapid Development artifacts and the Digital Literacy Curriculum , where ethical considerations and equitable access are explicitly addressed.",
      "materials": [
        {
          "title": "Rapid Development",
          "href": "https://everettstuckey.github.io/IS_LT-7383/"
        },
        {
          "title": "Digital Literacy Curriculum",
          "href": "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html"
        }
      ]
    },
    {
      "course_code": "IS_LT 9417",
      "title": "Action Research",
      "description": "Inquiry cycles to investigate and improve practice.",
      "term": "2025 Summer",
      "reflection": "This course transformed reflective practice into systematic inquiry. I framed a practical, context-relevant question - would simplifying the scholarship site's information architecture improve student follow-through from discovery to application? - and designed a modest study to investigate it. I began with a baseline map of the existing journey, identifying the typical paths students took and where drop-offs occurred. I then documented specific changes (clearer labels, consistent placement of deadlines, shorter instructions, and a persistent \"Apply\" affordance) and predicted how each might reduce friction.\n\nI collected evidence with an eye toward proportionality and feasibility. Rather than implementing heavy instrumentation, I tracked a handful of indicators: page visits to key resources, clicks on application links, time-on-task for guidance pages, and the number of initiated applications within a set period. I paired these with short student reflections gathered at natural checkpoints - two or three questions about clarity, confidence, and what they expected to do next. This mixed-methods approach allowed me to see both movement in the numbers and the reasons behind it.\n\nAnalysis was iterative. Early results showed more visits to deadlines pages but no corresponding increase in application starts. Student comments revealed that while deadlines were clearer, they still felt overwhelmed by the number of steps. In response, I consolidated instructions into checklists, added concise summaries at the top of longer pages, and created a lightweight \"start here\" page that guided students through the first two actions. The next cycle showed improvement: students reached application pages faster and reported greater confidence about what to do.\n\nI also explored communication nudges outside the site. Short, targeted reminders in newsletters - linked to specific actions rather than general announcements - helped maintain momentum. Importantly, I tracked unsubscribes and opt-outs to ensure we were not adding noise. The goal was to support, not pester. These outreach experiments were documented alongside site changes, creating a fuller picture of the ecosystem that influences student behavior.\n\nThroughout the study, I attended to ethics and participant burden. All surveys were brief and optional, data were aggregated for reporting, and I avoided collecting sensitive information. Findings were shared back with students and counselors in plain terms, including what we tried that did not make a difference. That transparency built trust and generated better ideas for subsequent cycles.\n\nThe most valuable outcome was not a single optimization but a repeatable process. I now approach improvements using small, testable changes, simple measures aligned to the behavior I care about, and short cycles that let me adapt quickly. This stance has spilled over into other projects, from curricular materials to advising workflows. Action research gave me the structure to learn from my own context without waiting for perfect studies or exhaustive datasets.\n\nArtifacts across the portfolio - particularly the navigation revisions and microcopy updates in the scholarship resources - trace this inquiry process and the rationale behind each change. The habit of documenting what I expected to happen, what occurred, and what I will try next continues to guide my practice."
    },
    {
      "course_code": "IS_LT 9455",
      "title": "Design Thinking Evaluation",
      "description": "Human-centered evaluation approaches for learning experiences.",
      "term": "2025 Spring",
      "reflection": "In this course, I learned to evaluate learning experiences with empathy and rigor. We began with foundational questions: Who are the learners? What are they trying to accomplish? Where and how will they engage with the materials? These questions anchored every method we used, from heuristic evaluations to think-alouds. I practiced observing without leading, listening for moments of hesitation, and distinguishing between true usability issues and individual preferences. This mindset helped me see barriers I had normalized over time.\n\nMy focal project was an evaluation of scholarship resources. I constructed representative tasks (e.g., identify eligibility, locate deadlines, begin an application) and recruited a small, diverse set of students to complete them on their own devices. During think-alouds, I noted places where instructions felt dense, labels were ambiguous, or layout implied the wrong hierarchy. I triangulated these findings with a quick heuristic review focused on consistency, recognition over recall, error prevention, and help and documentation. The convergence of observations and heuristics made it clear where to act first.\n\nRevisions prioritized clarity and momentum. I chunked instructions, front-loaded essential information, and used verbs in headings to set expectations (\"Submit FAFSA\" \"Request Transcript\"). I standardized the placement of deadlines and calls-to-action, improved contrast for key buttons, and ensured that the primary path remained visible on mobile screens. Microcopy addressed common points of confusion with short, supportive prompts. I also added small success moments - checkmarks and confirmations - to reinforce progress.\n\nMeasuring impact required balancing thoroughness with practicality. I compared task completion times and error rates before and after revisions, and I gathered brief post-task reflections on ease and confidence. The improvements were notable: students reached application links faster, backtracked less, and reported feeling more certain about next steps. Importantly, the changes held up across devices with different screen sizes and input methods.\n\nBeyond the immediate project, I developed patterns for scalable evaluation. I created short protocol templates for think-alouds and heuristic reviews, along with a lightweight issue tracker that categorizes findings by severity and effort. These artifacts help me sustain evaluation as an ongoing practice rather than a one-time event. I also learned to frame findings constructively with stakeholders - pairing evidence of friction with concrete, feasible recommendations - so teams feel empowered to act.\n\nThe course reinforced that evaluation is part of design, not an afterthought. By building evaluation into the cadence of development, I'm more likely to catch issues before they compound and to keep the learner experience at the center. The strategies I practiced here now inform my work across the portfolio: clear goals, representative tasks, small tests, and respectful observation that leads to actionable insights.\n\nEvidence of these changes appears across my artifacts, including the Rapid Development work and the Digital Literacy Curriculum , where evaluation shaped sequencing, instructions, and the balance between guidance and autonomy.",
      "materials": [
        {
          "title": "Rapid Development",
          "href": "https://everettstuckey.github.io/IS_LT-7383/"
        },
        {
          "title": "Digital Literacy Curriculum",
          "href": "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html"
        }
      ]
    },
    {
      "course_code": "IS_LT 9466",
      "title": "Learning Analytics",
      "description": "Data collection, analysis, and interpretation to improve learning.",
      "term": "2024 Spring",
      "reflection": "This course helped me separate curiosity from usefulness when it comes to data. I began by articulating the questions I actually needed to answer - Are students getting stuck on specific steps? Which explanations reduce confusion? Where does motivation flag? - and only then selecting indicators that could reasonably inform those questions. I resisted the temptation to instrument everything and focused instead on a small set of measures aligned to outcomes: task completion, time-on-task for key pages, error rates, and short reflections that captured perceived clarity and confidence.\n\nDesigning the pipeline required attention to proportionality and privacy. I avoided collecting sensitive data that were not essential to learning goals, and I aggregated results whenever possible. For example, rather than logging every click, I tracked transitions between meaningful states (e.g., from \"view deadlines, to \"start application,). I documented retention windows and access controls and communicated in plain language what was being collected and why. This transparency increased trust and reduced anxiety for students and families accustomed to opaque data practices.\n\nInterpreting analytics demanded humility. Time-on-task can reflect deep engagement or confusion; completion rates might improve because instructions are clearer or because learners are skipping important steps. To avoid false certainty, I paired quantitative indicators with quick qualitative checks - two-question pulses at natural breakpoints and occasional think-alouds with volunteers. These mixed methods helped me surface causal mechanisms and prioritize changes with the greatest impact on understanding and persistence.\n\nWith these practices in place, I used analytics to drive specific improvements. When I noticed extended dwell times on a guidance page with low subsequent application starts, I rewrote the first two paragraphs to front-load purpose and reduced reading burden by chunking steps. I also added a short checklist and a \"What you\"ll need, callout. In the next cycle, dwell time decreased and application starts rose, with student reflections citing \"clearer next steps, as the main factor. In another case, I discovered that mobile users abandoned a long form more frequently; responsive tweaks and a save-and-return affordance reduced drop-offs.\n\nPerhaps the most significant lesson was to keep analytics humane. Dashboards were designed for action, not for surveillance: simple visualizations tied to specific decisions, like which pages needed revision or where to place a clarifying prompt. I also built in guardrails - reminders about interpretation limits and links to the evidence behind a proposed change - so conversations stayed grounded. By closing the loop from evidence to iteration and back again, I created a sustainable rhythm of improvement that respected learners\" privacy and time.\n\nThe approach now permeates my portfolio. From the Digital Literacy Curriculum to scholarship resources, analytics serve design intent: to clarify, to support, and to empower. The course shifted my mindset from \"more data is better, to \"the right data, used ethically, makes learning better.,",
      "materials": [
        {
          "title": "Digital Literacy Curriculum",
          "href": "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html"
        }
      ]
    },
    {
      "course_code": "IS_LT 9467",
      "title": "Technology to Enhance Learning",
      "description": "Technology integration strategies for impact and equity.",
      "term": "2025 Summer",
      "reflection": "This course helped me to develop a deeper understanding of how technology can be used to enhance learning. I learned about the different types of technology integration, including substitution, augmentation, modification, and redefinition. I also learned about the importance of considering the SAMR model when designing technology-enhanced lessons.\n\nOne of the most significant takeaways from this course is the importance of equity in technology integration. I learned about the digital divide and how it can impact student learning. I also learned about the importance of providing access to technology for all students, regardless of their background or socioeconomic status.\n\nI also learned about the different types of technology that can be used to enhance learning, including learning management systems, educational software, and multimedia tools. I learned about the importance of selecting technology that is aligned with the learning objectives and that is accessible to all students.\n\nThroughout the course, I had the opportunity to design and implement technology-enhanced lessons. I learned about the importance of considering the instructional design model when designing lessons, and I learned about the different types of instructional strategies that can be used to enhance learning.\n\nOne of the most significant challenges I faced in this course was learning to navigate the different types of technology. I had to learn how to use new software and tools, and I had to learn how to troubleshoot technical issues. However, with the support of my instructor and my peers, I was able to overcome these challenges and develop a deeper understanding of how technology can be used to enhance learning.\n\nOverall, this course was incredibly valuable. I learned about the importance of technology integration in education, and I developed a deeper understanding of how technology can be used to enhance learning. I also developed a range of skills, including instructional design, technology integration, and troubleshooting.\n\nAs I reflect on the impact of this course, I realize that it has fundamentally changed the way I approach teaching and learning. I no longer see technology as a separate entity from instruction, but rather as an integral part of the learning process. I understand that technology can be used to enhance learning, and I am committed to using it in a way that is equitable and accessible to all students.\n\nThis shift in mindset has had a profound impact on my practice. I am more intentional about using technology to enhance learning, and I am more focused on creating lessons that are accessible and inclusive. I am also more willing to take risks and try new things, knowing that I can always iterate and improve.\n\nOne of the most significant takeaways from this course is the importance of ongoing professional development. I learned about the importance of staying current with the latest research and trends in technology integration, and I learned about the importance of seeking out opportunities for professional growth and development.\n\nOverall, this course has been a game-changer for me. It has helped me to develop a more nuanced understanding of technology integration and to see myself as a facilitator of learning. I am excited to continue to apply the principles and practices I have learned in this course to my future work."
    },
    {
      "course_code": "IS_LT 9471",
      "title": "Instructional Systems Design",
      "description": "Systematic analysis, design, development, and evaluation.",
      "term": "2025 Spring",
      "reflection": "This course operationalized ADDIE in my day-to-day work. In Analysis, I clarified the problem space with stakeholders - counselors, students, and families - using interviews, journey mapping, and quick artifact audits. We defined the gap in terms of learner performance and experience, not just content coverage. From there, I wrote measurable objectives that described what learners should be able to do and under what conditions, aligning them to authentic tasks they actually encounter.\n\nDuring Design, I translated objectives into assessments and learning activities. I favored performance tasks over recall, and I arranged sequences to activate prior knowledge, reduce extraneous load, and support transfer. I made accessibility a non-negotiable constraint, specifying patterns (heading hierarchy, alt text, focus states), contrast thresholds, and mobile-first layouts. Prototypes at this stage were intentionally low-fidelity so I could iterate quickly based on feedback.\n\nDevelopment focused on turning designs into maintainable artifacts. I built reusable components and wrote microcopy that anticipated sticking points. I organized assets for traceability and reuse, and documented decisions so collaborators could pick up where I left off. Throughout, I validated materials with quick checks - does the assessment truly measure the objective? Does the activity provide the right level of support? - and I fixed mismatches before they calcified.\n\nImplementation emphasized readiness. I created facilitator notes, checklists, and support guides, and I rehearsed delivery with colleagues to catch practical issues. I monitored early sessions for surprises and captured them as candidates for the next iteration. Importantly, I framed changes as part of the process, not as evidence that something had \"failed\" which encouraged honest feedback and steady improvement.\n\nEvaluation ran through everything, not just the end. I combined formative evidence (observations, quick polls, error patterns) with summative indicators (completion, performance against rubrics) to judge effectiveness. When evidence suggested a misalignment, say, learners succeeded on a task but could not transfer the skill, I revisited objectives and activities to tighten the link. I also reported findings in concise, actionable summaries to stakeholders so decisions about what to keep, revise, or retire were grounded in shared understanding.\n\nAcross the portfolio, this disciplined approach improved coherence and outcomes. Artifacts like the Digital Literacy Curriculum show objectives, tasks, and assessments pulling in the same direction, while the rapid prototypes demonstrate how early evaluation prevents costly rework. ADDIE is no longer an abstract model for me; it is a practical rhythm for designing learning that respects people's time and leads to measurable, equitable gains.",
      "materials": [
        {
          "title": "Digital Literacy Curriculum",
          "href": "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html"
        }
      ]
    },
    {
      "course_code": "IS_LT 9473",
      "title": "Project Management",
      "description": "Planning, execution, risk, and stakeholder communication.",
      "term": "2024 Spring",
      "reflection": "This course provided the scaffolding to deliver learning products predictably and sustainably. I learned to define scope crisply - what is in, what is out, and why - and to translate that scope into milestones with observable outcomes. I time-boxed work, used Kanban-style boards to visualize flow, and limited work in progress so quality did not suffer. These habits reduced context switching and created momentum without requiring heroic efforts.\n\nRisk management became a routine, not a formality. I identified technical, content, and schedule risks early and documented triggers, likelihood, and mitigations in a simple log. For web artifacts, recurring risks like link rot and browser quirks received standing checks: automated link health scans, manual spot checks on common devices, and a brief checklist for major releases. By making risk visible, I could negotiate trade-offs honestly with stakeholders - choosing to descale a feature, for instance, to protect a deadline and quality bar.\n\nCommunication was another pillar. I wrote succinct status updates that focused on decisions, risks, and next steps rather than play-by-play activity. I set expectations for feedback windows and established one \"source of truth\" for assets to avoid version sprawl. Retrospectives closed each cycle, capturing what worked, what was painful, and what we would try next. Over time, these rituals built trust and reduced surprises.\n\nResourcing and scheduling were treated as design problems. I estimated effort using comparative sizing rather than false precision, grouped similar tasks to gain efficiencies, and reserved capacity for the inevitable unknowns. I also created templates - issue trackers, release notes, change logs - that shortened ramp-up for collaborators and preserved context for future revisions.\n\nThe payoff is evident across my portfolio. Releases are steadier, quality is more consistent, and maintenance overhead is lower. Students and colleagues benefit from reliable updates and clear communication; I benefit from a process that is humane and sustainable. Project management did not make the work less complex, but it made the path through that complexity clearer and fairer for everyone involved."
    },
    {
      "course_code": "IS_LT 9474",
      "title": "Front End Analysis",
      "description": "Needs assessment and problem framing techniques.",
      "term": "2024 Fall",
      "reflection": "This course taught me to slow down and frame the right problem before building solutions. I mapped the ecosystem around my learners - students, families, counselors, platforms - and gathered perspectives through interviews, quick surveys, and artifact reviews. I resisted the urge to jump to features and instead synthesized what I heard into jobs-to-be-done and pain points. Journey mapping exposed where energy was lost: unclear eligibility, scattered deadlines, instructions that assumed background knowledge students did not yet have.\n\nFrom this analysis, I wrote problem statements that described the gap in observable terms and identified constraints we had to honor (devices, time, bandwidth, privacy). I evaluated candidate solutions against criteria grounded in this reality: Does the change reduce steps? Does it improve comprehension at the moment of decision? Is it sustainable for counselors to maintain? This filtered out ideas that were flashy but fragile and surfaced simpler interventions with outsized impact - clearer labels, checklists, consistent placement of critical information, and targeted microcopy.\n\nI also honed techniques for gathering just enough data to move forward. Lightweight card sorts clarified language; five-minute usability checks with students revealed misleading layouts; quick analytics confirmed whether changes produced the expected behavior. Importantly, I documented not only what we learned but what we chose not to do and why, preserving rationale for future teams and preventing drift.\n\nFront-end analysis improved alignment across stakeholders. With a shared understanding of the problem and the criteria for success, conversations shifted from personal preference to learner impact. That alignment made subsequent design and development faster and less contentious. It also made evaluation clearer: we knew what we intended to change and how we would know if it worked.\n\nUltimately, this course embedded a habit of inquiry that now precedes major changes in my portfolio. I approach new challenges with curiosity and discipline, confident that the time invested up front will pay dividends in clarity, quality, and equity for the learners I serve."
    }
  ],
  "artifact_map": {
    "https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html": {
      "title": "Digital Literacy Curriculum",
      "course_codes": [
        "IS_LT 7355",
        "IS_LT 9410",
        "IS_LT 9455",
        "IS_LT 9466",
        "IS_LT 9471"
      ],
      "mentions": {
        "IS_LT 7355": [
          "Digital Literacy Curriculum"
        ],
        "IS_LT 9410": [
          "Digital Literacy Curriculum"
        ],
        "IS_LT 9455": [
          "Digital Literacy Curriculum"
        ],
        "IS_LT 9466": [
          "Digital Literacy Curriculum"
        ],
        "IS_LT 9471": [
          "Digital Literacy Curriculum"
        ]
      }
    },
    "https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial Markets Slide Show.html": {
      "title": "Financial Markets Slideshow",
      "course_codes": [
        "IS_LT 7383"
      ],
      "mentions": {
        "IS_LT 7383": [
          "Financial Markets Slideshow"
        ]
      }
    },
    "https://everettstuckey.github.io/IS_LT-7383/": {
      "title": "Rapid Development Tools Portfolio",
      "course_codes": [
        "IS_LT 7383",
        "IS_LT 9410",
        "IS_LT 9455"
      ],
      "mentions": {
        "IS_LT 7383": [
          "Rapid Development Tools Portfolio"
        ],
        "IS_LT 9410": [
          "Rapid Development"
        ],
        "IS_LT 9455": [
          "Rapid Development"
        ]
      }
    }
  }
};
