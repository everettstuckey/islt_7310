<div class="container">
<h2>Learning Technologies Program Goals and Student Learning Outcomes</h2>
<p>The Learning Technologies (LTD) program prepares practitioners to systematically design, develop, evaluate, and lead learning technology and instructional solutions across educational and organizational contexts. The reflections below connect my artifacts to each outcome, demonstrating how theory and research informed my practice in equitable, accessible, and sustainable ways.</p>
<h3>Goal 1</h3>
<p>Develop theory- and research-based skills for innovative, aesthetic, accessible, equitable, effective, and sustainable design and development of technologies for learning opportunities and systems.</p>
<h4>Student Learning Outcome 1.1  Reflection</h4>
<p>My approach to design and development matured from assembling activities to architecting coherent learning experiences. At the center of this shift is a commitment to build opportunities that are accessible, equitable, and durable across contexts. I now begin by clarifying performance goals and actionable success criteria, then select modalities - direct instruction, inquiry, collaboration, or performance support - that best serve those goals for my learners and setting. This outcome challenged me to move beyond personal preference and to ground choices in evidence about how people actually learn and what they need at the moment of action.</p>
<p>The <a href="https://everettstuckey.github.io/islt_7310/elearning/digital_literacy_curriculum.html" rel="noopener" target="_blank">Digital Literacy Curriculum</a> exemplifies this evolution. Early drafts were content-heavy; learners had to sift through long passages to find critical steps. Iteration led me to concise, purposeful explanations followed by practice where thinking is visible. I embedded options consistent with Universal Design for Learning: text paired with visuals, short scenarios in place of abstractions, and flexible response formats. Every section answers a simple question - what do learners need to understand or do, and what support will help them get there - and unnecessary flourish is trimmed away.</p>
<p>Complementing direct instruction, I designed student-centered inquiry where learners choose variables, collect data, and test claims. In the <a href="https://everettstuckey.github.io/IS_LT-7383/" rel="noopener" target="_blank">Financial Data Analysis with Python &amp; Google Sheets</a> work, students co-constructed datasets and iterated on visualizations to explore market behavior. I provided exemplars, templates, and guardrails, but I resisted over-scaffolding; the intent was to promote productive struggle with clear milestones. This balance helped students practice critical evaluation - of their own assumptions and of the tools they were using - while keeping the cognitive load appropriate.</p>
<p>Collaboration was designed, not assumed. I specified norms, roles, and artifacts that make contributions visible: shared sheets with version history, comment threads with prompts, and brief reflection checkpoints to consolidate learning. Small procedural details mattered: giving teams a first five-minute "define the question" stage, for example, prevented premature data chasing and led to clearer end products. These structures, while lightweight, set expectations that foster accountability and psychological safety.</p>
<p>Performance supports rounded out the ecosystem. I built quick-reference visuals, checklists, and micro-tutorials that learners could access at the moment of need. Rather than hiding help in a separate "resources" area, I placed it adjacent to tasks and wrote it in plain language. This reduced unnecessary help-seeking and empowered students to move forward independently. Importantly, supports were iterative: analytics and brief reflections told me which aids were actually used, and I retired or simplified the rest.</p>
<p>Across these designs, accessibility and equity remained non-negotiable. I used semantic HTML and heading hierarchy for structure, ensured color contrast and visible focus states, and designed mobile-first so materials worked well on phones. I also prepared printable and offline-friendly versions when feasible, recognizing that access conditions vary day to day. The goal is not to create one "perfect" artifact but a resilient set of experiences that meet learners where they are and invite them further.</p>
<p>Finally, I documented design decisions and component patterns so I could extend and maintain them over time. This small "system" reduces rework and keeps experiences consistent as new artifacts are added. The result is a portfolio of opportunities that feel cohesive while allowing for variety in activity type and learner agency. In short, this outcome taught me to design with intention, test with humility, and develop with the long view in mind.</p>
<h4>Student Learning Outcome 1.2  Reflection</h4>
<p>This outcome taught me to link content, context, and cognition in concrete ways. I now start by activating prior knowledge and explicitly naming the transfer target - what learners should be able to do outside the activity. This framing prevents the all-too-common drift toward interesting but disconnected tasks. I then sequence experiences from worked examples to guided practice to more open challenges, tracking cognitive load along the way so that the difficulty comes from the thinking, not from deciphering instructions.</p>
<p>In the Digital Literacy Curriculum, meaningful learning shows up as scenario-based practice with tight feedback loops. Students analyze realistic posts, privacy prompts, and sources, applying credibility checks and ethical considerations in context. I scaffold judgement with short checklists and contrastive examples, then remove supports as learners demonstrate readiness. Reflection prompts ask students to connect decisions to their own online lives - an intentional move to improve transfer and self-regulation.</p>
<p>In the Financial Data Analysis activities, technology choices serve pedagogy. Python and Google Sheets are instruments to visualize patterns, test conjectures, and revise claims - not ends in themselves. I provided starter notebooks and sheet templates that emphasized thinking, not syntax. Learners selected variables relevant to questions they cared about, which increased motivation and made discussions more authentic. As they iterated on visuals, I facilitated sense-making with prompts like "What would you expect if your claim were false?, and "Which representation reveals the structure you"re looking for?,</p>
<p>Media design supported attention and retention. The <a href="https://everettstuckey.github.io/IS_LT-7383/slideshows/Financial%20Markets%20Slide%20Show.html" rel="noopener" target="_blank">Financial Markets Slideshow</a> uses concise narration, consistent visual metaphors, and pacing aligned to learning objectives. I reduced extraneous load by limiting new ideas per slide, aligning text and visuals, and inserting brief checks for understanding to surface misconceptions early. Students reported that the structure helped them follow the argument and prepared them to ask better questions in subsequent activities.</p>
<p>Choice and agency were built into the experience. Learners could select datasets, questions, or formats for demonstrating understanding, within clearly defined constraints. This balance respected individual interests while maintaining comparability for feedback. I also designed collaboration deliberately - rotating roles, short planning intervals, and shared artifacts - to ensure that meaningful learning was a collective endeavor rather than a divide-and-conquer exercise.</p>
<p>Assessment aligned with the goals of meaning and transfer. Rubrics emphasized reasoning, evidence, and communication over rote recall. I used low-stakes formative checks to guide instruction and reserved summative tasks for integrative demonstrations. When results revealed gaps, I adjusted the sequence or added targeted mini-lessons rather than simply reteaching the same material. Over time, these practices produced clearer thinking, more resilient understanding, and improved ability to apply concepts in new contexts.</p>
<p>Perhaps most importantly, I learned to narrate the why of design choices to learners. Explaining how a prompt reduces cognitive load or why a representation supports inference invited students into the design conversation and improved metacognition. Meaningful learning, I discovered, is not just something we design for learners; it is something we design with them as partners.</p>
<h4>Student Learning Outcome 1.3  Reflection</h4>
<p>This outcome pushed me to treat instructional design not only as a creative process but also as a disciplined practice supported by project management. I learned to define scope crisply - who the work is for, what success looks like in observable terms, what is in-bounds and out-of-bounds - and to translate that scope into milestones with concrete deliverables. Time-boxing work into short sprints, I set evidence-based checkpoints that invited feedback early, before small issues could snowball into large rework. These habits made delivery more predictable and focused my attention on outcomes rather than activity.</p>
<p>Risk management became routine instead of an afterthought. I identified technical, content, and schedule risks up front, recorded triggers and mitigations, and revisited the list weekly. For web artifacts, recurring risks like link rot, browser quirks, and inconsistent mobile behavior received standing checks: automated link health scans, spot testing on common devices, and a short pre-release checklist. By making risks visible, I could negotiate trade-offs honestly with stakeholders - descope a lower-value enhancement, for example, to protect an accessibility fix or a critical deadline.</p>
<p>Versioning and traceability supported iteration at speed. I organized assets with clear naming, dates, and notes so collaborators (and my future self) could pick up work without losing context. I documented decisions in lightweight logs that captured the problem, options considered, the chosen path, and the rationale. When feedback or analytics suggested a pivot, these notes accelerated the change and preserved the why behind it, which in turn built trust with colleagues and administrators who wanted both agility and accountability.</p>
<p>Usability evidence anchored milestone reviews. Rather than relying on intuition, I scheduled brief checks - five-minute think-alouds with students, quick heuristic passes, and small A/B comparisons of competing microcopy. The cadence mattered: frequent, low-overhead sessions produced a steady stream of actionable findings without derailing progress. This rhythm helped me catch issues like ambiguous labels, excessive reading load, or misaligned hierarchy long before release, saving hours of downstream support.</p>
<p>Maintenance planning was part of delivery, not a separate phase. I adopted a small design system - tokens for color, spacing, and typography; reusable components for navigation, callouts, and tables - so changes propagated consistently across artifacts. I added print styles for key pages and compressed assets to keep performance snappy on school Wi-Fi and cellular connections. I also created release notes and a simple change log so counselors knew what had shifted and why. These choices lowered total cost of ownership and made continuous improvement realistic within busy school calendars.</p>
<p>Crucially, the project management lens improved equity. Predictable milestones and transparent communication helped me keep accessibility fixes and clarity enhancements at the front of the queue, not as "stretch, work that slips when time is tight. By measuring progress against learner-centered criteria - task completion, reduced confusion, better transfer - I kept impact on students and families central to schedule decisions.</p>
<p>Across Rapid Development artifacts and the Digital Literacy Curriculum, this outcome shows up as steadier releases, fewer regressions, and clearer, more maintainable materials. The combination of scope definition, risk tracking, versioning, and retrospectives created a sustainable engine for improvement: ship small, learn quickly, capture decisions, and move forward with purpose. In short, project management practices did not constrain creativity; they protected it by giving the work cadence, clarity, and care.</p>
<h4>Student Learning Outcome 1.4  Reflection</h4>
<p>This outcome represents the craft side of my work - the ability to produce web and media artifacts that are both technically sound and pedagogically effective. I deepened my command of semantic HTML, ensuring that structure communicates meaning to assistive technologies and creates a reliable foundation for styling and scripting. I refined responsive CSS patterns that scale gracefully from phones to desktops, and I standardized spacing, typography, and component styles so learners encounter a coherent visual language across pages. These skills turn abstract accessibility commitments into everyday defaults.</p>
<p>Color and contrast received special attention. Using WCAG guidelines as a floor (not a ceiling), I adjusted palettes for sufficient contrast and tested focus states under real conditions - glare, older monitors, small screens. I learned to enhance discoverability without visual clutter: subtle elevation for interactive cards, generous hit areas for links on mobile, and legible type scales that respect cognitive load. Alt text moved from a checklist item to a communication choice; I wrote descriptions that conveyed purpose and context, not just appearance, and used accessible names for icon-only elements.</p>
<p>On the media side, I practiced narrative pacing and purposeful visuals. In the Financial Markets Slideshow, early versions tried to do too much per slide. Iteration taught me to align each segment to a single objective, reduce density, and synchronize narration with visual changes. I learned to use contrastive examples, consistent metaphors, and brief formative checks to keep attention and surface misconceptions. The result was clearer explanations and richer conversation, as students arrived at activities with a shared understanding of key ideas.</p>
<p>Data visualization work reinforced the principle that form serves understanding. I selected representations (lines for trends, bars for comparisons, scatterplots for relationships) based on the question at hand and annotated them with short, purposeful labels. I avoided decorative flourishes that obscure signal, and I used color sparingly to guide, not distract. When students created their own visuals, I provided starter templates with accessible defaults, then encouraged iteration as claims evolved - an approach that balanced support with agency.</p>
<p>Production also includes the "invisible, qualities that make artifacts dependable. I optimized images, deferred noncritical assets, and added print styles so materials work offline when needed. I tested across browsers and input methods and kept a small matrix of representative devices to prevent regressions. I wrote microcopy that anticipates confusion and provides just-in-time guidance, and I established patterns for error prevention and recovery so learners stay focused on ideas rather than troubleshooting.</p>
<p>Finally, I treated my production workflow as a learning system. I documented reusable snippets, created checklists for common tasks (e.g., preparing a new page, exporting media with captions), and left commentary in the code where future collaborators would need context. These habits reduced ramp-up time and preserved quality during revisions. Together, the web and media skills I strengthened in this outcome ensure that the materials I build are not only attractive, but also accessible, maintainable, and aligned to the work of learning.</p>
<h3>Goal 2</h3>
<p>Apply systematic methods for analyzing instructional or training needs, using the results to drive design and development, and collecting and analyzing evaluation data to improve products.</p>
<h4>Student Learning Outcome 2.1  Reflection</h4>
<p>This outcome taught me to treat analysis as a purposeful and proportional inquiry rather than an endless phase. I began by identifying the decisions the analysis needed to inform - labels to use, sequences to present, supports to provide - and chose methods that would yield just enough evidence to move forward. Learner interviews, short task walkthroughs, and concise content audits became my primary tools because they surface friction quickly without imposing large time costs on students or staff.</p>
<p>Early reviews of scholarship resources exposed two issues: findability and language complexity. Students reported that they "didn"t know where to start,, and when they found the right page, the instructions felt dense. I mapped the journey from discovery to application and marked points of hesitation. Card sorts clarified which labels felt intuitive, while five-minute usability checks with students revealed where layout implied the wrong hierarchy. These small studies consistently pointed to the same remedies: fewer categories, clearer verbs in headings, and chunked steps with visible progress.</p>
<p>In parallel, I met with counselors and families to understand constraints outside the screen - time, bandwidth, device limits, and competing responsibilities. Their perspective reminded me that the best design is unusable if it assumes ideal conditions. I translated these findings into explicit requirements: mobile-first layouts, printable quick guides, and visible "what you"ll need, sections that prevent back-and-forth searching. I also committed to accessibility as a baseline, not a bonus, specifying contrast, focus, and alt-text practices in my component documentation.</p>
<p>For the Financial Data Analysis work, stakeholder conversations highlighted the value of collaborative spreadsheets and simple, interpretable visualizations to make progress visible. Students wanted to see their thinking accumulate in a shared place. That requirement shaped both the artifacts and the procedures: we used common templates, explicit norms for comments, and checkpoints that turned intermediate results into learning moments. Analysis, here, was not separate from design; it was the lens that kept choices anchored to the realities of the learners and goals.</p>
<p>Crucially, I framed analysis as iterative. Each design sprint began with a hypothesis ("If we simplify labels and front-load purpose, application starts will rise,), a small test, and a plan for what evidence would confirm or challenge the claim. This created a practical rhythm: gather light evidence, act, and learn. The result was a pipeline that mapped directly to real needs, avoided over-collection of data, and respected the time and privacy of participants.</p>
<p>The payoff was clarity and momentum. Navigation labels became simpler and more consistent, instructions became shorter and easier to scan, and supports appeared where learners actually needed them. By keeping analysis close to decisions and right-sized to the context, I created a design practice that is rigorous enough to be trustworthy and light enough to sustain in a busy school environment.</p>
<h4>Student Learning Outcome 2.2  Reflection</h4>
<p>This outcome focused my attention on alignment - between outcomes, activities, and evidence - and on the practical ways assessment can drive iteration without exhausting learners. I began writing outcomes that describe performance in context ("Given a scholarship description, the student identifies eligibility and next steps with supporting rationale,), then designed tasks that elicit that performance. Assessment, in this view, is not a separate event; it is built into the experience and yields evidence we can act on.</p>
<p>In the Digital Literacy Curriculum, checks for understanding target judgement, not recall. Short prompts ask students to label claims, identify missing information, or explain a privacy decision. Rubrics emphasize reasoning, evidence, and communication. I used exemplars - both strong and developing - to calibrate expectations and to normalize revision as part of learning. When reflections or results showed patterns of confusion, I rewrote prompts, added contrastive examples, or introduced a mini-lesson to address the gap before moving on.</p>
<p>Within Rapid Development artifacts, I combined interaction metrics with brief reflections to learn where learners struggled or disengaged. I tracked transitions between meaningful states (e.g., from guidance to action), time spent on key steps, and completion of short checks. Because metrics alone can mislead, I paired them with one- to two-question pulses ("What was unclear here?, "What would have helped you move faster?,). This blend produced actionable signals while respecting privacy and time.</p>
<p>Iteration closed the loop. When data suggested instructions were dense, I chunked steps and front-loaded purpose. When learners missed a conceptual pivot, I slowed the pacing and added a guiding question. When mobile users abandoned a page, I adjusted layout and tap targets. Each change came with a small prediction and a plan to verify impact in the next cycle. Over time, the materials became clearer and the learning interactions more purposeful.</p>
<p>Finally, I shared evidence with learners and colleagues. Quick, visual summaries helped students see their progress and understand why adjustments were being made. For staff, short debriefs connected results to the next step, building a shared commitment to evidence-informed practice. Assessment, used this way, is not a gate - it is a guide that keeps the work honest and aligned to the outcomes we value.</p>
<h3>Goal 3</h3>
<p>Understand the societal impact of technology and strive to produce ethical, inclusive, and equitable instructional systems and technologies.</p>
<h4>Student Learning Outcome 3.1  Reflection</h4>
<p>This outcome centers the ethical and societal dimensions of educational technology. I committed to practices that are transparent, proportional, and protective of learner dignity. In concrete terms, that meant minimizing data collection to what is necessary to support learning, favoring aggregate patterns over individual tracking, and communicating in plain language what information is being collected, how it will be used, and when it will be deleted. These commitments were documented and visible, not implied.</p>
<p>Privacy by design guided implementation. I audited data flows for each artifact, noting collection points, storage locations, access controls, and retention windows. I removed sensitive fields that were not essential to the learning goal, de-identified where possible, and set short, default retention periods for operational data. For analytics, I tracked transitions between meaningful states rather than raw clickstreams, which reduced granularity without sacrificing insight. When I needed more detail to answer a question, I obtained consent and explained the trade-offs.</p>
<p>In communication with students and families, I emphasized agency. I explained what metrics I was watching (e.g., completion of a checklist, time spent on a guidance step) and why those metrics helped me improve materials. I invited questions and provided alternatives when feasible. This two-way dialogue built trust and made the rationale for changes clear. Importantly, I avoided framing analytics as judgments of individual worth or effort; instead, they were signals about where the design might be getting in the way.</p>
<p>Equity informed content and access decisions. I prioritized mobile-friendly, low-bandwidth designs, provided printable resources, and ensured that critical instructions lived on pages under my control so I could maintain accessibility. I reviewed language for clarity and cultural resonance, avoiding idioms and assumptions that might exclude. I also considered the broader ecosystem - how school policies, home technology, and time constraints interact - and adjusted expectations and supports accordingly.</p>
<p>Ethics also shaped how I reported findings. I favored aggregate summaries with actionable recommendations over detailed logs. When sharing examples, I masked identities and sought consent when quotes were used. I included limitations sections that clarified what the evidence could and could not tell us, to guard against overreach. These practices reinforced a culture where evidence improves instruction without compromising the people it is meant to serve.</p>
<p>Ultimately, this outcome reminded me that technology is not neutral. The choices we make about what to measure, how to display it, and what to reward carry values. By foregrounding privacy, proportionality, and inclusion, I aim to ensure that the benefits of technology flow to all learners - not just those best positioned to navigate opaque systems - and that the means of improvement honor the very people we hope to support.</p>
<h4>Student Learning Outcome 3.2  Reflection</h4>
<p>This outcome challenged me to operationalize Universal Design for Learning (UDL) as a practical set of design moves rather than a slogan. I worked to create materials that offer multiple means of engagement, representation, and action/expression so that variability is anticipated from the start. Concretely, this meant providing choices in tasks and formats, designing explanations that combine concise text with supportive visuals, and building in scaffolds that fade as learners gain confidence.</p>
<p>In the Digital Literacy Curriculum, I presented core ideas through short, plain-language explanations paired with examples and visuals. I used scenario prompts that connect to students" real online experiences, and I offered alternative ways to demonstrate understanding - written justification, quick audio reflections, or annotated screenshots. Captions, clear focus styles, and sufficient contrast made the materials more usable across devices and contexts. I also provided printable versions and lightweight downloads for learners with limited connectivity.</p>
<p>The Financial Markets Slideshow illustrates UDL in media. I paced narration to align with visual changes, reduced extraneous text, and used consistent metaphors to support schema building. Quick checks for understanding prompted retrieval and transfer without high stakes, and transcripts ensured access for those who prefer to read or who need accommodations. When learners moved into analysis, templates and exemplars provided structure while still allowing for choice of variables and visualizations.</p>
<p>Choice, however, is only empowering when it is bounded and supported. I learned to provide clear constraints (e.g., select from these datasets, address this question) and to include planning prompts so students do not spend all their cognitive resources deciding what to do. I also introduced peer support structures - guided feedback using simple rubrics - that made collaboration accessible and reduced the social risk of sharing work in progress.</p>
<p>UDL also informed how I write and organize content. Headings communicate structure, microcopy anticipates confusion, and examples are drawn from diverse contexts to promote relevance. I pilot wording with students to catch assumptions and to simplify where possible. Accessibility is embedded in the code (semantic HTML, proper labels and roles) and in the design (contrast, spacing, hit targets), making inclusive practice the default rather than an accommodation layered on top.</p>
<p>Across artifacts, these moves reduced barriers and improved participation. Students reported that they could enter tasks more confidently, navigate more easily on phones, and choose ways of demonstrating understanding that felt authentic. By combining UDL with an iterative mindset, I continue to refine materials so that inclusion is not a one-time effort but a living practice that evolves with my learners" needs.</p>
</div>
